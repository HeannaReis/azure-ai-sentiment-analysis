2025-06-28 11:33:14,871 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:33:14,872 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:33:14,873 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:33:14,874 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:33:14,875 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:33:14,876 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:33:14,876 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:33:14,877 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:33:14,882 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:33:14,883 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:33:14,884 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:33:14,886 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:33:14,887 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:33:14,888 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:33:14,889 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:37:53,170 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:37:53,173 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:37:53,174 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:37:53,175 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:37:53,177 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:37:53,182 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:37:53,183 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:37:53,184 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:37:53,185 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:37:53,185 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:37:53,186 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:37:53,189 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:37:53,197 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:37:53,198 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:37:53,199 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:37:53,214 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-06-28 11:39:13,368 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:39:13,369 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:39:13,371 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:39:13,372 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:39:13,373 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:39:13,376 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:39:13,379 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:39:13,380 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:39:13,383 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:39:13,386 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:39:13,398 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:39:13,399 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:39:13,403 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:39:13,422 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:39:13,425 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:39:13,814 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:39:13,816 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:39:13,817 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:39:13,818 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:39:13,820 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:39:13,826 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:39:13,830 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:39:13,837 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:39:13,840 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:39:13,850 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:39:13,851 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:39:13,856 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:39:13,864 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:39:13,867 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:39:13,869 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:39:15,171 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 11:39:23,837 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 11:39:25,091 [INFO] AFC remote call 1 is done.
2025-06-28 11:42:04,218 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:42:04,219 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:42:04,220 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:42:04,221 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:42:04,222 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:42:04,223 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:42:04,225 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:42:04,228 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:42:04,229 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:42:04,231 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:42:04,232 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:42:04,234 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:42:04,236 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:42:04,237 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:42:04,239 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:42:04,536 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:42:04,537 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:42:04,539 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:42:04,545 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:42:04,549 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:42:04,554 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:42:04,561 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:42:04,567 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:42:04,588 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:42:04,593 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:42:04,615 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:42:04,618 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:42:04,621 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:42:04,632 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:42:04,635 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:42:04,649 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")



=============

Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000018E29EB7B50>

erro na minha aplica√ß√£o conforme c√≥digos anexos
2025-06-28 11:42:16,509 [INFO] Resposta da IA (texto): O erro "cannot identify image file <_io.BytesIO object at...>" indica que a fun√ß√£o `Image.open()` do PIL (Pillow) n√£o conseguiu identificar o formato da imagem dentro do objeto `BytesIO`. Isso geralmente acontece quando os dados dentro do `BytesIO` n√£o s√£o dados de imagem v√°lidos ou o PIL n√£o consegue determinar o formato.

Analisando o c√≥digo, o problema mais prov√°vel reside na fun√ß√£o `generate_image` em `services\image_services.py` e, consequentemente, no uso dessa fun√ß√£o no script `chat_streamlit.py`.

Aqui est√£o os pontos a serem revisados e as poss√≠veis solu√ß√µes:

**1. Confirma√ß√£o do Tipo MIME (services\image_services.py):**

Certifique-se de que o `part.mime_type` seja realmente 'image/png' (ou outro tipo de imagem v√°lido como 'image/jpeg') antes de tentar abrir com `Image.open()`.  Se o tipo MIME for diferente, trate-o adequadamente ou ignore a parte.

```python
def generate_image(prompt: str) -> Image.Image | None:
    # ... (c√≥digo anterior) ...
    if response.parts:
        for part in response.parts:
            if part.mime_type == 'image/png' or part.mime_type == 'image/jpeg':  # Adicionado suporte a JPEG
                try:
                    return Image.open(BytesIO(part.data))
                except Exception as e:
                    print(f"Erro ao abrir imagem do BytesIO: {e}")
                    return None
            else:
                print(f"Parte ignorada, tipo MIME: {part.mime_type}") # Debugging
    # ... (c√≥digo posterior) ...
```

**2. Tratamento de Erros ao Abrir a Imagem (services\image_services.py):**

Adicione um bloco `try...except` dentro do loop `for part in response.parts:` para capturar erros que podem ocorrer ao tentar abrir a imagem com `Image.open()`. Isso pode fornecer informa√ß√µes mais espec√≠ficas sobre o motivo da falha.  A impress√£o do `part.mime_type` ajudar√° a entender se o problema √© um tipo mime inesperado.

**3. Valida√ß√£o dos Dados da Imagem (services\image_services.py):**

Antes de usar `Image.open()`, verifique se `part.data` cont√©m dados.  Se estiver vazio, n√£o tente abrir.

```python
if part.inline_data is not None:
    if part.inline_data.data:  # Verifica se h√° dados
        image = Image.open(BytesIO(part.inline_data.data))
        st.session_state.generated_image = image
        return image
    else:
        print("Dados da imagem vazios!") # Debugging
```

**4. Chame a fun√ß√£o generate_image corretamente no Streamlit (chat_streamlit.py):**

Certifique-se que `st.session_state.generated_image` seja None no in√≠cio da fun√ß√£o `generate_image`, para evitar estados incorretos.

**5. API Key e Modelo:**

Embora o c√≥digo verifique se a API Key est√° definida, verifique se ela √© v√°lida e tem permiss√£o para usar o modelo `gemini-2.0-flash-exp-image-generation`.  A mensagem de erro pode ser gen√©rica e esconder um problema de autentica√ß√£o ou permiss√£o.

**6.  Informa√ß√µes Adicionais no Log:**

Adicione mais informa√ß√µes de log nos pontos cr√≠ticos do c√≥digo para facilitar a depura√ß√£o.  Por exemplo, registre o tamanho de `part.data` antes de tentar abrir a imagem.

```python
logger.info(f"Tamanho dos dados da imagem: {len(part.inline_data.data)}") # Log do tamanho dos dados
```

**7. Contexto dos Arquivos Python (prompt):**

O prompt que voc√™ est√° enviando para o modelo Gemini inclui o c√≥digo fonte de v√°rios arquivos Python. Isso √© √∫til para o Gemini responder perguntas sobre seu c√≥digo, mas pode estar consumindo muitos tokens e potencialmente afetando o comportamento do modelo (e talvez at√© contribuindo para o erro de gera√ß√£o de imagem). Considere remover o c√≥digo fonte do prompt, a menos que seja absolutamente necess√°rio.

**8. Depend√™ncias:**

Certifique-se de que todas as depend√™ncias necess√°rias (google-generativeai, Pillow, etc.) estejam instaladas corretamente.

**9.  Rate Limiting:**

O c√≥digo possui um mecanismo de rate limiting, mas certifique-se de que ele esteja configurado corretamente e n√£o esteja bloqueando as requisi√ß√µes.  Aumente o tempo de espera para teste, s√≥ para ter certeza que o problema n√£o √© esse.

**10. Teste Simplificado:**

Crie um script de teste muito simples (fora do Streamlit) que apenas chame a fun√ß√£o `generate_image` com um prompt b√°sico e imprima os resultados. Isso ajudar√° a isolar o problema e determinar se ele est√° relacionado ao c√≥digo de gera√ß√£o de imagem em si ou √† integra√ß√£o com o Streamlit. O exemplo no final do arquivo `services\image_services.py` √© um bom come√ßo, mas adicione tratamento de erros mais detalhado e logging.

**Exemplo de c√≥digo de depura√ß√£o:**

```python
from services.image_services import generate_image

try:
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        print("Imagem gerada com sucesso!")
        image.show()  # Exibe a imagem (opcional)
        image.save("gato_astronauta.png")  # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")
except Exception as e:
    print(f"Erro geral: {e}")

```

Lembre-se de verificar os logs (tanto os logs do sistema quanto os logs adicionados no seu c√≥digo) para obter mais informa√ß√µes sobre a causa do erro.  Analise a sa√≠da da impress√£o dos tipos MIME para entender melhor o que est√° acontecendo.

2025-06-28 11:42:16,738 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:42:16,739 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:42:16,740 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:42:16,741 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:42:16,742 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:42:16,743 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:42:16,746 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:42:16,748 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:42:16,753 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:42:16,754 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:42:16,756 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:42:16,756 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:42:16,758 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:42:16,759 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:42:16,762 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:44:32,605 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:44:32,606 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:44:32,607 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:44:32,608 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:44:32,609 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:44:32,611 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:44:32,612 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:44:32,615 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:44:32,618 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:44:32,619 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:44:32,620 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:44:32,621 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:44:32,624 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:44:32,625 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:44:32,627 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:44:33,146 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:44:33,147 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:44:33,148 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:44:33,150 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:44:33,151 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:44:33,152 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:44:33,153 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:44:33,155 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:44:33,157 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:44:33,160 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:44:33,166 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:44:33,167 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:44:33,169 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:44:33,170 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:44:33,171 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:44:33,180 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")



=============

Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000018E29EB7B50>

erro na minha aplica√ß√£o conforme c√≥digos anexos
Assistant: O erro "cannot identify image file <_io.BytesIO object at...>" indica que a fun√ß√£o `Image.open()` do PIL (Pillow) n√£o conseguiu identificar o formato da imagem dentro do objeto `BytesIO`. Isso geralmente acontece quando os dados dentro do `BytesIO` n√£o s√£o dados de imagem v√°lidos ou o PIL n√£o consegue determinar o formato.

Analisando o c√≥digo, o problema mais prov√°vel reside na fun√ß√£o `generate_image` em `services\image_services.py` e, consequentemente, no uso dessa fun√ß√£o no script `chat_streamlit.py`.

Aqui est√£o os pontos a serem revisados e as poss√≠veis solu√ß√µes:

**1. Confirma√ß√£o do Tipo MIME (services\image_services.py):**

Certifique-se de que o `part.mime_type` seja realmente 'image/png' (ou outro tipo de imagem v√°lido como 'image/jpeg') antes de tentar abrir com `Image.open()`.  Se o tipo MIME for diferente, trate-o adequadamente ou ignore a parte.

```python
def generate_image(prompt: str) -> Image.Image | None:
    # ... (c√≥digo anterior) ...
    if response.parts:
        for part in response.parts:
            if part.mime_type == 'image/png' or part.mime_type == 'image/jpeg':  # Adicionado suporte a JPEG
                try:
                    return Image.open(BytesIO(part.data))
                except Exception as e:
                    print(f"Erro ao abrir imagem do BytesIO: {e}")
                    return None
            else:
                print(f"Parte ignorada, tipo MIME: {part.mime_type}") # Debugging
    # ... (c√≥digo posterior) ...
```

**2. Tratamento de Erros ao Abrir a Imagem (services\image_services.py):**

Adicione um bloco `try...except` dentro do loop `for part in response.parts:` para capturar erros que podem ocorrer ao tentar abrir a imagem com `Image.open()`. Isso pode fornecer informa√ß√µes mais espec√≠ficas sobre o motivo da falha.  A impress√£o do `part.mime_type` ajudar√° a entender se o problema √© um tipo mime inesperado.

**3. Valida√ß√£o dos Dados da Imagem (services\image_services.py):**

Antes de usar `Image.open()`, verifique se `part.data` cont√©m dados.  Se estiver vazio, n√£o tente abrir.

```python
if part.inline_data is not None:
    if part.inline_data.data:  # Verifica se h√° dados
        image = Image.open(BytesIO(part.inline_data.data))
        st.session_state.generated_image = image
        return image
    else:
        print("Dados da imagem vazios!") # Debugging
```

**4. Chame a fun√ß√£o generate_image corretamente no Streamlit (chat_streamlit.py):**

Certifique-se que `st.session_state.generated_image` seja None no in√≠cio da fun√ß√£o `generate_image`, para evitar estados incorretos.

**5. API Key e Modelo:**

Embora o c√≥digo verifique se a API Key est√° definida, verifique se ela √© v√°lida e tem permiss√£o para usar o modelo `gemini-2.0-flash-exp-image-generation`.  A mensagem de erro pode ser gen√©rica e esconder um problema de autentica√ß√£o ou permiss√£o.

**6.  Informa√ß√µes Adicionais no Log:**

Adicione mais informa√ß√µes de log nos pontos cr√≠ticos do c√≥digo para facilitar a depura√ß√£o.  Por exemplo, registre o tamanho de `part.data` antes de tentar abrir a imagem.

```python
logger.info(f"Tamanho dos dados da imagem: {len(part.inline_data.data)}") # Log do tamanho dos dados
```

**7. Contexto dos Arquivos Python (prompt):**

O prompt que voc√™ est√° enviando para o modelo Gemini inclui o c√≥digo fonte de v√°rios arquivos Python. Isso √© √∫til para o Gemini responder perguntas sobre seu c√≥digo, mas pode estar consumindo muitos tokens e potencialmente afetando o comportamento do modelo (e talvez at√© contribuindo para o erro de gera√ß√£o de imagem). Considere remover o c√≥digo fonte do prompt, a menos que seja absolutamente necess√°rio.

**8. Depend√™ncias:**

Certifique-se de que todas as depend√™ncias necess√°rias (google-generativeai, Pillow, etc.) estejam instaladas corretamente.

**9.  Rate Limiting:**

O c√≥digo possui um mecanismo de rate limiting, mas certifique-se de que ele esteja configurado corretamente e n√£o esteja bloqueando as requisi√ß√µes.  Aumente o tempo de espera para teste, s√≥ para ter certeza que o problema n√£o √© esse.

**10. Teste Simplificado:**

Crie um script de teste muito simples (fora do Streamlit) que apenas chame a fun√ß√£o `generate_image` com um prompt b√°sico e imprima os resultados. Isso ajudar√° a isolar o problema e determinar se ele est√° relacionado ao c√≥digo de gera√ß√£o de imagem em si ou √† integra√ß√£o com o Streamlit. O exemplo no final do arquivo `services\image_services.py` √© um bom come√ßo, mas adicione tratamento de erros mais detalhado e logging.

**Exemplo de c√≥digo de depura√ß√£o:**

```python
from services.image_services import generate_image

try:
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        print("Imagem gerada com sucesso!")
        image.show()  # Exibe a imagem (opcional)
        image.save("gato_astronauta.png")  # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")
except Exception as e:
    print(f"Erro geral: {e}")

```

Lembre-se de verificar os logs (tanto os logs do sistema quanto os logs adicionados no seu c√≥digo) para obter mais informa√ß√µes sobre a causa do erro.  Analise a sa√≠da da impress√£o dos tipos MIME para entender melhor o que est√° acontecendo.

User: preciso instalar pilow com pip ?
2025-06-28 11:44:36,189 [INFO] Resposta da IA (texto): Assistant: Sim, voc√™ precisa instalar a biblioteca Pillow (PIL) usando o pip, caso ainda n√£o a tenha instalada. Pillow √© a vers√£o moderna e ativa do PIL (Python Imaging Library), que √© necess√°ria para manipular imagens em Python.

Abra seu terminal ou prompt de comando e execute o seguinte comando:

```bash
pip install Pillow
```

Este comando ir√° baixar e instalar a vers√£o mais recente da Pillow no seu ambiente Python. Ap√≥s a instala√ß√£o, voc√™ poder√° importar e usar a biblioteca Pillow no seu c√≥digo sem problemas.

Certifique-se de que voc√™ est√° executando o `pip` dentro do mesmo ambiente Python que voc√™ est√° usando para executar seu script Streamlit.  Se voc√™ estiver usando um ambiente virtual (venv), ative-o antes de executar o `pip install Pillow`.

2025-06-28 11:44:36,394 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:44:36,396 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:44:36,398 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:44:36,399 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:44:36,401 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:44:36,403 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:44:36,404 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:44:36,405 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:44:36,416 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:44:36,417 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:44:36,418 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:44:36,418 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:44:36,419 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:44:36,420 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:44:36,420 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:48:37,002 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:48:37,003 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:48:37,005 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:48:37,006 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:48:37,007 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:48:37,008 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:48:37,009 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:48:37,011 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:48:37,013 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:48:37,014 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:48:37,015 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:48:37,016 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:48:37,017 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:48:37,018 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:48:37,020 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:48:37,184 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:48:37,185 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:48:37,187 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:48:37,189 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:48:37,190 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:48:37,192 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:48:37,193 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:48:37,194 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:48:37,195 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:48:37,197 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:48:37,200 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:48:37,201 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:48:37,203 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:48:37,204 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:48:37,205 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:48:37,210 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")



=============

Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000018E29EB7B50>

erro na minha aplica√ß√£o conforme c√≥digos anexos
Assistant: O erro "cannot identify image file <_io.BytesIO object at...>" indica que a fun√ß√£o `Image.open()` do PIL (Pillow) n√£o conseguiu identificar o formato da imagem dentro do objeto `BytesIO`. Isso geralmente acontece quando os dados dentro do `BytesIO` n√£o s√£o dados de imagem v√°lidos ou o PIL n√£o consegue determinar o formato.

Analisando o c√≥digo, o problema mais prov√°vel reside na fun√ß√£o `generate_image` em `services\image_services.py` e, consequentemente, no uso dessa fun√ß√£o no script `chat_streamlit.py`.

Aqui est√£o os pontos a serem revisados e as poss√≠veis solu√ß√µes:

**1. Confirma√ß√£o do Tipo MIME (services\image_services.py):**

Certifique-se de que o `part.mime_type` seja realmente 'image/png' (ou outro tipo de imagem v√°lido como 'image/jpeg') antes de tentar abrir com `Image.open()`.  Se o tipo MIME for diferente, trate-o adequadamente ou ignore a parte.

```python
def generate_image(prompt: str) -> Image.Image | None:
    # ... (c√≥digo anterior) ...
    if response.parts:
        for part in response.parts:
            if part.mime_type == 'image/png' or part.mime_type == 'image/jpeg':  # Adicionado suporte a JPEG
                try:
                    return Image.open(BytesIO(part.data))
                except Exception as e:
                    print(f"Erro ao abrir imagem do BytesIO: {e}")
                    return None
            else:
                print(f"Parte ignorada, tipo MIME: {part.mime_type}") # Debugging
    # ... (c√≥digo posterior) ...
```

**2. Tratamento de Erros ao Abrir a Imagem (services\image_services.py):**

Adicione um bloco `try...except` dentro do loop `for part in response.parts:` para capturar erros que podem ocorrer ao tentar abrir a imagem com `Image.open()`. Isso pode fornecer informa√ß√µes mais espec√≠ficas sobre o motivo da falha.  A impress√£o do `part.mime_type` ajudar√° a entender se o problema √© um tipo mime inesperado.

**3. Valida√ß√£o dos Dados da Imagem (services\image_services.py):**

Antes de usar `Image.open()`, verifique se `part.data` cont√©m dados.  Se estiver vazio, n√£o tente abrir.

```python
if part.inline_data is not None:
    if part.inline_data.data:  # Verifica se h√° dados
        image = Image.open(BytesIO(part.inline_data.data))
        st.session_state.generated_image = image
        return image
    else:
        print("Dados da imagem vazios!") # Debugging
```

**4. Chame a fun√ß√£o generate_image corretamente no Streamlit (chat_streamlit.py):**

Certifique-se que `st.session_state.generated_image` seja None no in√≠cio da fun√ß√£o `generate_image`, para evitar estados incorretos.

**5. API Key e Modelo:**

Embora o c√≥digo verifique se a API Key est√° definida, verifique se ela √© v√°lida e tem permiss√£o para usar o modelo `gemini-2.0-flash-exp-image-generation`.  A mensagem de erro pode ser gen√©rica e esconder um problema de autentica√ß√£o ou permiss√£o.

**6.  Informa√ß√µes Adicionais no Log:**

Adicione mais informa√ß√µes de log nos pontos cr√≠ticos do c√≥digo para facilitar a depura√ß√£o.  Por exemplo, registre o tamanho de `part.data` antes de tentar abrir a imagem.

```python
logger.info(f"Tamanho dos dados da imagem: {len(part.inline_data.data)}") # Log do tamanho dos dados
```

**7. Contexto dos Arquivos Python (prompt):**

O prompt que voc√™ est√° enviando para o modelo Gemini inclui o c√≥digo fonte de v√°rios arquivos Python. Isso √© √∫til para o Gemini responder perguntas sobre seu c√≥digo, mas pode estar consumindo muitos tokens e potencialmente afetando o comportamento do modelo (e talvez at√© contribuindo para o erro de gera√ß√£o de imagem). Considere remover o c√≥digo fonte do prompt, a menos que seja absolutamente necess√°rio.

**8. Depend√™ncias:**

Certifique-se de que todas as depend√™ncias necess√°rias (google-generativeai, Pillow, etc.) estejam instaladas corretamente.

**9.  Rate Limiting:**

O c√≥digo possui um mecanismo de rate limiting, mas certifique-se de que ele esteja configurado corretamente e n√£o esteja bloqueando as requisi√ß√µes.  Aumente o tempo de espera para teste, s√≥ para ter certeza que o problema n√£o √© esse.

**10. Teste Simplificado:**

Crie um script de teste muito simples (fora do Streamlit) que apenas chame a fun√ß√£o `generate_image` com um prompt b√°sico e imprima os resultados. Isso ajudar√° a isolar o problema e determinar se ele est√° relacionado ao c√≥digo de gera√ß√£o de imagem em si ou √† integra√ß√£o com o Streamlit. O exemplo no final do arquivo `services\image_services.py` √© um bom come√ßo, mas adicione tratamento de erros mais detalhado e logging.

**Exemplo de c√≥digo de depura√ß√£o:**

```python
from services.image_services import generate_image

try:
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        print("Imagem gerada com sucesso!")
        image.show()  # Exibe a imagem (opcional)
        image.save("gato_astronauta.png")  # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")
except Exception as e:
    print(f"Erro geral: {e}")

```

Lembre-se de verificar os logs (tanto os logs do sistema quanto os logs adicionados no seu c√≥digo) para obter mais informa√ß√µes sobre a causa do erro.  Analise a sa√≠da da impress√£o dos tipos MIME para entender melhor o que est√° acontecendo.

User: preciso instalar pilow com pip ?
Assistant: Assistant: Sim, voc√™ precisa instalar a biblioteca Pillow (PIL) usando o pip, caso ainda n√£o a tenha instalada. Pillow √© a vers√£o moderna e ativa do PIL (Python Imaging Library), que √© necess√°ria para manipular imagens em Python.

Abra seu terminal ou prompt de comando e execute o seguinte comando:

```bash
pip install Pillow
```

Este comando ir√° baixar e instalar a vers√£o mais recente da Pillow no seu ambiente Python. Ap√≥s a instala√ß√£o, voc√™ poder√° importar e usar a biblioteca Pillow no seu c√≥digo sem problemas.

Certifique-se de que voc√™ est√° executando o `pip` dentro do mesmo ambiente Python que voc√™ est√° usando para executar seu script Streamlit.  Se voc√™ estiver usando um ambiente virtual (venv), ative-o antes de executar o `pip install Pillow`.

User: enviar os arquivos que devem ser alterados completos.
2025-06-28 11:49:05,114 [INFO] Resposta da IA (texto): Assistant: Ok, aqui est√£o os arquivos `services\image_services.py` e `chat_streamlit.py` completos com as altera√ß√µes sugeridas.  Eu combinei as mudan√ßas de tratamento de erros e logging para maior clareza.

**services\image_services.py**

```python
import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO
import logging
from core.logger_config import logger  # Importe o logger

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            logger.warning(f"Prompt Feedback Reason: {response.prompt_feedback.block_reason}")

        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                logger.info(f"Tipo MIME da parte: {part.mime_type}")  # Log do tipo MIME
                if part.mime_type == 'image/png' or part.mime_type == 'image/jpeg':
                    try:
                        image_data = part.inline_data.data if hasattr(part, 'inline_data') and part.inline_data else part.data

                        if not image_data:
                            logger.warning("Dados da imagem vazios!")
                            continue  # Pula para a pr√≥xima parte

                        logger.info(f"Tamanho dos dados da imagem: {len(image_data)}")  # Log do tamanho dos dados
                        image = Image.open(BytesIO(image_data))
                        logger.info("Imagem aberta com sucesso!")
                        return image
                    except Exception as e:
                        logger.error(f"Erro ao abrir imagem do BytesIO: {e}", exc_info=True)
                        continue  # Tenta a pr√≥xima parte
                else:
                    logger.warning(f"Parte ignorada, tipo MIME: {part.mime_type}")  # Debugging
        logger.info(f"Resposta da IA (texto): {response.text}")  # Log da resposta de texto
        return None
    except Exception as e:
        logger.error(f"Erro ao gerar imagem: {e}", exc_info=True)
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    try:
        image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
        if image:
            print("Imagem gerada com sucesso!")
            image.show() # Exibe a imagem (opcional)
            image.save("gato_astronauta.png") # Salva a imagem (opcional)
        else:
            print("Falha ao gerar a imagem.")
    except Exception as e:
        print(f"Erro geral no exemplo de uso: {e}")
```

**chat_streamlit.py**

```python
import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python
from services.image_services import generate_image  # Importa a fun√ß√£o generate_image
from core.logger_config import logger  # Importe o logger

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 20

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image_wrapper(prompt):
    try:
        st.session_state.generated_image = None # Limpa para evitar estados incorretos
        image = generate_image(prompt)

        if image:
            return image
        else:
            st.error("Falha ao gerar a imagem (verifique os logs).") # Mensagem mais clara
            return None
    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        logger.error(f"Erro ao gerar imagem: {e}", exc_info=True) # Log completo
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):
        if image_prompt:
            generated_image = generate_image_wrapper(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)
```

**Principais Mudan√ßas e Explica√ß√µes:**

*   **Logging:** Adicionei logging mais detalhado em `services\image_services.py` para ajudar a identificar exatamente onde a falha est√° ocorrendo.  Usei o `logger` do seu projeto, configurado em `core\logger_config.py`.
*   **Tratamento de `inline_data` e `part.data`:**  O c√≥digo agora verifica se `part.inline_data` existe antes de tentar acessar `part.inline_data.data`.  Tamb√©m, caso `part.inline_data` n√£o exista, ele tenta usar `part.data` diretamente.
*   **Verifica√ß√£o do Tipo MIME:** O c√≥digo agora verifica explicitamente se o `mime_type` √© `'image/png'` ou `'image/jpeg'` antes de tentar abrir a imagem. Ele tamb√©m imprime um aviso se o tipo MIME for diferente.
*   **Verifica√ß√£o de Dados Vazios:** O c√≥digo agora verifica se `image_data` (anteriormente `part.inline_data.data`) n√£o est√° vazio antes de tentar abrir a imagem.
*   **Tratamento de Erros:**  Adicionei blocos `try...except` para capturar erros que podem ocorrer ao abrir a imagem com `Image.open()` e registrar esses erros.
*   **Fun√ß√£o Wrapper para `generate_image`:** Criei uma fun√ß√£o `generate_image_wrapper` em `chat_streamlit.py` para lidar com erros que podem ocorrer ao chamar a fun√ß√£o `generate_image` e para garantir que o `st.session_state.generated_image` seja limpo corretamente.
*   **Manipula√ß√£o de Erros no Streamlit:** A mensagem de erro no Streamlit foi aprimorada para fornecer informa√ß√µes mais √∫teis ao usu√°rio.
*   **Log Completo em Caso de Erro:** O `logger.error` agora inclui `exc_info=True` para registrar o stack trace completo em caso de erro.
*   **Limpeza do Estado do Streamlit:** O `st.session_state.generated_image` √© explicitamente definido como `None` antes de chamar a fun√ß√£o `generate_image`, para evitar estados incorretos.
*   **Importa√ß√£o de `generate_image`:** Certifique-se de importar a fun√ß√£o `generate_image` de `services\image_services.py` no `chat_streamlit.py`.

**Como Usar:**

1.  Substitua o conte√∫do dos arquivos `services\image_services.py` e `chat_streamlit.py` pelo c√≥digo acima.
2.  Execute sua aplica√ß√£o Streamlit.
3.  Se ocorrerem erros, verifique os logs do seu aplicativo (os logs ser√£o gravados em um arquivo e tamb√©m exibidos no console) para obter mais informa√ß√µes sobre a causa do problema.
4.  Analise a sa√≠da da impress√£o dos tipos MIME para entender melhor o que est√° acontecendo.

Lembre-se de que essas mudan√ßas adicionam mais tratamento de erros e logging para ajudar a diagnosticar o problema. Se o problema persistir, a an√°lise dos logs fornecer√° informa√ß√µes cruciais para a resolu√ß√£o.

2025-06-28 11:49:05,341 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:49:05,346 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:49:05,348 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:49:05,350 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:49:05,351 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:49:05,353 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:49:05,358 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:49:05,360 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:49:05,361 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:49:05,362 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:49:05,364 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:49:05,366 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:49:05,367 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:49:05,368 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:49:05,368 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:51:31,967 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:51:31,968 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:51:31,969 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:51:31,969 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:51:31,970 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:51:31,971 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:51:31,972 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:51:31,973 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:51:31,975 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:51:31,975 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:51:31,976 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:51:31,977 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:51:31,978 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:51:31,979 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:51:31,979 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:51:39,659 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:51:39,662 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:51:39,663 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:51:39,665 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:51:39,666 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:51:39,668 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:51:39,669 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:51:39,671 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:51:39,672 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:51:39,674 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:51:39,677 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:51:39,678 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:51:39,680 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:51:39,682 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:51:39,684 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:51:40,093 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:51:40,094 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:51:40,095 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:51:40,096 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:51:40,097 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:51:40,098 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:51:40,099 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:51:40,102 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:51:40,103 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:51:40,104 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:51:40,107 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:51:40,108 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:51:40,111 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:51:40,112 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:51:40,113 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:51:40,946 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 11:51:45,794 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 11:51:46,983 [INFO] AFC remote call 1 is done.
2025-06-28 11:53:03,536 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:53:03,540 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:53:03,548 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:53:03,560 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:53:03,565 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:53:03,634 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:53:03,735 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:53:03,817 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:53:03,823 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:53:03,828 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:53:03,830 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:53:03,832 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:53:03,834 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:53:03,836 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:53:03,837 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:54:02,532 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:54:02,537 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:54:02,539 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:54:02,540 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:54:02,541 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:54:02,542 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:54:02,545 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:54:02,547 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:54:02,548 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:54:02,551 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:54:02,553 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:54:02,555 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:54:02,556 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:54:02,557 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:54:02,559 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:54:02,857 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:54:02,858 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:54:02,859 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:54:02,861 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:54:02,862 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:54:02,863 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:54:02,865 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:54:02,869 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:54:02,871 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:54:02,873 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:54:02,874 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:54:02,875 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:54:02,876 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:54:02,878 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:54:02,879 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:54:03,845 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 11:54:14,048 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 11:54:15,147 [INFO] AFC remote call 1 is done.
2025-06-28 11:55:53,157 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:55:53,159 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:55:53,160 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:55:53,161 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:55:53,162 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:55:53,162 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:55:53,163 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:55:53,164 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:55:53,165 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:55:53,166 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:55:53,167 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:55:53,168 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:55:53,169 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:55:53,170 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:55:53,171 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:55:53,956 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 11:56:00,448 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 11:56:01,460 [INFO] AFC remote call 1 is done.
2025-06-28 11:56:58,340 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:56:58,347 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:56:58,357 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:56:58,368 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:56:58,369 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:56:58,370 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:56:58,373 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:56:58,376 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:56:58,379 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:56:58,380 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:56:58,387 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:56:58,388 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:56:58,390 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:56:58,395 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:56:58,399 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:57:54,778 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:57:54,779 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:57:54,780 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:57:54,781 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:57:54,783 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:57:54,784 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:57:54,786 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:57:54,787 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:57:54,789 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:57:54,790 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:57:54,792 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:57:54,793 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:57:54,794 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:57:54,795 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:57:54,796 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:58:02,898 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:58:02,903 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:58:02,905 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:58:02,906 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:58:02,908 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:58:02,909 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:58:02,910 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:58:02,912 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:58:02,918 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:58:02,919 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:58:02,920 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:58:02,922 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:58:02,923 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:58:02,925 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:58:02,928 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:58:24,356 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:58:24,360 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:58:24,361 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:58:24,363 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:58:24,364 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:58:24,366 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:58:24,367 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:58:24,368 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:58:24,369 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:58:24,371 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:58:24,373 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:58:24,374 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:58:24,376 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:58:24,379 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:58:24,380 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:58:24,565 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:58:24,569 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:58:24,572 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:58:24,574 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:58:24,578 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:58:24,582 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:58:24,584 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:58:24,586 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:58:24,587 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:58:24,589 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:58:24,590 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:58:24,590 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:58:24,594 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:58:24,598 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:58:24,600 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:58:24,606 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: TypeError: ImageMixin.image() got an unexpected keyword argument 'use_container_width'
Traceback:
File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 589, in _run_script
    exec(code, module.__dict__)
File "C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\chat_streamlit.py", line 322, in <module>
    st.image(st.session_state.uploaded_image, use_container_width=True)
File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\metrics_util.py", line 408, in wrapped_func
    result = non_optional_func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

porque tive esse erro ap√≥s inserir uma imagem da minha m√°quina nesse aplica√ß√£o ?
2025-06-28 11:58:32,589 [INFO] Resposta da IA (texto): O erro `TypeError: ImageMixin.image() got an unexpected keyword argument 'use_container_width'` indica que a fun√ß√£o `st.image()` na vers√£o do Streamlit que voc√™ est√° usando n√£o aceita o argumento `use_container_width`. Esse argumento foi introduzido em vers√µes mais recentes do Streamlit.

Aqui est√£o os passos para resolver o problema:

1.  **Verifique a vers√£o do Streamlit:**

    *   Abra um terminal ou prompt de comando.
    *   Execute o comando: `pip show streamlit` ou `conda list streamlit`.
    *   Verifique a vers√£o instalada.

2.  **Atualize o Streamlit (se necess√°rio):**

    *   Se a sua vers√£o for antiga, atualize para a vers√£o mais recente: `pip install --upgrade streamlit` ou `conda update streamlit`.
    *   Ap√≥s a atualiza√ß√£o, verifique a vers√£o novamente para confirmar a atualiza√ß√£o.

3.  **Se a atualiza√ß√£o n√£o resolver (vers√£o j√° atualizada ou impossibilidade de atualizar):**

    *   Remova o argumento `use_container_width` das chamadas para `st.image()`. A imagem ainda ser√° exibida, mas pode n√£o ocupar toda a largura do cont√™iner.
    *   Use CSS personalizado para estilizar a imagem para ocupar a largura do cont√™iner (op√ß√£o mais avan√ßada).

**Corre√ß√µes no seu c√≥digo:**

Remova `use_container_width=True` de todas as chamadas `st.image()`.  Especifique a largura diretamente.  Por exemplo, use `width=None` para permitir que a imagem dimensione automaticamente ou defina uma largura fixa em pixels.

```python
# src\chat_streamlit.py

# ...

# Exibe a imagem selecionada na barra lateral
if st.session_state.clipboard_image_preview:
    st.image(st.session_state.clipboard_image_preview, width=None)
    st.caption("Imagem da √°rea de transfer√™ncia")
elif st.session_state.uploaded_image:
    st.image(st.session_state.uploaded_image, width=None)
    st.caption("Imagem carregada")

# ...

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], width=None)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# ...
```

**Explica√ß√£o:**

O erro ocorre porque voc√™ est√° passando um argumento (`use_container_width`) que n√£o √© suportado pela fun√ß√£o `st.image()` na vers√£o do Streamlit que voc√™ est√° usando.  Remover o argumento √© a maneira mais direta de resolver o problema, a menos que voc√™ precise desesperadamente que a imagem ocupe toda a largura do cont√™iner e n√£o possa atualizar o Streamlit.

Se remover o argumento causar problemas de layout, voc√™ pode usar CSS personalizado para ajustar o tamanho da imagem. Isso requer um pouco mais de conhecimento de HTML e CSS, mas oferece mais flexibilidade.  Exemplo:

```python
# src\chat_streamlit.py

import streamlit as st

# ...

if st.session_state.uploaded_image:
    st.image(st.session_state.uploaded_image, width=200)  # Largura em pixels

    # CSS para fazer a imagem ocupar a largura do cont√™iner
    st.markdown(
        """
        <style>
        [data-testid="stImage"] {
            width: 100%; /* Faz a imagem ocupar toda a largura do cont√™iner */
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

# ...
```

**Resumo:**

A solu√ß√£o mais simples √© atualizar o Streamlit. Se isso n√£o for poss√≠vel, remova o argumento `use_container_width` e ajuste o layout da imagem se necess√°rio.  Se voc√™ precisa que a imagem ocupe a largura total do cont√™iner e n√£o pode atualizar o Streamlit, use CSS personalizado.

2025-06-28 11:58:32,747 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:58:32,748 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:58:32,749 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:58:32,750 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:58:32,751 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:58:32,752 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:58:32,753 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:58:32,754 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:58:32,756 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:58:32,757 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:58:32,759 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:58:32,760 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:58:32,761 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:58:32,762 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:58:32,763 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:03:07,402 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:03:07,403 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:03:07,405 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:03:07,406 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:03:07,407 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:03:07,408 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:03:07,410 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:03:07,412 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:03:07,413 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:03:07,418 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:03:07,420 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:03:07,423 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:03:07,424 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:03:07,425 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:03:07,426 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:03:13,181 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:03:13,182 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:03:13,183 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:03:13,184 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:03:13,185 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:03:13,186 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:03:13,187 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:03:13,190 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:03:13,192 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:03:13,194 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:03:13,195 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:03:13,197 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:03:13,198 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:03:13,199 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:03:13,200 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:03:16,861 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:03:16,864 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:03:16,865 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:03:16,867 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:03:16,871 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:03:16,873 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:03:16,877 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:03:16,878 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:03:16,883 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:03:16,885 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:03:16,887 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:03:16,893 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:03:16,896 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:03:16,897 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:03:16,899 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:03:45,255 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:03:45,256 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:03:45,257 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:03:45,258 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:03:45,260 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:03:45,261 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:03:45,263 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:03:45,264 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:03:45,266 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:03:45,267 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:03:45,269 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:03:45,271 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:03:45,272 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:03:45,272 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:03:45,273 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:03:45,566 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:03:45,568 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:03:45,569 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:03:45,570 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:03:45,571 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:03:45,572 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:03:45,574 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:03:45,578 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:03:45,582 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:03:45,584 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:03:45,585 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:03:45,586 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:03:45,587 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:03:45,588 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:03:45,589 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:12:42,984 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:12:42,988 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:12:42,991 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:12:42,995 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:12:43,001 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:12:43,004 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:12:43,008 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:12:43,011 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:12:43,018 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:12:43,021 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:12:43,022 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:12:43,026 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:12:43,030 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:12:43,035 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:12:43,039 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:12:48,632 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:12:48,635 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:12:48,636 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:12:48,638 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:12:48,640 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:12:48,649 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:12:48,650 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:12:48,653 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:12:48,655 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:12:48,658 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:12:48,660 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:12:48,663 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:12:48,664 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:12:48,666 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:12:48,669 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:12:52,905 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:12:52,913 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:12:52,969 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:12:52,983 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:12:53,011 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:12:53,030 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:12:53,050 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:12:53,068 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:12:53,081 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:12:53,086 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:12:53,103 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:12:53,115 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:12:53,117 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:12:53,118 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:12:53,120 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:12:58,800 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:12:58,801 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:12:58,802 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:12:58,804 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:12:58,805 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:12:58,806 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:12:58,808 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:12:58,809 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:12:58,810 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:12:58,811 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:12:58,812 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:12:58,817 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:12:58,818 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:12:58,819 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:12:58,819 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:12:59,085 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:12:59,086 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:12:59,087 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:12:59,088 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:12:59,090 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:12:59,091 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:12:59,093 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:12:59,094 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:12:59,095 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:12:59,096 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:12:59,102 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:12:59,103 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:12:59,104 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:12:59,105 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:12:59,106 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:12:59,857 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 12:13:05,615 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 12:13:06,634 [INFO] AFC remote call 1 is done.
2025-06-28 12:13:19,107 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:13:19,111 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:13:19,114 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:13:19,115 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:13:19,117 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:13:19,118 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:13:19,120 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:13:19,121 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:13:19,123 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:13:19,125 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:13:19,126 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:13:19,126 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:13:19,128 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:13:19,129 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:13:19,132 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:13:41,236 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:13:41,238 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:13:41,239 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:13:41,240 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:13:41,241 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:13:41,243 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:13:41,246 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:13:41,247 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:13:41,249 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:13:41,250 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:13:41,252 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:13:41,253 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:13:41,254 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:13:41,255 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:13:41,259 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:13:41,265 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 12:13:44,482 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:13:44,483 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:13:44,484 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:13:44,487 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:13:44,491 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:13:44,493 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:13:44,495 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:13:44,500 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:13:44,501 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:13:44,503 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:13:44,506 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:13:44,507 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:13:44,508 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:13:44,509 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:13:44,510 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:13:45,944 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:13:45,945 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:13:45,946 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:13:45,947 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:13:45,948 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:13:45,950 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:13:45,951 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:13:45,954 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:13:45,956 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:13:45,958 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:13:45,959 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:13:45,960 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:13:45,962 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:13:45,963 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:13:45,969 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:13:46,201 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:13:46,203 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:13:46,204 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:13:46,205 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:13:46,207 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:13:46,210 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:13:46,212 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:13:46,214 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:13:46,216 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:13:46,220 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:13:46,231 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:13:46,232 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:13:46,234 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:13:46,235 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:13:46,241 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:13:51,005 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:13:51,021 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:13:51,022 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:13:51,025 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:13:51,026 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:13:51,036 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:13:51,047 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:13:51,050 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:13:51,067 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:13:51,071 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:13:51,073 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:13:51,077 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:13:51,081 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:13:51,083 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:13:51,085 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:14:15,990 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:14:15,992 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:14:15,993 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:14:15,994 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:14:15,996 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:14:15,998 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:14:15,999 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:14:16,001 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:14:16,002 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:14:16,003 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:14:16,004 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:14:16,007 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:14:16,008 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:14:16,009 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:14:16,010 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:14:16,218 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:14:16,219 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:14:16,220 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:14:16,224 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:14:16,225 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:14:16,228 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:14:16,229 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:14:16,232 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:14:16,238 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:14:16,245 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:14:16,249 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:14:16,252 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:14:16,253 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:14:16,254 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:14:16,258 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:14:16,261 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espa√ßo sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: TypeError: ImageMixin.image() got an unexpected keyword argument 'use_container_width'
Traceback:
File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 589, in _run_script
    exec(code, module.__dict__)
File "C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\chat_streamlit.py", line 322, in <module>
    st.image(st.session_state.uploaded_image, use_container_width=True)
File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\metrics_util.py", line 408, in wrapped_func
    result = non_optional_func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

porque tive esse erro ap√≥s inserir uma imagem da minha m√°quina nesse aplica√ß√£o ?
Assistant: O erro `TypeError: ImageMixin.image() got an unexpected keyword argument 'use_container_width'` indica que a fun√ß√£o `st.image()` na vers√£o do Streamlit que voc√™ est√° usando n√£o aceita o argumento `use_container_width`. Esse argumento foi introduzido em vers√µes mais recentes do Streamlit.

Aqui est√£o os passos para resolver o problema:

1.  **Verifique a vers√£o do Streamlit:**

    *   Abra um terminal ou prompt de comando.
    *   Execute o comando: `pip show streamlit` ou `conda list streamlit`.
    *   Verifique a vers√£o instalada.

2.  **Atualize o Streamlit (se necess√°rio):**

    *   Se a sua vers√£o for antiga, atualize para a vers√£o mais recente: `pip install --upgrade streamlit` ou `conda update streamlit`.
    *   Ap√≥s a atualiza√ß√£o, verifique a vers√£o novamente para confirmar a atualiza√ß√£o.

3.  **Se a atualiza√ß√£o n√£o resolver (vers√£o j√° atualizada ou impossibilidade de atualizar):**

    *   Remova o argumento `use_container_width` das chamadas para `st.image()`. A imagem ainda ser√° exibida, mas pode n√£o ocupar toda a largura do cont√™iner.
    *   Use CSS personalizado para estilizar a imagem para ocupar a largura do cont√™iner (op√ß√£o mais avan√ßada).

**Corre√ß√µes no seu c√≥digo:**

Remova `use_container_width=True` de todas as chamadas `st.image()`.  Especifique a largura diretamente.  Por exemplo, use `width=None` para permitir que a imagem dimensione automaticamente ou defina uma largura fixa em pixels.

```python
# src\chat_streamlit.py

# ...

# Exibe a imagem selecionada na barra lateral
if st.session_state.clipboard_image_preview:
    st.image(st.session_state.clipboard_image_preview, width=None)
    st.caption("Imagem da √°rea de transfer√™ncia")
elif st.session_state.uploaded_image:
    st.image(st.session_state.uploaded_image, width=None)
    st.caption("Imagem carregada")

# ...

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], width=None)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# ...
```

**Explica√ß√£o:**

O erro ocorre porque voc√™ est√° passando um argumento (`use_container_width`) que n√£o √© suportado pela fun√ß√£o `st.image()` na vers√£o do Streamlit que voc√™ est√° usando.  Remover o argumento √© a maneira mais direta de resolver o problema, a menos que voc√™ precise desesperadamente que a imagem ocupe toda a largura do cont√™iner e n√£o possa atualizar o Streamlit.

Se remover o argumento causar problemas de layout, voc√™ pode usar CSS personalizado para ajustar o tamanho da imagem. Isso requer um pouco mais de conhecimento de HTML e CSS, mas oferece mais flexibilidade.  Exemplo:

```python
# src\chat_streamlit.py

import streamlit as st

# ...

if st.session_state.uploaded_image:
    st.image(st.session_state.uploaded_image, width=200)  # Largura em pixels

    # CSS para fazer a imagem ocupar a largura do cont√™iner
    st.markdown(
        """
        <style>
        [data-testid="stImage"] {
            width: 100%; /* Faz a imagem ocupar toda a largura do cont√™iner */
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

# ...
```

**Resumo:**

A solu√ß√£o mais simples √© atualizar o Streamlit. Se isso n√£o for poss√≠vel, remova o argumento `use_container_width` e ajuste o layout da imagem se necess√°rio.  Se voc√™ precisa que a imagem ocupe a largura total do cont√™iner e n√£o pode atualizar o Streamlit, use CSS personalizado.

User: em qual arquivo devo alterar ?

TypeError: ImageMixin.image() got an unexpected keyword argument 'use_container_width'
Traceback:
File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 589, in _run_script
    exec(code, module.__dict__)
File "C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\chat_streamlit.py", line 322, in <module>
    st.image(st.session_state.uploaded_image, use_container_width=True)
File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\metrics_util.py", line 408, in wrapped_func
    result = non_optional_func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-06-28 12:14:25,912 [INFO] Resposta da IA (texto): O erro aponta diretamente para o arquivo e a linha onde a chamada problem√°tica est√° acontecendo:

Voc√™ deve alterar o arquivo:
`C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\chat_streamlit.py`

As linhas que precisam ser corrigidas s√£o as que utilizam `use_container_width=True`. No c√≥digo que voc√™ forneceu, encontrei tr√™s ocorr√™ncias:

1.  **Linha 319:**
    ```python
    st.image(st.session_state.clipboard_image_preview, use_container_width=True)
    ```
2.  **Linha 322:** (Esta √© a linha exata que o traceback est√° apontando)
    ```python
    st.image(st.session_state.uploaded_image, use_container_width=True)
    ```
3.  **Linha 333:**
    ```python
    st.image(message["image"], use_container_width=True)
    ```

**Como corrigir:**

Voc√™ precisa remover o argumento `use_container_width=True` e, se desejar, pode substitu√≠-lo por `width=None` (para que o Streamlit tente ajustar a largura automaticamente com base no conte√∫do) ou por um valor num√©rico em pixels (ex: `width=400`) para definir uma largura fixa.

**Altere essas linhas no `chat_streamlit.py`:**

```python
# ... (c√≥digo anterior)

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        # Altere esta linha:
        st.image(st.session_state.clipboard_image_preview, width=None) # Removido use_container_width=True
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        # Altere esta linha:
        st.image(st.session_state.uploaded_image, width=None) # Removido use_container_width=True
        st.caption("Imagem carregada")

    st.markdown("---")

# ... (c√≥digo intermedi√°rio)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            # Altere esta linha:
            st.image(message["image"], width=None) # Removido use_container_width=True
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# ... (c√≥digo posterior)
```

**Recomenda√ß√£o:**

Como mencionei antes, a melhor solu√ß√£o a longo prazo √© **atualizar sua vers√£o do Streamlit** para uma que suporte o argumento `use_container_width`. Isso geralmente resolve esses problemas de compatibilidade e permite que voc√™ use os recursos mais recentes da biblioteca.

Para atualizar, execute no seu terminal (com o ambiente virtual ativado, se estiver usando um):
`pip install --upgrade streamlit`
2025-06-28 12:14:26,081 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:14:26,082 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:14:26,083 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:14:26,084 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:14:26,085 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:14:26,086 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:14:26,087 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:14:26,089 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:14:26,090 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:14:26,092 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:14:26,093 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:14:26,094 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:14:26,096 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:14:26,097 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:14:26,098 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:17:02,053 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:17:02,054 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:17:02,055 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:17:02,056 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:17:02,058 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:17:02,071 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:17:02,072 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:17:02,073 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:17:02,084 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:17:02,086 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:17:02,089 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:17:02,093 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:17:02,164 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:17:02,217 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:17:02,256 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:17:11,805 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:17:11,806 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:17:11,807 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:17:11,808 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:17:11,809 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:17:11,811 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:17:11,813 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:17:11,815 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:17:11,823 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:17:11,825 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:17:11,826 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:17:11,828 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:17:11,831 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:17:11,836 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:17:11,839 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:17:39,425 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:17:39,428 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:17:39,429 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:17:39,430 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:17:39,433 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:17:39,435 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:17:39,436 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:17:39,437 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:17:39,443 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:17:39,444 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:17:39,445 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:17:39,446 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:17:39,447 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:17:39,449 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:17:39,450 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:17:45,340 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:17:45,343 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:17:45,345 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:17:45,346 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:17:45,352 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:17:45,354 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:17:45,357 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:17:45,377 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:17:45,386 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:17:45,388 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:17:45,392 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:17:45,394 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:17:45,396 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:17:45,397 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:17:45,398 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:18:54,156 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:18:54,163 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:18:54,173 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:18:54,178 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:18:54,183 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:18:54,188 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:18:54,191 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:18:54,195 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:18:54,204 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:18:54,209 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:18:54,212 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:18:54,219 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:18:54,223 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:18:54,225 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:18:54,226 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:18:59,176 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:18:59,180 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:18:59,181 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:18:59,182 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:18:59,183 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:18:59,184 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:18:59,187 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:18:59,189 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:18:59,192 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:18:59,199 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:18:59,200 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:18:59,201 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:18:59,202 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:18:59,203 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:18:59,205 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:19:07,776 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:19:07,778 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:19:07,779 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:19:07,780 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:19:07,782 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:19:07,783 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:19:07,784 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:19:07,789 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:19:07,791 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:19:07,792 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:19:07,797 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:19:07,799 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:19:07,809 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:19:07,811 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:19:07,813 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:19:24,445 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:19:24,447 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:19:24,448 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:19:24,452 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:19:24,453 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:19:24,455 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:19:24,457 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:19:24,462 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:19:24,464 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:19:24,465 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:19:24,467 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:19:24,467 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:19:24,469 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:19:24,472 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:19:24,473 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:19:26,650 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:19:26,651 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:19:26,652 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:19:26,654 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:19:26,655 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:19:26,656 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:19:26,657 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:19:26,660 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:19:26,662 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:19:26,664 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:19:26,666 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:19:26,667 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:19:26,668 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:19:26,669 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:19:26,672 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:08:16,207 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:08:16,208 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:08:16,209 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:08:16,210 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:08:16,210 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:08:16,211 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:08:16,212 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:08:16,213 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:08:16,214 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:08:16,216 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:08:16,218 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:08:16,222 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:08:16,225 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:08:16,228 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:08:16,229 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:08:17,161 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 13:08:39,356 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:08:39,357 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:08:39,359 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:08:39,360 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:08:39,361 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:08:39,362 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:08:39,368 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:08:39,371 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:08:39,374 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:08:39,377 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:08:39,378 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:08:39,380 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:08:39,381 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:08:39,387 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:08:39,401 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:08:39,643 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:08:39,644 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:08:39,645 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:08:39,647 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:08:39,656 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:08:39,658 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:08:39,664 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:08:39,675 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:08:39,677 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:08:39,679 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:08:39,685 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:08:39,691 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:08:39,692 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:08:39,694 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:08:39,697 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:08:41,118 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:08:44,997 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:08:46,088 [INFO] AFC remote call 1 is done.
2025-06-28 13:08:52,441 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:08:52,442 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:08:52,444 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:08:52,447 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:08:52,455 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:08:52,457 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:08:52,464 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:08:52,469 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:08:52,470 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:08:52,481 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:08:52,483 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:08:52,484 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:08:52,493 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:08:52,495 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:08:52,499 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:14:17,788 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:14:17,793 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:14:17,796 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:14:17,803 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:14:17,813 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:14:17,824 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:14:17,830 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:14:17,840 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:14:17,844 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:14:17,846 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:14:17,860 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:14:17,902 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:14:17,905 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:14:17,907 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:14:17,909 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:16:40,917 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:16:40,918 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:16:40,919 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:16:40,920 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:16:40,921 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:16:40,922 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:16:40,923 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:16:40,927 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:16:40,929 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:16:40,932 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:16:40,933 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:16:40,935 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:16:40,943 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:16:40,945 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:16:40,948 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:16:41,394 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:16:41,395 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:16:41,396 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:16:41,397 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:16:41,398 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:16:41,399 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:16:41,404 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:16:41,408 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:16:41,409 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:16:41,411 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:16:41,417 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:16:41,419 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:16:41,420 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:16:41,422 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:16:41,427 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:16:41,466 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque essa aplica√ß√£o apresenta esse erro 

2025-06-28 13:08:52.540 Uncaught app exception
Traceback (most recent call last):
  File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 589, in _run_script
    exec(code, module.__dict__)
  File "C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\chat_streamlit.py", line 322, in <module>
    st.image(st.session_state.uploaded_image, use_container_width=True)
  File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\metrics_util.py", line 408, in wrapped_func
    result = non_optional_func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: ImageMixin.image() got an unexpected keyword argument 'use_container_width'


por√©m eu programei ela em outra m√°quina e s√≥ fiz o git clone do reposit√≥rio atualizado.
2025-06-28 13:16:57,566 [INFO] Resposta da IA (texto): O erro que voc√™ est√° enfrentando, `TypeError: ImageMixin.image() got an unexpected keyword argument 'use_container_width'`, indica um problema de **compatibilidade de vers√£o** da biblioteca `streamlit`.

**Entendendo o Erro:**

*   **`ImageMixin.image()`** refere-se √† fun√ß√£o `st.image()`.
*   **`got an unexpected keyword argument 'use_container_width'`** significa que a vers√£o do Streamlit instalada na sua nova m√°quina n√£o reconhece ou suporta o argumento `use_container_width=True` que est√° sendo usado na linha:
    ```python
    st.image(st.session_state.uploaded_image, use_container_width=True)
    ```
    Este argumento foi introduzido em vers√µes mais recentes do Streamlit (a partir da vers√£o 1.10.0, se n√£o me engano). A m√°quina onde voc√™ programou inicialmente provavelmente tem uma vers√£o mais nova, enquanto a m√°quina onde voc√™ clonou o reposit√≥rio tem uma vers√£o mais antiga do Streamlit.

**Causa Principal:**

*   **Vers√µes diferentes do Streamlit:** Embora o c√≥digo seja o mesmo (clonado via Git), o ambiente de execu√ß√£o (as bibliotecas Python instaladas) n√£o √©. A vers√£o do Streamlit na segunda m√°quina √© mais antiga e n√£o inclui a funcionalidade `use_container_width`.

**Como Resolver:**

A solu√ß√£o mais direta e recomendada √© atualizar o Streamlit na m√°quina onde o erro ocorre.

1.  **Atualizar o Streamlit:**
    Abra o terminal ou prompt de comando na m√°quina onde o erro est√° acontecendo e execute:
    ```bash
    pip install --upgrade streamlit
    ```
    Isso instalar√° a vers√£o mais recente do Streamlit, que certamente suportar√° `use_container_width`.

2.  **Verificar a Vers√£o (Opcional):**
    Para confirmar as vers√µes, voc√™ pode executar em ambas as m√°quinas:
    ```bash
    pip show streamlit
    ```
    Voc√™ ver√° a vers√£o na sa√≠da. A m√°quina que funciona deve ter uma vers√£o 1.10.0 ou superior.

**Alternativas (Menos recomendadas, mas poss√≠veis):**

Se por algum motivo voc√™ n√£o puder atualizar o Streamlit (o que √© raro e desaconselhado para este tipo de problema), voc√™ teria que adaptar o c√≥digo:

*   **Remover o argumento:**
    ```python
    st.image(st.session_state.uploaded_image) # A imagem pode n√£o preencher a largura total
    ```
*   **Usar um `width` fixo (em pixels):**
    ```python
    st.image(st.session_state.uploaded_image, width=500) # Exemplo: 500 pixels de largura
    ```
    No entanto, `use_container_width=True` √© geralmente a op√ß√£o preferida por ser responsiva e se ajustar automaticamente ao layout do Streamlit.

**Melhores Pr√°ticas para Prevenir Problemas Semelhantes no Futuro:**

Este √© um problema cl√°ssico de "funciona na minha m√°quina". Para evitar isso, adote as seguintes pr√°ticas:

1.  **Use Ambientes Virtuais (Virtual Environments):**
    Sempre crie e ative um ambiente virtual para cada projeto. Isso isola as depend√™ncias do projeto do seu ambiente global Python.
    ```bash
    # Na pasta do seu projeto
    python -m venv venv
    # No Windows
    .\venv\Scripts\activate
    # No macOS/Linux
    source venv/bin/activate
    ```
    Depois de ativar, instale todas as suas depend√™ncias dentro deste ambiente.

2.  **Crie um `requirements.txt`:**
    Depois de instalar todas as bibliotecas necess√°rias e ter certeza de que o projeto funciona, gere um arquivo `requirements.txt` que lista todas as depend√™ncias e suas vers√µes exatas:
    ```bash
    pip freeze > requirements.txt
    ```
    Este arquivo deve ser commitado no seu reposit√≥rio Git.

3.  **Instale Depend√™ncias a Partir do `requirements.txt`:**
    Quando voc√™ clonar o reposit√≥rio em uma nova m√°quina (e tiver ativado seu ambiente virtual), basta executar:
    ```bash
    pip install -r requirements.txt
    ```
    Isso garantir√° que exatamente as mesmas vers√µes de todas as bibliotecas sejam instaladas, replicando seu ambiente de desenvolvimento.

Ao seguir essas pr√°ticas, voc√™ garante que o ambiente de execu√ß√£o seja consistente em todas as m√°quinas, eliminando problemas como o que voc√™ encontrou.
2025-06-28 13:16:57,777 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:16:57,779 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:16:57,780 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:16:57,781 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:16:57,782 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:16:57,783 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:16:57,784 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:16:57,786 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:16:57,788 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:16:57,789 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:16:57,791 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:16:57,795 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:16:57,799 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:16:57,800 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:16:57,802 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:19:42,325 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:19:42,325 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:19:42,326 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:19:42,327 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:19:42,327 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:19:42,327 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:19:42,328 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:19:42,328 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:19:42,329 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:19:42,329 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:19:42,330 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:19:42,330 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:19:42,331 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:19:42,332 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:19:42,333 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:19:42,334 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:19:42,334 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:19:42,335 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:19:42,336 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:19:42,337 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:19:42,337 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:19:42,338 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:19:42,339 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:19:42,339 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:19:42,340 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:19:42,341 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:19:42,341 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:19:42,343 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:19:42,344 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:19:42,347 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:19:42,360 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 13:19:49,182 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:19:49,183 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:19:49,184 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:19:49,185 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:19:49,186 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:19:49,188 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:19:49,191 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:19:49,193 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:19:49,198 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:19:49,200 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:19:49,201 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:19:49,202 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:19:49,204 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:19:49,205 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:19:49,206 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:19:54,528 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:19:54,529 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:19:54,530 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:19:54,531 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:19:54,534 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:19:54,537 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:19:54,542 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:19:54,543 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:19:54,545 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:19:54,546 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:19:54,547 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:19:54,550 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:19:54,552 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:19:54,554 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:19:54,556 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:19:54,796 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:19:54,797 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:19:54,799 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:19:54,800 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:19:54,801 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:19:54,802 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:19:54,803 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:19:54,805 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:19:54,806 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:19:54,808 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:19:54,811 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:19:54,813 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:19:54,814 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:19:54,816 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:19:54,824 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:19:56,107 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:20:01,454 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:20:02,577 [INFO] AFC remote call 1 is done.
2025-06-28 13:20:19,554 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:20:19,561 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:20:19,563 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:20:19,564 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:20:19,569 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:20:19,582 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:20:19,584 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:20:19,586 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:20:19,644 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:20:19,705 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:20:19,712 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:20:19,714 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:20:19,718 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:20:19,719 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:20:19,723 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:20:31,985 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:20:31,986 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:20:31,987 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:20:31,988 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:20:31,990 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:20:31,991 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:20:31,992 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:20:31,994 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:20:31,995 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:20:31,999 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:20:32,002 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:20:32,004 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:20:32,005 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:20:32,006 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:20:32,007 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:20:32,391 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:20:32,393 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:20:32,394 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:20:32,396 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:20:32,404 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:20:32,407 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:20:32,408 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:20:32,410 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:20:32,411 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:20:32,413 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:20:32,414 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:20:32,420 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:20:32,422 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:20:32,423 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:20:32,424 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:20:32,572 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628132032_cybersecurity-hacker-with-a-concealed-face-wearing-a-hoodie-in-a-technology-themed-wallpaper-photo.jpg, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: Descreva essa imagem.
2025-06-28 13:20:41,975 [INFO] Resposta da IA (imagem): A imagem apresenta uma pessoa, provavelmente um homem jovem, usando um capuz escuro que cobre parcialmente seu rosto. Ele olha diretamente para a frente com uma express√£o intensa e s√©ria.

O rosto da pessoa √© iluminado por uma luz amarela-alaranjada vinda de baixo, criando um contraste dram√°tico com as sombras profundas sob o capuz. H√° um elemento not√°vel em seu rosto: linhas luminosas e pontilhadas, de cor √¢mbar, que descem dos cantos internos dos olhos, assemelhando-se a l√°grimas de energia ou dados. Ao redor dessas linhas, h√° tamb√©m algumas marcas escuras e borradas, como tinta ou sujeira.

O fundo √© composto por uma matriz de luzes azuis-esverdeadas brilhantes e desfocadas, dispostas em padr√µes que lembram um ambiente digital ou cibern√©tico, refor√ßando a sensa√ß√£o de tecnologia ou hacking. A ilumina√ß√£o geral confere √† imagem uma atmosfera misteriosa e futurista, remetendo a temas de fic√ß√£o cient√≠fica ou cyberpunk.
2025-06-28 13:20:42,170 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:20:42,172 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:20:42,173 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:20:42,174 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:20:42,175 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:20:42,177 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:20:42,179 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:20:42,180 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:20:42,182 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:20:42,185 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:20:42,188 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:20:42,190 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:20:42,193 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:20:42,194 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:20:42,195 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:22:30,904 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:22:30,906 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:22:30,907 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:22:30,908 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:22:30,910 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:22:30,911 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:22:30,912 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:22:30,914 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:22:30,916 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:22:30,919 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:22:30,920 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:22:30,922 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:22:30,924 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:22:30,925 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:22:30,926 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:22:31,423 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:22:31,424 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:22:31,425 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:22:31,426 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:22:31,427 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:22:31,429 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:22:31,431 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:22:31,434 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:22:31,437 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:22:31,439 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:22:31,440 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:22:31,441 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:22:31,442 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:22:31,446 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:22:31,451 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:22:40,462 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:22:40,463 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:22:40,464 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:22:40,465 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:22:40,466 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:22:40,467 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:22:40,468 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:22:40,471 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:22:40,473 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:22:40,474 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:22:40,475 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:22:40,475 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:22:40,478 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:22:40,479 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:22:40,481 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:22:40,973 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:22:40,974 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:22:40,975 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:22:40,976 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:22:40,978 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:22:40,979 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:22:40,980 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:22:40,981 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:22:40,983 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:22:40,987 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:22:40,992 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:22:40,993 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:22:40,994 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:22:40,995 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:22:40,996 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:22:41,091 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628132240_clipboard_20250628132231.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: Descreva essa imagem.
Assistant: A imagem apresenta uma pessoa, provavelmente um homem jovem, usando um capuz escuro que cobre parcialmente seu rosto. Ele olha diretamente para a frente com uma express√£o intensa e s√©ria.

O rosto da pessoa √© iluminado por uma luz amarela-alaranjada vinda de baixo, criando um contraste dram√°tico com as sombras profundas sob o capuz. H√° um elemento not√°vel em seu rosto: linhas luminosas e pontilhadas, de cor √¢mbar, que descem dos cantos internos dos olhos, assemelhando-se a l√°grimas de energia ou dados. Ao redor dessas linhas, h√° tamb√©m algumas marcas escuras e borradas, como tinta ou sujeira.

O fundo √© composto por uma matriz de luzes azuis-esverdeadas brilhantes e desfocadas, dispostas em padr√µes que lembram um ambiente digital ou cibern√©tico, refor√ßando a sensa√ß√£o de tecnologia ou hacking. A ilumina√ß√£o geral confere √† imagem uma atmosfera misteriosa e futurista, remetendo a temas de fic√ß√£o cient√≠fica ou cyberpunk.
User: descreva a imagem enviada.
2025-06-28 13:22:49,068 [INFO] Resposta da IA (imagem): Como descrito anteriormente, a imagem que voc√™ enviou apresenta uma pessoa, provavelmente um homem jovem, usando um capuz escuro que cobre parcialmente seu rosto.

Ele olha diretamente para a frente com uma express√£o intensa e s√©ria. O rosto √© iluminado por uma luz amarela-alaranjada vinda de baixo, com linhas luminosas e pontilhadas, de cor √¢mbar, que descem dos cantos internos dos olhos, assemelhando-se a l√°grimas de energia ou dados.

O fundo √© composto por uma matriz de luzes azuis-esverdeadas brilhantes e desfocadas, dispostas em padr√µes que lembram um ambiente digital ou cibern√©tico, refor√ßando a sensa√ß√£o de tecnologia ou hacking. A ilumina√ß√£o geral confere √† imagem uma atmosfera misteriosa e futurista.
2025-06-28 13:22:49,368 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:22:49,370 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:22:49,371 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:22:49,372 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:22:49,372 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:22:49,373 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:22:49,374 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:22:49,376 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:22:49,377 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:22:49,380 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:22:49,386 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:22:49,387 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:22:49,388 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:22:49,389 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:22:49,390 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:23:03,265 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:23:03,266 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:23:03,267 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:23:03,268 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:23:03,269 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:23:03,270 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:23:03,273 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:23:03,274 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:23:03,282 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:23:03,284 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:23:03,287 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:23:03,288 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:23:03,289 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:23:03,291 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:23:03,293 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:23:03,583 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:23:03,587 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:23:03,589 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:23:03,590 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:23:03,592 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:23:03,594 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:23:03,600 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:23:03,603 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:23:03,605 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:23:03,606 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:23:03,607 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:23:03,608 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:23:03,610 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:23:03,613 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:23:03,619 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:23:13,112 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:23:13,114 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:23:13,115 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:23:13,115 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:23:13,116 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:23:13,118 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:23:13,118 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:23:13,121 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:23:13,123 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:23:13,124 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:23:13,126 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:23:13,127 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:23:13,128 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:23:13,129 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:23:13,129 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:23:13,432 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:23:13,433 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:23:13,434 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:23:13,435 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:23:13,435 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:23:13,436 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:23:13,437 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:23:13,439 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:23:13,440 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:23:13,441 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:23:13,444 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:23:13,448 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:23:13,449 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:23:13,450 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:23:13,451 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:23:13,546 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628132313_clipboard_20250628132231.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: o que tem na imagem ?
2025-06-28 13:23:34,789 [INFO] Resposta da IA (imagem): A imagem que voc√™ forneceu √© uma captura de tela de uma aplica√ß√£o de chat constru√≠da com Streamlit, que permite interagir com uma intelig√™ncia artificial e inclui funcionalidades de processamento de imagens.

### Descri√ß√£o da Imagem

A imagem principal exibida na tela √© um retrato de uma pessoa, provavelmente um homem jovem, usando um capuz escuro que cobre parcialmente seu rosto. Ele olha diretamente para a frente com uma express√£o intensa e s√©ria.

*   **Ilumina√ß√£o e Efeitos Visuais:** O rosto da pessoa √© dramaticamente iluminado por uma luz amarela-alaranjada que parece vir de baixo ou de uma fonte interna. Isso cria um forte contraste com as sombras profundas sob o capuz e ao redor do rosto. O elemento mais not√°vel s√£o as linhas luminosas e pontilhadas, de cor √¢mbar (amarelo-alaranjado), que descem dos cantos internos dos olhos, assemelhando-se a l√°grimas de energia, dados ou circuitos digitais. Ao redor dessas "l√°grimas", h√° tamb√©m algumas marcas escuras e borradas, como se fosse tinta escorrendo ou sujeira.
*   **Fundo:** O fundo √© composto por padr√µes abstratos de linhas e pontos luminosos em tons de azul e verde, criando um efeito futurista e digital, semelhante a um c√≥digo de computador, uma rede de dados ou o interior de um sistema cibern√©tico. H√° marcas d'√°gua "Vecteezy" vis√≠veis na imagem.
*   **Tem√°tica:** No geral, a imagem evoca temas de hacking, ciberseguran√ßa, tecnologia e mist√©rio, com uma atmosfera sombria e intensa.

### Como o Sistema Lida com a Imagem (Baseado no C√≥digo Fornecido)

O c√≥digo Python fornecido detalha como a aplica√ß√£o Streamlit (`chat_streamlit.py`) e os servi√ßos de IA (`services/gpt_services.py`, `core/handlers/gemini_handler.py`, `services/image_services.py`) interagem para processar e, agora, tamb√©m **gerar** imagens.

Aqui est√° um resumo do fluxo:

1.  **Entrada da Imagem (UI - `chat_streamlit.py`):**
    *   **Upload de Arquivo:** O usu√°rio pode carregar uma imagem (`.png`, `.jpg`, `.jpeg`) usando o `st.file_uploader`. O arquivo carregado √© armazenado em `st.session_state.uploaded_image`.
    *   **Colar da √Årea de Transfer√™ncia:** O bot√£o "üìã Colar" utiliza a fun√ß√£o `check_clipboard()` (que tenta acessar `PIL.ImageGrab.grabclipboard()`) para verificar se h√° uma imagem na √°rea de transfer√™ncia do sistema. Se encontrada, a imagem √© convertida para bytes e armazenada em `st.session_state.clipboard_image_file` e `st.session_state.clipboard_image_preview`.
    *   **Gera√ß√£o de Imagem (NOVO):** H√° uma nova se√ß√£o na barra lateral ("Gerar Imagem") onde o usu√°rio pode digitar um prompt e clicar em "Gerar Imagem". A fun√ß√£o `generate_image` (em `chat_streamlit.py`, que chama `services.image_services.generate_image`) utiliza o modelo `gemini-2.0-flash-exp-image-generation` para criar uma imagem a partir do prompt. A imagem gerada √© ent√£o exibida no hist√≥rico de mensagens e pode ser referenciada para futuras perguntas.
    *   **Limpar Imagem:** O bot√£o "üóëÔ∏è Limpar" e a fun√ß√£o `clear_all_images()` removem qualquer imagem carregada ou colada dos estados da sess√£o.

2.  **Prepara√ß√£o para o Processamento (`process_message`, `execute_processing` em `chat_streamlit.py`):**
    *   Quando o usu√°rio digita uma pergunta no `st.chat_input` e h√° uma imagem (carregada, colada ou gerada) dispon√≠vel (`current_image`), a fun√ß√£o `process_message` √© chamada.
    *   Esta fun√ß√£o marca o estado `processing` como `True` e armazena o prompt e a imagem. Isso for√ßa uma reexecu√ß√£o do Streamlit.
    *   Na reexecu√ß√£o, se `processing` for `True`, a fun√ß√£o `execute_processing` √© ativada.
    *   `execute_processing` primeiramente salva a imagem de entrada (seja do upload, clipboard ou gerada) em um arquivo tempor√°rio dentro do diret√≥rio `assets` (`Config.ASSETS_DIR`). Isso √© necess√°rio porque o `GeminiHandler` trabalha com caminhos de arquivo.
    *   Adiciona a mensagem do usu√°rio (e a imagem, se houver) ao `st.session_state.messages` (o hist√≥rico do chat).

3.  **Intera√ß√£o com a IA (`core/handlers/gemini_handler.py`, `services/gpt_services.py`):**
    *   O `GeminiHandler` √© instanciado e gerencia a comunica√ß√£o com a API do Google Gemini.
    *   Ele usa um `RateLimiter` (`core/rate_limiter.py`) para garantir que o n√∫mero de requisi√ß√µes √† API n√£o exceda os limites definidos (ex: 15 requisi√ß√µes por minuto).
    *   A fun√ß√£o `gemini_handler.generate_content()` √© chamada:
        *   Se um `img_path` (caminho para a imagem tempor√°ria) √© fornecido, ele invoca `self.handler.generate_content_from_image()`.
        *   `generate_content_from_image` (dentro de `services/gpt_services.py`) l√™ os bytes da imagem do arquivo tempor√°rio e os envia, juntamente com o prompt do usu√°rio, para o modelo Gemini configurado (`gemini-2.5-flash`) usando `model.generate_content([{"mime_type": "image/png", "data": image_bytes}, prompt])`.
        *   Se n√£o houver imagem, ele chama `generate_content_from_text()` para processar apenas o prompt de texto.

4.  **Resposta e Limpeza (`execute_processing` em `chat_streamlit.py`):**
    *   A resposta textual gerada pela IA √© recebida.
    *   A resposta √© adicionada ao `st.session_state.messages` (hist√≥rico do chat).
    *   O arquivo de imagem tempor√°rio salvo em `assets` √© removido (`os.remove(img_path)`) para economizar espa√ßo e manter o sistema limpo.
    *   O estado `processing` √© definido como `False`, liberando o `st.chat_input` para uma nova entrada do usu√°rio.

Em resumo, o sistema permite que o usu√°rio forne√ßa imagens (via upload, colar ou gera√ß√£o) para que o modelo Gemini as analise e responda com base no conte√∫do visual e textual, mantendo um hist√≥rico da conversa e gerenciando as requisi√ß√µes √† API de forma controlada.
2025-06-28 13:23:34,989 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:23:34,990 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:23:34,991 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:23:34,992 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:23:34,993 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:23:34,994 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:23:34,996 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:23:34,997 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:23:34,998 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:23:34,999 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:23:35,000 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:23:35,004 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:23:35,007 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:23:35,008 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:23:35,009 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:24:24,647 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:24:24,650 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:24:24,653 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:24:24,654 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:24:24,657 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:24:24,658 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:24:24,662 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:24:24,668 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:24:24,672 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:24:24,673 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:24:24,674 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:24:24,675 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:24:24,677 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:24:24,678 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:24:24,681 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:24:26,118 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:24:26,119 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:24:26,120 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:24:26,121 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:24:26,122 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:24:26,123 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:24:26,125 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:24:26,127 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:24:26,130 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:24:26,134 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:24:26,156 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:24:26,160 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:24:26,193 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:24:26,194 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:24:26,196 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:24:27,071 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:24:32,807 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:24:34,119 [INFO] AFC remote call 1 is done.
2025-06-28 13:25:04,364 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:25:04,365 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:25:04,366 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:25:04,369 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:25:04,370 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:25:04,372 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:25:04,383 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:25:04,385 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:25:04,387 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:25:04,388 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:25:04,389 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:25:04,391 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:25:04,393 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:25:04,395 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:25:04,401 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:25:04,720 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:25:04,721 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:25:04,722 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:25:04,723 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:25:04,724 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:25:04,725 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:25:04,734 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:25:04,738 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:25:04,739 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:25:04,740 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:25:04,740 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:25:04,741 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:25:04,742 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:25:04,746 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:25:04,808 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:26:13,809 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:26:13,810 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:26:13,811 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:26:13,812 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:26:13,822 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:26:13,824 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:26:13,827 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:26:13,829 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:26:13,830 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:26:13,831 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:26:13,832 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:26:13,833 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:26:13,844 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:26:13,877 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:26:13,888 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:26:14,403 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:26:14,404 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:26:14,405 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:26:14,406 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:26:14,407 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:26:14,408 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:26:14,409 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:26:14,412 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:26:14,413 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:26:14,416 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:26:14,420 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:26:14,421 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:26:14,422 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:26:14,423 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:26:14,424 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:26:14,474 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628132614_clipboard_20250628132504.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: o que tem na imagem ?
Assistant: A imagem que voc√™ forneceu √© uma captura de tela de uma aplica√ß√£o de chat constru√≠da com Streamlit, que permite interagir com uma intelig√™ncia artificial e inclui funcionalidades de processamento de imagens.

### Descri√ß√£o da Imagem

A imagem principal exibida na tela √© um retrato de uma pessoa, provavelmente um homem jovem, usando um capuz escuro que cobre parcialmente seu rosto. Ele olha diretamente para a frente com uma express√£o intensa e s√©ria.

*   **Ilumina√ß√£o e Efeitos Visuais:** O rosto da pessoa √© dramaticamente iluminado por uma luz amarela-alaranjada que parece vir de baixo ou de uma fonte interna. Isso cria um forte contraste com as sombras profundas sob o capuz e ao redor do rosto. O elemento mais not√°vel s√£o as linhas luminosas e pontilhadas, de cor √¢mbar (amarelo-alaranjado), que descem dos cantos internos dos olhos, assemelhando-se a l√°grimas de energia, dados ou circuitos digitais. Ao redor dessas "l√°grimas", h√° tamb√©m algumas marcas escuras e borradas, como se fosse tinta escorrendo ou sujeira.
*   **Fundo:** O fundo √© composto por padr√µes abstratos de linhas e pontos luminosos em tons de azul e verde, criando um efeito futurista e digital, semelhante a um c√≥digo de computador, uma rede de dados ou o interior de um sistema cibern√©tico. H√° marcas d'√°gua "Vecteezy" vis√≠veis na imagem.
*   **Tem√°tica:** No geral, a imagem evoca temas de hacking, ciberseguran√ßa, tecnologia e mist√©rio, com uma atmosfera sombria e intensa.

### Como o Sistema Lida com a Imagem (Baseado no C√≥digo Fornecido)

O c√≥digo Python fornecido detalha como a aplica√ß√£o Streamlit (`chat_streamlit.py`) e os servi√ßos de IA (`services/gpt_services.py`, `core/handlers/gemini_handler.py`, `services/image_services.py`) interagem para processar e, agora, tamb√©m **gerar** imagens.

Aqui est√° um resumo do fluxo:

1.  **Entrada da Imagem (UI - `chat_streamlit.py`):**
    *   **Upload de Arquivo:** O usu√°rio pode carregar uma imagem (`.png`, `.jpg`, `.jpeg`) usando o `st.file_uploader`. O arquivo carregado √© armazenado em `st.session_state.uploaded_image`.
    *   **Colar da √Årea de Transfer√™ncia:** O bot√£o "üìã Colar" utiliza a fun√ß√£o `check_clipboard()` (que tenta acessar `PIL.ImageGrab.grabclipboard()`) para verificar se h√° uma imagem na √°rea de transfer√™ncia do sistema. Se encontrada, a imagem √© convertida para bytes e armazenada em `st.session_state.clipboard_image_file` e `st.session_state.clipboard_image_preview`.
    *   **Gera√ß√£o de Imagem (NOVO):** H√° uma nova se√ß√£o na barra lateral ("Gerar Imagem") onde o usu√°rio pode digitar um prompt e clicar em "Gerar Imagem". A fun√ß√£o `generate_image` (em `chat_streamlit.py`, que chama `services.image_services.generate_image`) utiliza o modelo `gemini-2.0-flash-exp-image-generation` para criar uma imagem a partir do prompt. A imagem gerada √© ent√£o exibida no hist√≥rico de mensagens e pode ser referenciada para futuras perguntas.
    *   **Limpar Imagem:** O bot√£o "üóëÔ∏è Limpar" e a fun√ß√£o `clear_all_images()` removem qualquer imagem carregada ou colada dos estados da sess√£o.

2.  **Prepara√ß√£o para o Processamento (`process_message`, `execute_processing` em `chat_streamlit.py`):**
    *   Quando o usu√°rio digita uma pergunta no `st.chat_input` e h√° uma imagem (carregada, colada ou gerada) dispon√≠vel (`current_image`), a fun√ß√£o `process_message` √© chamada.
    *   Esta fun√ß√£o marca o estado `processing` como `True` e armazena o prompt e a imagem. Isso for√ßa uma reexecu√ß√£o do Streamlit.
    *   Na reexecu√ß√£o, se `processing` for `True`, a fun√ß√£o `execute_processing` √© ativada.
    *   `execute_processing` primeiramente salva a imagem de entrada (seja do upload, clipboard ou gerada) em um arquivo tempor√°rio dentro do diret√≥rio `assets` (`Config.ASSETS_DIR`). Isso √© necess√°rio porque o `GeminiHandler` trabalha com caminhos de arquivo.
    *   Adiciona a mensagem do usu√°rio (e a imagem, se houver) ao `st.session_state.messages` (o hist√≥rico do chat).

3.  **Intera√ß√£o com a IA (`core/handlers/gemini_handler.py`, `services/gpt_services.py`):**
    *   O `GeminiHandler` √© instanciado e gerencia a comunica√ß√£o com a API do Google Gemini.
    *   Ele usa um `RateLimiter` (`core/rate_limiter.py`) para garantir que o n√∫mero de requisi√ß√µes √† API n√£o exceda os limites definidos (ex: 15 requisi√ß√µes por minuto).
    *   A fun√ß√£o `gemini_handler.generate_content()` √© chamada:
        *   Se um `img_path` (caminho para a imagem tempor√°ria) √© fornecido, ele invoca `self.handler.generate_content_from_image()`.
        *   `generate_content_from_image` (dentro de `services/gpt_services.py`) l√™ os bytes da imagem do arquivo tempor√°rio e os envia, juntamente com o prompt do usu√°rio, para o modelo Gemini configurado (`gemini-2.5-flash`) usando `model.generate_content([{"mime_type": "image/png", "data": image_bytes}, prompt])`.
        *   Se n√£o houver imagem, ele chama `generate_content_from_text()` para processar apenas o prompt de texto.

4.  **Resposta e Limpeza (`execute_processing` em `chat_streamlit.py`):**
    *   A resposta textual gerada pela IA √© recebida.
    *   A resposta √© adicionada ao `st.session_state.messages` (hist√≥rico do chat).
    *   O arquivo de imagem tempor√°rio salvo em `assets` √© removido (`os.remove(img_path)`) para economizar espa√ßo e manter o sistema limpo.
    *   O estado `processing` √© definido como `False`, liberando o `st.chat_input` para uma nova entrada do usu√°rio.

Em resumo, o sistema permite que o usu√°rio forne√ßa imagens (via upload, colar ou gera√ß√£o) para que o modelo Gemini as analise e responda com base no conte√∫do visual e textual, mantendo um hist√≥rico da conversa e gerenciando as requisi√ß√µes √† API de forma controlada.
User: a imagem representa o erro que est√° ocorrendo na aplica√ß√£o que estou enviando o c√≥digo.

qual Lib pode ser necess√°rio installar ?

pois eu apenas fiz o git clone do mesmo resposit√≥rio que estou usando em outro notebook na minha casa e l√° funciona com o mesmo c√≥digo. e n√£o gera esse erro.
2025-06-28 13:26:29,705 [INFO] Resposta da IA (imagem): O erro `Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x00000273AAB653A0>` indica que a biblioteca respons√°vel por processar (neste caso, abrir) os dados de imagem recebidos da API do Gemini n√£o conseguiu reconhecer o formato do arquivo.

Analisando o seu c√≥digo, a linha cr√≠tica est√° na fun√ß√£o `generate_image` dentro de `chat_streamlit.py`:

```python
                elif part.inline_data is not None:
                    image = Image.open(io.BytesIO(part.inline_data.data)) # <-- Acontece aqui
                    st.session_state.generated_image = image
                    return image
```

Essa linha utiliza a biblioteca `Pillow` (ou PIL) para converter os dados brutos da imagem (que v√™m em um objeto `io.BytesIO`) em um objeto de imagem que pode ser exibido pelo Streamlit (`st.image`).

Se funciona em um notebook e n√£o em outro com o mesmo c√≥digo, a causa mais prov√°vel √© uma **discrep√¢ncia no ambiente de Python**, especificamente na instala√ß√£o da biblioteca `Pillow` ou em suas depend√™ncias subjacentes.

### Lib Pode ser Necess√°rio Instalar (ou Reinstalar)

A biblioteca principal envolvida neste erro √© a **Pillow**.

**Passos para Resolver:**

1.  **Reinstalar Pillow:**
    √â poss√≠vel que a instala√ß√£o do Pillow esteja corrompida ou incompleta no notebook onde o erro ocorre. Tente desinstal√°-la e reinstal√°-la:

    ```bash
    pip uninstall Pillow
    pip install Pillow
    ```

    Ou, para garantir que voc√™ est√° usando a vers√£o mais recente e que todas as depend√™ncias s√£o resolvidas:

    ```bash
    pip install --upgrade Pillow
    ```

2.  **Garantir Consist√™ncia de Ambiente (Melhor Pr√°tica):**
    A forma mais robusta de garantir que seu ambiente de desenvolvimento seja id√™ntico em todas as m√°quinas √© usando um arquivo `requirements.txt`.

    *   **No notebook onde funciona (o "bom" ambiente):**
        Abra o terminal na pasta raiz do seu projeto (ou no ambiente virtual que voc√™ usa para o projeto) e execute:
        ```bash
        pip freeze > requirements.txt
        ```
        Isso criar√° um arquivo `requirements.txt` com todas as bibliotecas e suas vers√µes exatas que est√£o instaladas e funcionando.

    *   **No notebook onde n√£o funciona:**
        Primeiro, garanta que voc√™ est√° usando um ambiente virtual (√© sempre uma boa pr√°tica). Se n√£o estiver usando um, crie um e ative-o.
        ```bash
        python -m venv venv_project  # Cria um ambiente virtual (nome 'venv_project')
        source venv_project/bin/activate # No Linux/macOS
        # Ou venv_project\Scripts\activate # No Windows
        ```
        Em seguida, navegue at√© a pasta onde est√° o `requirements.txt` (que voc√™ gerou no outro notebook) e instale tudo:
        ```bash
        pip install -r requirements.txt
        ```
        Isso garantir√° que voc√™ tenha *exatamente* as mesmas vers√µes de todas as bibliotecas, incluindo `Pillow` e `google-generativeai`, que funcionam na sua outra m√°quina.

3.  **Verificar a Instala√ß√£o do `google-generativeai`:**
    Embora o erro aponte mais para `Pillow`, o `google-generativeai` tamb√©m est√° envolvido. Certifique-se de que ele esteja instalado corretamente.
    ```bash
    pip install google-generativeai
    ```
    Ou via `requirements.txt` como mencionado acima.

4.  **Verificar Vers√£o do Python:**
    Confirme se a vers√£o do Python √© a mesma ou compat√≠vel nos dois notebooks. Diferen√ßas significativas podem levar a problemas de compatibilidade com algumas bibliotecas.

5.  **Depend√™ncias do Sistema Operacional (Menos Comum para Pillow em geral, mas poss√≠vel):**
    Em alguns sistemas Linux, bibliotecas de sistema para imagem (como `libjpeg-dev`, `zlib1g-dev`, `libpng-dev`) podem precisar ser instaladas antes que `Pillow` possa ser totalmente compilada com suporte a todos os formatos. Se voc√™ estiver usando Linux, tente:
    ```bash
    sudo apt-get update
    sudo apt-get install libjpeg-dev zlib1g-dev libpng-dev
    ```
    E ent√£o reinstale o `Pillow`.

**Resumo da Causa Prov√°vel:**

O erro `cannot identify image file <_io.BytesIO object` quase sempre significa que a biblioteca `Pillow` (ou uma de suas depend√™ncias internas para lidar com formatos de imagem) n√£o est√° funcionando corretamente para decodificar os bytes da imagem que ela recebeu. A consist√™ncia do ambiente via `requirements.txt` √© a solu√ß√£o mais confi√°vel para esse tipo de problema quando o c√≥digo funciona em outro lugar.
2025-06-28 13:26:29,931 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:26:29,932 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:26:29,933 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:26:29,935 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:26:29,936 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:26:29,938 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:26:29,939 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:26:29,950 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:26:29,952 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:26:29,953 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:26:29,955 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:26:29,956 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:26:29,959 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:26:29,965 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:26:29,970 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:28:47,295 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:28:47,296 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:28:47,299 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:28:47,301 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:28:47,302 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:28:47,307 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:28:47,309 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:28:47,311 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:28:47,314 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:28:47,315 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:28:47,316 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:28:47,318 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:28:47,319 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:28:47,321 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:28:47,321 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:28:47,673 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:28:47,676 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:28:47,677 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:28:47,678 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:28:47,679 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:28:47,681 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:28:47,685 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:28:47,687 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:28:47,689 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:28:47,690 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:28:47,692 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:28:47,694 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:28:47,696 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:28:47,697 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:28:47,698 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:28:48,536 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:28:54,885 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:28:55,811 [INFO] AFC remote call 1 is done.
2025-06-28 13:29:41,733 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:29:41,734 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:29:41,735 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:29:41,736 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:29:41,737 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:29:41,739 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:29:41,741 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:29:41,743 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:29:41,751 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:29:41,752 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:29:41,753 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:29:41,756 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:29:41,757 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:29:41,758 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:29:41,759 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:29:42,073 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:29:42,074 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:29:42,075 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:29:42,076 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:29:42,077 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:29:42,078 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:29:42,080 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:29:42,082 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:29:42,084 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:29:42,090 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:29:42,092 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:29:42,094 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:29:42,095 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:29:42,097 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:29:42,098 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:29:42,133 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628132942_clipboard_20250628132504.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: o que tem na imagem ?
Assistant: A imagem que voc√™ forneceu √© uma captura de tela de uma aplica√ß√£o de chat constru√≠da com Streamlit, que permite interagir com uma intelig√™ncia artificial e inclui funcionalidades de processamento de imagens.

### Descri√ß√£o da Imagem

A imagem principal exibida na tela √© um retrato de uma pessoa, provavelmente um homem jovem, usando um capuz escuro que cobre parcialmente seu rosto. Ele olha diretamente para a frente com uma express√£o intensa e s√©ria.

*   **Ilumina√ß√£o e Efeitos Visuais:** O rosto da pessoa √© dramaticamente iluminado por uma luz amarela-alaranjada que parece vir de baixo ou de uma fonte interna. Isso cria um forte contraste com as sombras profundas sob o capuz e ao redor do rosto. O elemento mais not√°vel s√£o as linhas luminosas e pontilhadas, de cor √¢mbar (amarelo-alaranjado), que descem dos cantos internos dos olhos, assemelhando-se a l√°grimas de energia, dados ou circuitos digitais. Ao redor dessas "l√°grimas", h√° tamb√©m algumas marcas escuras e borradas, como se fosse tinta escorrendo ou sujeira.
*   **Fundo:** O fundo √© composto por padr√µes abstratos de linhas e pontos luminosos em tons de azul e verde, criando um efeito futurista e digital, semelhante a um c√≥digo de computador, uma rede de dados ou o interior de um sistema cibern√©tico. H√° marcas d'√°gua "Vecteezy" vis√≠veis na imagem.
*   **Tem√°tica:** No geral, a imagem evoca temas de hacking, ciberseguran√ßa, tecnologia e mist√©rio, com uma atmosfera sombria e intensa.

### Como o Sistema Lida com a Imagem (Baseado no C√≥digo Fornecido)

O c√≥digo Python fornecido detalha como a aplica√ß√£o Streamlit (`chat_streamlit.py`) e os servi√ßos de IA (`services/gpt_services.py`, `core/handlers/gemini_handler.py`, `services/image_services.py`) interagem para processar e, agora, tamb√©m **gerar** imagens.

Aqui est√° um resumo do fluxo:

1.  **Entrada da Imagem (UI - `chat_streamlit.py`):**
    *   **Upload de Arquivo:** O usu√°rio pode carregar uma imagem (`.png`, `.jpg`, `.jpeg`) usando o `st.file_uploader`. O arquivo carregado √© armazenado em `st.session_state.uploaded_image`.
    *   **Colar da √Årea de Transfer√™ncia:** O bot√£o "üìã Colar" utiliza a fun√ß√£o `check_clipboard()` (que tenta acessar `PIL.ImageGrab.grabclipboard()`) para verificar se h√° uma imagem na √°rea de transfer√™ncia do sistema. Se encontrada, a imagem √© convertida para bytes e armazenada em `st.session_state.clipboard_image_file` e `st.session_state.clipboard_image_preview`.
    *   **Gera√ß√£o de Imagem (NOVO):** H√° uma nova se√ß√£o na barra lateral ("Gerar Imagem") onde o usu√°rio pode digitar um prompt e clicar em "Gerar Imagem". A fun√ß√£o `generate_image` (em `chat_streamlit.py`, que chama `services.image_services.generate_image`) utiliza o modelo `gemini-2.0-flash-exp-image-generation` para criar uma imagem a partir do prompt. A imagem gerada √© ent√£o exibida no hist√≥rico de mensagens e pode ser referenciada para futuras perguntas.
    *   **Limpar Imagem:** O bot√£o "üóëÔ∏è Limpar" e a fun√ß√£o `clear_all_images()` removem qualquer imagem carregada ou colada dos estados da sess√£o.

2.  **Prepara√ß√£o para o Processamento (`process_message`, `execute_processing` em `chat_streamlit.py`):**
    *   Quando o usu√°rio digita uma pergunta no `st.chat_input` e h√° uma imagem (carregada, colada ou gerada) dispon√≠vel (`current_image`), a fun√ß√£o `process_message` √© chamada.
    *   Esta fun√ß√£o marca o estado `processing` como `True` e armazena o prompt e a imagem. Isso for√ßa uma reexecu√ß√£o do Streamlit.
    *   Na reexecu√ß√£o, se `processing` for `True`, a fun√ß√£o `execute_processing` √© ativada.
    *   `execute_processing` primeiramente salva a imagem de entrada (seja do upload, clipboard ou gerada) em um arquivo tempor√°rio dentro do diret√≥rio `assets` (`Config.ASSETS_DIR`). Isso √© necess√°rio porque o `GeminiHandler` trabalha com caminhos de arquivo.
    *   Adiciona a mensagem do usu√°rio (e a imagem, se houver) ao `st.session_state.messages` (o hist√≥rico do chat).

3.  **Intera√ß√£o com a IA (`core/handlers/gemini_handler.py`, `services/gpt_services.py`):**
    *   O `GeminiHandler` √© instanciado e gerencia a comunica√ß√£o com a API do Google Gemini.
    *   Ele usa um `RateLimiter` (`core/rate_limiter.py`) para garantir que o n√∫mero de requisi√ß√µes √† API n√£o exceda os limites definidos (ex: 15 requisi√ß√µes por minuto).
    *   A fun√ß√£o `gemini_handler.generate_content()` √© chamada:
        *   Se um `img_path` (caminho para a imagem tempor√°ria) √© fornecido, ele invoca `self.handler.generate_content_from_image()`.
        *   `generate_content_from_image` (dentro de `services/gpt_services.py`) l√™ os bytes da imagem do arquivo tempor√°rio e os envia, juntamente com o prompt do usu√°rio, para o modelo Gemini configurado (`gemini-2.5-flash`) usando `model.generate_content([{"mime_type": "image/png", "data": image_bytes}, prompt])`.
        *   Se n√£o houver imagem, ele chama `generate_content_from_text()` para processar apenas o prompt de texto.

4.  **Resposta e Limpeza (`execute_processing` em `chat_streamlit.py`):**
    *   A resposta textual gerada pela IA √© recebida.
    *   A resposta √© adicionada ao `st.session_state.messages` (hist√≥rico do chat).
    *   O arquivo de imagem tempor√°rio salvo em `assets` √© removido (`os.remove(img_path)`) para economizar espa√ßo e manter o sistema limpo.
    *   O estado `processing` √© definido como `False`, liberando o `st.chat_input` para uma nova entrada do usu√°rio.

Em resumo, o sistema permite que o usu√°rio forne√ßa imagens (via upload, colar ou gera√ß√£o) para que o modelo Gemini as analise e responda com base no conte√∫do visual e textual, mantendo um hist√≥rico da conversa e gerenciando as requisi√ß√µes √† API de forma controlada.
User: a imagem representa o erro que est√° ocorrendo na aplica√ß√£o que estou enviando o c√≥digo.

qual Lib pode ser necess√°rio installar ?

pois eu apenas fiz o git clone do mesmo resposit√≥rio que estou usando em outro notebook na minha casa e l√° funciona com o mesmo c√≥digo. e n√£o gera esse erro.
Assistant: O erro `Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x00000273AAB653A0>` indica que a biblioteca respons√°vel por processar (neste caso, abrir) os dados de imagem recebidos da API do Gemini n√£o conseguiu reconhecer o formato do arquivo.

Analisando o seu c√≥digo, a linha cr√≠tica est√° na fun√ß√£o `generate_image` dentro de `chat_streamlit.py`:

```python
                elif part.inline_data is not None:
                    image = Image.open(io.BytesIO(part.inline_data.data)) # <-- Acontece aqui
                    st.session_state.generated_image = image
                    return image
```

Essa linha utiliza a biblioteca `Pillow` (ou PIL) para converter os dados brutos da imagem (que v√™m em um objeto `io.BytesIO`) em um objeto de imagem que pode ser exibido pelo Streamlit (`st.image`).

Se funciona em um notebook e n√£o em outro com o mesmo c√≥digo, a causa mais prov√°vel √© uma **discrep√¢ncia no ambiente de Python**, especificamente na instala√ß√£o da biblioteca `Pillow` ou em suas depend√™ncias subjacentes.

### Lib Pode ser Necess√°rio Instalar (ou Reinstalar)

A biblioteca principal envolvida neste erro √© a **Pillow**.

**Passos para Resolver:**

1.  **Reinstalar Pillow:**
    √â poss√≠vel que a instala√ß√£o do Pillow esteja corrompida ou incompleta no notebook onde o erro ocorre. Tente desinstal√°-la e reinstal√°-la:

    ```bash
    pip uninstall Pillow
    pip install Pillow
    ```

    Ou, para garantir que voc√™ est√° usando a vers√£o mais recente e que todas as depend√™ncias s√£o resolvidas:

    ```bash
    pip install --upgrade Pillow
    ```

2.  **Garantir Consist√™ncia de Ambiente (Melhor Pr√°tica):**
    A forma mais robusta de garantir que seu ambiente de desenvolvimento seja id√™ntico em todas as m√°quinas √© usando um arquivo `requirements.txt`.

    *   **No notebook onde funciona (o "bom" ambiente):**
        Abra o terminal na pasta raiz do seu projeto (ou no ambiente virtual que voc√™ usa para o projeto) e execute:
        ```bash
        pip freeze > requirements.txt
        ```
        Isso criar√° um arquivo `requirements.txt` com todas as bibliotecas e suas vers√µes exatas que est√£o instaladas e funcionando.

    *   **No notebook onde n√£o funciona:**
        Primeiro, garanta que voc√™ est√° usando um ambiente virtual (√© sempre uma boa pr√°tica). Se n√£o estiver usando um, crie um e ative-o.
        ```bash
        python -m venv venv_project  # Cria um ambiente virtual (nome 'venv_project')
        source venv_project/bin/activate # No Linux/macOS
        # Ou venv_project\Scripts\activate # No Windows
        ```
        Em seguida, navegue at√© a pasta onde est√° o `requirements.txt` (que voc√™ gerou no outro notebook) e instale tudo:
        ```bash
        pip install -r requirements.txt
        ```
        Isso garantir√° que voc√™ tenha *exatamente* as mesmas vers√µes de todas as bibliotecas, incluindo `Pillow` e `google-generativeai`, que funcionam na sua outra m√°quina.

3.  **Verificar a Instala√ß√£o do `google-generativeai`:**
    Embora o erro aponte mais para `Pillow`, o `google-generativeai` tamb√©m est√° envolvido. Certifique-se de que ele esteja instalado corretamente.
    ```bash
    pip install google-generativeai
    ```
    Ou via `requirements.txt` como mencionado acima.

4.  **Verificar Vers√£o do Python:**
    Confirme se a vers√£o do Python √© a mesma ou compat√≠vel nos dois notebooks. Diferen√ßas significativas podem levar a problemas de compatibilidade com algumas bibliotecas.

5.  **Depend√™ncias do Sistema Operacional (Menos Comum para Pillow em geral, mas poss√≠vel):**
    Em alguns sistemas Linux, bibliotecas de sistema para imagem (como `libjpeg-dev`, `zlib1g-dev`, `libpng-dev`) podem precisar ser instaladas antes que `Pillow` possa ser totalmente compilada com suporte a todos os formatos. Se voc√™ estiver usando Linux, tente:
    ```bash
    sudo apt-get update
    sudo apt-get install libjpeg-dev zlib1g-dev libpng-dev
    ```
    E ent√£o reinstale o `Pillow`.

**Resumo da Causa Prov√°vel:**

O erro `cannot identify image file <_io.BytesIO object` quase sempre significa que a biblioteca `Pillow` (ou uma de suas depend√™ncias internas para lidar com formatos de imagem) n√£o est√° funcionando corretamente para decodificar os bytes da imagem que ela recebeu. A consist√™ncia do ambiente via `requirements.txt` √© a solu√ß√£o mais confi√°vel para esse tipo de problema quando o c√≥digo funciona em outro lugar.
User: preciso dar um restart  na aplica√ß√£o ?

porque no log do vscode chega essa mensagem de 200 OK 

2025-06-28 13:28:47,694 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:28:47,696 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:28:47,697 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:28:47,698 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:28:48,536 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:28:54,885 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent 
"HTTP/1.1 200 OK"
2025-06-28 13:28:55,811 [INFO] AFC remote call 1 is done.
A imagem mostrar√° um homem de 40 anos, no estilo Pixar, usando √≥culos e com cabelo castanho m√©dio. Ele ter√° barba e bigode baixos e ralos cobrindo todo o rosto. O homem estar√° localizado em um campo de futebol vibrante, repleto de uma multid√£o animada de torcedores ao fundo.

2025-06-28 13:30:00,076 [INFO] Resposta da IA (imagem): Sim, voc√™ **precisa reiniciar a aplica√ß√£o Streamlit** ap√≥s fazer qualquer altera√ß√£o ou reinstala√ß√£o de bibliotecas Python.

**Por que voc√™ precisa reiniciar a aplica√ß√£o, mesmo com "200 OK"?**

O log `HTTP/1.1 200 OK` √© uma informa√ß√£o crucial e muito √∫til! Ele significa que:

1.  **A comunica√ß√£o com a API do Gemini foi um sucesso:** Seu c√≥digo conseguiu enviar o prompt para o Gemini.
2.  **O Gemini gerou a imagem e a enviou de volta:** O `200 OK` confirma que os dados da imagem foram transmitidos da Google para o seu ambiente.

O erro `cannot identify image file <_io.BytesIO object at 0x...>` ocorre **AP√ìS** a recep√ß√£o desses dados. Ele acontece especificamente nesta linha do seu `chat_streamlit.py`:

```python
                elif part.inline_data is not None:
                    image = Image.open(io.BytesIO(part.inline_data.data)) # <-- O erro acontece aqui
                    st.session_state.generated_image = image
                    return image
```

Esta linha est√° tentando usar a biblioteca `Pillow` (atrav√©s de `Image.open()`) para **interpretar os bytes da imagem** que foram recebidos da API do Gemini.

**A causa prov√°vel, refor√ßada pelo seu log, √© que a sua instala√ß√£o da biblioteca `Pillow` est√° de alguma forma corrompida ou incompleta no ambiente onde voc√™ est√° rodando a aplica√ß√£o.** Ela recebeu os dados, mas n√£o consegue "entender" o que s√£o esses dados para transform√°-los em uma imagem manipul√°vel.

**Quando voc√™ instala ou reinstala uma biblioteca Python (`pip install ...`), o processo em execu√ß√£o do Streamlit (ou qualquer outro script Python) n√£o "percebe" essas mudan√ßas automaticamente.** Ele j√° carregou as bibliotecas em sua mem√≥ria no momento em que foi iniciado. Para que as novas vers√µes ou instala√ß√µes corrigidas sejam utilizadas, voc√™ precisa:

1.  **Parar o Streamlit que est√° rodando** (geralmente pressionando `Ctrl+C` no terminal onde ele foi iniciado).
2.  **Executar os comandos de instala√ß√£o/atualiza√ß√£o da biblioteca `Pillow`** (conforme sugeri na resposta anterior, sendo `pip install --upgrade Pillow` o mais direto).
3.  **Iniciar a aplica√ß√£o Streamlit novamente** (`streamlit run chat_streamlit.py`).

**A√ß√µes recomendadas, com base nas novas informa√ß√µes:**

1.  **Pare a aplica√ß√£o Streamlit:** V√° para o terminal onde o Streamlit est√° rodando e pressione `Ctrl+C`.
2.  **Reinstale/Atualize o Pillow (novamente, para garantir):**
    ```bash
    pip install --upgrade Pillow
    ```
    Ou, se voc√™ criou o `requirements.txt` no seu outro notebook:
    ```bash
    pip install -r requirements.txt
    ```
3.  **Reinicie o Streamlit:**
    ```bash
    streamlit run chat_streamlit.py
    ```

Isso deve resolver o problema, pois garante que o ambiente Python que o Streamlit usa esteja com a vers√£o funcional e completa do `Pillow` carregada.
2025-06-28 13:30:00,242 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:30:00,245 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:30:00,246 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:30:00,247 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:30:00,248 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:30:00,249 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:30:00,251 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:30:00,252 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:30:00,294 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:30:00,329 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:30:00,367 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:30:00,395 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:30:00,408 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:30:00,412 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:30:00,415 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:31:03,308 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:31:03,309 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:31:03,310 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:31:03,312 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:31:03,313 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:31:03,315 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:31:03,317 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:31:03,319 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:31:03,321 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:31:03,322 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:31:03,323 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:31:03,324 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:31:03,326 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:31:03,332 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:31:03,333 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:31:04,083 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 13:31:09,710 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:31:09,712 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:31:09,714 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:31:09,715 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:31:09,717 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:31:09,720 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:31:09,726 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:31:09,729 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:31:09,730 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:31:09,734 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:31:09,789 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:31:09,810 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:31:09,813 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:31:09,815 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:31:09,985 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:31:10,044 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:31:10,284 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:31:10,285 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:31:10,286 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:31:10,287 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:31:10,287 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:31:10,288 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:31:10,289 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:31:10,290 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:31:10,291 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:31:10,293 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:31:10,295 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:31:10,297 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:31:10,299 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:31:10,299 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:31:11,703 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:31:16,122 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:31:17,573 [INFO] AFC remote call 1 is done.
2025-06-28 13:33:22,602 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:33:22,603 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:33:22,604 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:33:22,605 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:33:22,606 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:33:22,607 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:33:22,607 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:33:22,612 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:33:22,613 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:33:22,614 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:33:22,615 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:33:22,616 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:33:22,617 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:33:22,618 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:33:22,620 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:33:23,304 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 13:33:26,291 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:33:26,295 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:33:26,296 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:33:26,298 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:33:26,304 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:33:26,305 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:33:26,314 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:33:26,315 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:33:26,320 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:33:26,323 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:33:26,325 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:33:26,326 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:33:26,328 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:33:26,349 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:33:26,361 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:33:28,767 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:33:28,769 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:33:28,770 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:33:28,771 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:33:28,772 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:33:28,773 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:33:28,774 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:33:28,775 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:33:28,776 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:33:28,779 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:33:28,785 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:33:28,787 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:33:28,789 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:33:28,790 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:33:28,791 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:33:30,065 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:33:34,878 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:33:34,883 [INFO] AFC remote call 1 is done.
2025-06-28 13:33:44,920 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:33:44,921 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:33:44,923 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:33:44,924 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:33:44,925 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:33:44,929 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:33:44,931 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:33:44,939 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:33:44,941 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:33:44,942 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:33:44,943 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:33:44,945 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:33:44,947 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:33:44,950 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:33:44,958 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:33:45,181 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:33:45,182 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:33:45,183 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:33:45,183 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:33:45,184 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:33:45,185 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:33:45,186 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:33:45,187 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:33:45,190 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:33:45,191 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:33:45,193 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:33:45,196 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:33:45,201 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:33:45,204 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:33:45,207 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:33:46,017 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:33:49,592 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:33:50,717 [INFO] AFC remote call 1 is done.
2025-06-28 13:34:31,093 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:34:31,094 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:34:31,094 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:34:31,095 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:34:31,096 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:34:31,096 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:34:31,097 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:34:31,098 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:34:31,099 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:34:31,100 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:34:31,101 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:34:31,104 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:34:31,106 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:34:31,107 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:34:31,108 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:34:31,912 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 13:34:35,264 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:34:35,267 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:34:35,269 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:34:35,282 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:34:35,295 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:34:35,297 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:34:35,302 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:34:35,306 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:34:35,308 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:34:35,309 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:34:35,310 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:34:35,328 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:34:35,341 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:34:35,342 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:34:35,343 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:34:35,576 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:34:35,577 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:34:35,578 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:34:35,579 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:34:35,580 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:34:35,581 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:34:35,582 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:34:35,583 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:34:35,586 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:34:35,590 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:34:35,592 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:34:35,593 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:34:35,594 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:34:35,595 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:34:35,596 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:34:36,756 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:34:40,485 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:34:41,475 [INFO] AFC remote call 1 is done.
2025-06-28 13:42:40,066 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:42:40,067 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:42:40,068 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:42:40,069 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:42:40,070 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:42:40,070 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:42:40,072 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:42:40,074 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:42:40,075 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:42:40,077 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:42:40,078 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:42:40,079 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:42:40,081 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:42:40,081 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:42:40,082 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:42:40,800 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:42:44,644 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 500 Internal Server Error"
2025-06-28 13:43:27,211 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:43:27,212 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:43:27,213 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:43:27,214 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:43:27,215 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:43:27,216 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:43:27,217 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:43:27,218 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:43:27,219 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:43:27,220 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:43:27,221 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:43:27,222 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:43:27,223 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:43:27,224 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:43:27,224 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:43:27,883 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 13:43:33,073 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:43:33,074 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:43:33,075 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:43:33,076 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:43:33,077 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:43:33,078 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:43:33,079 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:43:33,080 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:43:33,082 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:43:33,085 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:43:33,092 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:43:33,092 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:43:33,094 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:43:33,095 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:43:33,096 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:43:56,165 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:43:56,166 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:43:56,168 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:43:56,169 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:43:56,170 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:43:56,171 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:43:56,172 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:43:56,173 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:43:56,175 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:43:56,179 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:43:56,186 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:43:56,189 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:43:56,190 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:43:56,191 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:43:56,192 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:43:56,406 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:43:56,407 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:43:56,408 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:43:56,409 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:43:56,410 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:43:56,411 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:43:56,413 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:43:56,418 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:43:56,421 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:43:56,423 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:43:56,424 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:43:56,425 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:43:56,426 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:43:56,428 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:43:56,432 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:43:56,438 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: resuma a aplica√ß√£o enviada
2025-06-28 13:44:03,407 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:44:03,410 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:44:03,411 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:44:03,413 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:44:03,415 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:44:03,418 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:44:03,434 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:44:03,435 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:44:03,436 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:44:03,438 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:44:03,439 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:44:03,445 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:44:03,449 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:44:03,452 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:44:03,454 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:44:04,245 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:44:04,247 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:44:04,248 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:44:04,249 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:44:04,250 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:44:04,251 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:44:04,253 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:44:04,264 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:44:04,265 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:44:04,266 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:44:04,267 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:44:04,268 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:44:04,269 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:44:04,279 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:44:04,280 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:44:05,438 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:44:11,500 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:44:12,630 [INFO] AFC remote call 1 is done.
2025-06-28 13:44:18,871 [INFO] Resposta da IA (texto): Esta aplica√ß√£o √© um **Assistente de IA Multimodal** vers√°til, constru√≠do principalmente com **Streamlit** para uma interface de usu√°rio interativa e que se integra fortemente com a **API do Google Gemini**.

Aqui est√° um resumo detalhado de suas funcionalidades e arquitetura:

### 1. Funcionalidades Principais:

*   **Chat Interativo:**
    *   Permite conversas textuais com o modelo Gemini.
    *   O contexto do chat √© enriquecido com o pr√≥prio c√≥digo-fonte da aplica√ß√£o, tornando o assistente "c√¥nscio" de suas pr√≥prias funcionalidades.
*   **An√°lise de Imagens:**
    *   **Upload de Imagens:** Usu√°rios podem carregar arquivos de imagem (PNG, JPG, JPEG) diretamente via `st.file_uploader`.
    *   **Colar da √Årea de Transfer√™ncia:** Suporta colar imagens diretamente da √°rea de transfer√™ncia (clipboard), facilitando a an√°lise r√°pida de capturas de tela.
    *   Integra√ß√£o com o Gemini para descrever, analisar ou responder perguntas sobre as imagens fornecidas.
*   **Gera√ß√£o de Imagens:**
    *   Os usu√°rios podem fornecer um prompt de texto para gerar novas imagens usando o modelo `gemini-2.0-flash-exp-image-generation`.
    *   As imagens geradas s√£o exibidas no hist√≥rico do chat.
*   **Hist√≥rico de Conversas:** Mant√©m um hist√≥rico das mensagens e imagens trocadas na sess√£o do chat, exibindo-as de forma clara na interface.
*   **Processamento em Lote de Imagens (Funcionalidade Backend/Offline):**
    *   Um componente `ImageProcessor` permite analisar programaticamente um diret√≥rio de imagens (`assets/`).
    *   Para cada imagem, gera um resumo detalhado usando o Gemini.
    *   Salva os resumos e as imagens processadas em um documento **Microsoft Word (.docx)** e um arquivo **Markdown (.md)**.
    *   Move as imagens processadas para um diret√≥rio separado (`processed_images/`).
    *   Mant√©m um hist√≥rico JSON das an√°lises realizadas.

### 2. Arquitetura e Componentes-Chave:

*   **`chat_streamlit.py`:** O cora√ß√£o da interface de usu√°rio. Gerencia o estado da sess√£o (mensagens, imagens), interage com o usu√°rio, e orquestra as chamadas para os servi√ßos de IA e manipula√ß√£o de imagens.
*   **`main.py`:** O ponto de entrada para a funcionalidade de processamento em lote, utilizando o `ImageProcessor`.
*   **`config/config.py`:** Define vari√°veis de configura√ß√£o globais, como caminhos de diret√≥rios (ativos, logs, prompts, processados), nomes de arquivos de sa√≠da e configura√ß√µes de rate limiting.
*   **`core/`:**
    *   **`gemini_handler.py`:** Uma camada de abstra√ß√£o para interagir com a API do Gemini, incorporando o `RateLimiter`.
    *   **`rate_limiter.py`:** Uma classe fundamental que implementa um limitador de taxa robusto (usando `deque` e `Lock` para thread-safety), garantindo que as chamadas √† API do Gemini respeitem os limites de requisi√ß√µes por per√≠odo de tempo. √â usado tanto no chat interativo quanto no processamento em lote.
    *   **`logger_config.py`:** Configura um sistema de log centralizado para a aplica√ß√£o, registrando eventos em arquivo e no console.
    *   **`signal_handler.py`:** Lida com sinais do sistema (e.g., Ctrl+C) para permitir um encerramento gracioso da aplica√ß√£o.
*   **`services/`:**
    *   **`gpt_services.py`:** A classe principal para intera√ß√£o com a API Google Generative AI (Gemini), lidando com gera√ß√£o de conte√∫do de texto e de imagens (an√°lise multimodal).
    *   **`image_processor.py`:** Orquestra o fluxo de trabalho de processamento em lote de imagens: carrega prompts, chama o `gpt_handler` para an√°lise, e usa os servi√ßos de documento/markdown.
    *   **`document_service.py`:** Respons√°vel por criar e manipular documentos `.docx`, adicionando t√≠tulos, imagens e os resumos gerados pela IA.
    *   **`markdown_service.py`:** Similar ao `DocumentService`, mas focado em gerar arquivos `.md`.
    *   **`image_services.py`:** Um servi√ßo dedicado especificamente para a *gera√ß√£o* de imagens usando o modelo `gemini-2.0-flash-exp-image-generation`.
    *   **`search_files.py`:** Uma utilidade para ler o conte√∫do de todos os arquivos Python do projeto, que √© ent√£o injetado no prompt inicial do chat para fornecer contexto √† IA sobre sua pr√≥pria estrutura.
*   **`utils/file_utils.py`:** Fun√ß√µes auxiliares para manipula√ß√£o de arquivos, como listar imagens em um diret√≥rio.
*   **`test_image_generation.py`:** Um script de teste para a funcionalidade de gera√ß√£o de imagens.

### 3. Principais Destaques:

*   **Interatividade e Usabilidade:** A interface Streamlit torna o assistente f√°cil de usar para usu√°rios finais.
*   **Multimodalidade:** Capacidade de processar e interagir tanto com texto quanto com imagens.
*   **Autoconsci√™ncia (Contexto de C√≥digo):** A inje√ß√£o do pr√≥prio c√≥digo-fonte no prompt da IA √© uma caracter√≠stica inteligente que permite ao modelo entender melhor suas capacidades e a estrutura da aplica√ß√£o.
*   **Gerenciamento de API (Rate Limiting):** A implementa√ß√£o robusta de rate limiting √© crucial para aplica√ß√µes que dependem de APIs pagas ou com limites de uso.
*   **Relat√≥rios Automatizados:** A capacidade de gerar documentos Word e Markdown com as an√°lises √© valiosa para documenta√ß√£o e compartilhamento de resultados.

Em resumo, a aplica√ß√£o √© uma solu√ß√£o abrangente para intera√ß√£o com IA, combinando um frontend de chat din√¢mico com capacidades de processamento de imagem em segundo plano e gera√ß√£o de relat√≥rios, tudo isso enquanto gerencia eficientemente o uso da API.
2025-06-28 13:44:19,038 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:44:19,039 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:44:19,040 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:44:19,041 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:44:19,042 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:44:19,043 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:44:19,045 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:44:19,047 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:44:19,050 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:44:19,053 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:44:19,054 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:44:19,055 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:44:19,056 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:44:19,058 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:44:19,063 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:44:29,486 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:44:29,486 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:44:29,488 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:44:29,489 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:44:29,490 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:44:29,491 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:44:29,507 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:44:29,513 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:44:29,518 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:44:29,519 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:44:29,520 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:44:29,521 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:44:29,522 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:44:29,523 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:44:29,524 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:44:31,031 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:44:31,032 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:44:31,033 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:44:31,034 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:44:31,035 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:44:31,035 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:44:31,044 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:44:31,048 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:44:31,049 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:44:31,051 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:44:31,052 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:44:31,053 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:44:31,054 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:44:31,055 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:44:31,056 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:44:31,768 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:44:36,412 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:44:37,556 [INFO] AFC remote call 1 is done.
2025-06-28 13:45:14,148 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:45:14,149 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:45:14,150 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:45:14,152 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:45:14,153 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:45:14,154 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:45:14,156 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:45:14,159 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:45:14,241 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:45:14,243 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:45:14,244 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:45:14,304 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:45:14,316 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:45:14,340 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:45:14,358 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:45:14,585 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:45:14,586 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:45:14,587 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:45:14,588 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:45:14,589 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:45:14,590 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:45:14,592 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:45:14,596 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:45:14,597 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:45:14,603 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:45:14,605 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:45:14,607 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:45:14,609 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:45:14,610 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:45:14,612 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:45:14,614 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque estou recebendo esse erro nessa aplica√ß√£o sendo que no outro notebook a mesma aplica√ß√£o roda perfeitamente ?
2025-06-28 13:45:35,746 [INFO] Resposta da IA (texto): √â frustrante quando o mesmo c√≥digo se comporta de forma diferente em m√°quinas distintas! Isso quase sempre aponta para **diferen√ßas no ambiente de execu√ß√£o**, e n√£o no c√≥digo em si.

Analisando seu c√≥digo, o principal suspeito para um erro que ocorre em um notebook e n√£o em outro, especialmente se um for Windows e o outro Linux/macOS, √© o uso de `PIL.ImageGrab`.

---

**O SUSPEITO PRINCIPAL: `PIL.ImageGrab` (√Årea de Transfer√™ncia)**

No seu arquivo `chat_streamlit.py`, voc√™ tem a fun√ß√£o `check_clipboard()` que usa `PIL.ImageGrab`:

```python
# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab # <--- AQUI EST√Å O PROV√ÅVEL PROBLEMA

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        # ... (restante da fun√ß√£o)
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}") # <--- ESSA √â A MENSAGEM DE ERRO QUE VOC√ä DEVE ESTAR VENDO
        return None, None
```

A biblioteca `PIL.ImageGrab` (parte do Pillow) √© altamente dependente do sistema operacional:

1.  **Windows:** Geralmente funciona sem problemas, pois o Windows possui APIs diretas para acesso √† √°rea de transfer√™ncia de imagens.
2.  **Linux:** Requer utilit√°rios de linha de comando como `scrot` ou `gnome-screenshot` instalados e configurados corretamente, e a biblioteca `Pillow` precisa ser compilada com suporte a essas ferramentas. Muitas distribui√ß√µes Linux n√£o v√™m com isso pronto para o `ImageGrab`.
3.  **macOS:** Tamb√©m tem suas pr√≥prias depend√™ncias, geralmente exigindo bibliotecas como `pyobjc` para interagir com o ambiente Cocoa.

**Se o seu "outro notebook" roda Windows e o notebook com problema roda Linux ou macOS, este √© 99% a causa.** A mensagem de erro `"Erro ao acessar a √°rea de transfer√™ncia: {e}"` √© exatamente o que voc√™ veria na barra lateral do Streamlit se essa parte falhasse.

---

**Passos para Diagnosticar e Resolver:**

1.  **Obtenha a Mensagem de Erro Exata:**
    *   No notebook onde a aplica√ß√£o n√£o roda, verifique a barra lateral do Streamlit. Voc√™ ver√° a mensagem exata do erro que est√° ocorrendo, come√ßando com `"Erro ao acessar a √°rea de transfer√™ncia: ..."`. O que vem depois do `:` √© crucial.
    *   Se n√£o for `PIL.ImageGrab`, o erro estar√° em outro lugar.

2.  **Compare os Ambientes:**
    *   **Sistema Operacional:** Qual SO est√° rodando em cada notebook? (Ex: Windows 10 vs. Ubuntu 22.04 vs. macOS Ventura)
    *   **Vers√£o do Python:** `python --version` em ambos.
    *   **Depend√™ncias Instaladas:**
        *   No notebook que funciona, execute: `pip freeze > requirements_working.txt`
        *   No notebook que n√£o funciona, execute: `pip freeze > requirements_broken.txt`
        *   Compare os dois arquivos (`diff requirements_working.txt requirements_broken.txt`). Preste aten√ß√£o especial √† vers√£o do `Pillow` e a presen√ßa de pacotes como `pywin32` (Windows) ou qualquer coisa relacionada a screenshots em Linux/macOS.

3.  **Solu√ß√µes para o `PIL.ImageGrab`:**

    *   **Para Linux (Ubuntu/Debian):**
        *   Instale `scrot` (ou `gnome-screenshot`): `sudo apt-get update && sudo apt-get install scrot` (ou `sudo apt-get install gnome-screenshot`).
        *   Certifique-se de que o `Pillow` foi instalado *ap√≥s* a instala√ß√£o dessas ferramentas ou reinstale-o para que ele possa detectar as depend√™ncias.
        *   Pode ser necess√°rio tamb√©m `sudo apt-get install python3-tk python3-pil.imagetk` em algumas distribui√ß√µes.

    *   **Para macOS:**
        *   Pode ser necess√°rio instalar `pyobjc`: `pip install pyobjc`.

    *   **Solu√ß√£o Alternativa (Desativar ou Condicional):**
        Se voc√™ n√£o precisa desesperadamente da funcionalidade de colar imagem da √°rea de transfer√™ncia e quer que a aplica√ß√£o funcione em qualquer SO sem depend√™ncias externas complexas:
        Voc√™ j√° tem um `try-except` que lida com o erro. A aplica√ß√£o deve continuar funcionando mesmo se a funcionalidade de colar n√£o funcionar. No entanto, se o erro estiver parando a execu√ß√£o principal, isso √© um problema.

        **Para evitar o erro na barra lateral em ambientes n√£o-Windows:**
        Voc√™ pode adicionar uma verifica√ß√£o de sistema operacional:
        ```python
        import platform
        # ...

        def check_clipboard():
            # S√≥ tenta importar ImageGrab se for Windows
            if platform.system() == "Windows":
                try:
                    from PIL import ImageGrab
                    img = ImageGrab.grabclipboard()
                    # ... (restante do c√≥digo se img n√£o for None)
                    if img is not None and isinstance(img, Image.Image):
                        # ... seu c√≥digo para processar a imagem da √°rea de transfer√™ncia
                        # ...
                        return ClipboardFile(img_byte_arr), img
                    return None, None
                except Exception as e:
                    # A mensagem j√° √© exibida no Streamlit, ent√£o n√£o precisa de outro print aqui
                    st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
                    return None, None
            else:
                # Se n√£o for Windows, n√£o tenta acessar a √°rea de transfer√™ncia
                # E n√£o mostra erro na barra lateral
                # st.sidebar.info("Colar da √°rea de transfer√™ncia dispon√≠vel apenas no Windows.")
                return None, None
        ```
        Isso impediria que o erro espec√≠fico de `ImageGrab` fosse tentado em sistemas n√£o-Windows.

4.  **Verifique a Chave da API Gemini:**
    *   Certifique-se de que o arquivo `.env` existe no diret√≥rio raiz da aplica√ß√£o no notebook problem√°tico.
    *   Certifique-se de que `API_KEY_GEMINI` est√° definida corretamente dentro dele (Ex: `API_KEY_GEMINI="SUA_CHAVE_AQUI"`).
    *   Um erro relacionado √† API Key geralmente resultaria em `ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")` ou erros da pr√≥pria API (4xx, 5xx).

5.  **Verifique as Vers√µes das Depend√™ncias:**
    *   √â altamente recomend√°vel ter um `requirements.txt` e instal√°-lo em ambos os ambientes com `pip install -r requirements.txt`.
    *   Pode haver incompatibilidades sutis entre vers√µes de `streamlit`, `google-generativeai`, `Pillow`, etc.

6.  **Permiss√µes de Diret√≥rio:**
    *   Verifique se a aplica√ß√£o tem permiss√£o para criar e escrever nos diret√≥rios `assets/`, `logs/` e `processed_images/` no notebook problem√°tico. Isso √© mais comum em Linux/macOS com permiss√µes restritas.

7.  **Conectividade de Rede/Firewall:**
    *   O notebook problem√°tico consegue acessar a internet? Existem restri√ß√µes de firewall que podem estar bloqueando o acesso aos endpoints da API do Gemini?

---

**Resumo da A√ß√£o Mais Prov√°vel:**

O erro `Erro ao acessar a √°rea de transfer√™ncia: {e}` na barra lateral do Streamlit √© quase uma certeza de que o problema est√° na tentativa de usar `PIL.ImageGrab` em um sistema operacional (provavelmente Linux/macOS) que n√£o tem as depend√™ncias necess√°rias para isso.

Por favor, forne√ßa a mensagem de erro exata da barra lateral e o sistema operacional de ambos os notebooks para uma an√°lise mais precisa!
2025-06-28 13:45:35,983 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:45:35,984 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:45:35,986 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:45:35,989 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:45:35,990 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:45:35,992 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:45:35,996 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:45:36,004 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:45:36,006 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:45:36,007 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:45:36,010 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:45:36,011 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:45:36,013 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:45:36,014 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:45:36,017 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:47:19,204 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:47:19,205 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:47:19,206 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:47:19,207 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:47:19,208 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:47:19,209 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:47:19,210 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:47:19,212 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:47:19,215 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:47:19,221 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:47:19,222 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:47:19,223 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:47:19,224 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:47:19,224 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:47:19,225 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:47:19,478 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:47:19,479 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:47:19,480 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:47:19,481 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:47:19,482 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:47:19,483 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:47:19,484 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:47:19,486 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:47:19,489 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:47:19,491 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:47:19,492 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:47:19,493 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:47:19,495 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:47:19,496 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:47:19,498 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:47:19,502 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque estou recebendo esse erro nessa aplica√ß√£o sendo que no outro notebook a mesma aplica√ß√£o roda perfeitamente ?
Assistant: √â frustrante quando o mesmo c√≥digo se comporta de forma diferente em m√°quinas distintas! Isso quase sempre aponta para **diferen√ßas no ambiente de execu√ß√£o**, e n√£o no c√≥digo em si.

Analisando seu c√≥digo, o principal suspeito para um erro que ocorre em um notebook e n√£o em outro, especialmente se um for Windows e o outro Linux/macOS, √© o uso de `PIL.ImageGrab`.

---

**O SUSPEITO PRINCIPAL: `PIL.ImageGrab` (√Årea de Transfer√™ncia)**

No seu arquivo `chat_streamlit.py`, voc√™ tem a fun√ß√£o `check_clipboard()` que usa `PIL.ImageGrab`:

```python
# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab # <--- AQUI EST√Å O PROV√ÅVEL PROBLEMA

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        # ... (restante da fun√ß√£o)
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}") # <--- ESSA √â A MENSAGEM DE ERRO QUE VOC√ä DEVE ESTAR VENDO
        return None, None
```

A biblioteca `PIL.ImageGrab` (parte do Pillow) √© altamente dependente do sistema operacional:

1.  **Windows:** Geralmente funciona sem problemas, pois o Windows possui APIs diretas para acesso √† √°rea de transfer√™ncia de imagens.
2.  **Linux:** Requer utilit√°rios de linha de comando como `scrot` ou `gnome-screenshot` instalados e configurados corretamente, e a biblioteca `Pillow` precisa ser compilada com suporte a essas ferramentas. Muitas distribui√ß√µes Linux n√£o v√™m com isso pronto para o `ImageGrab`.
3.  **macOS:** Tamb√©m tem suas pr√≥prias depend√™ncias, geralmente exigindo bibliotecas como `pyobjc` para interagir com o ambiente Cocoa.

**Se o seu "outro notebook" roda Windows e o notebook com problema roda Linux ou macOS, este √© 99% a causa.** A mensagem de erro `"Erro ao acessar a √°rea de transfer√™ncia: {e}"` √© exatamente o que voc√™ veria na barra lateral do Streamlit se essa parte falhasse.

---

**Passos para Diagnosticar e Resolver:**

1.  **Obtenha a Mensagem de Erro Exata:**
    *   No notebook onde a aplica√ß√£o n√£o roda, verifique a barra lateral do Streamlit. Voc√™ ver√° a mensagem exata do erro que est√° ocorrendo, come√ßando com `"Erro ao acessar a √°rea de transfer√™ncia: ..."`. O que vem depois do `:` √© crucial.
    *   Se n√£o for `PIL.ImageGrab`, o erro estar√° em outro lugar.

2.  **Compare os Ambientes:**
    *   **Sistema Operacional:** Qual SO est√° rodando em cada notebook? (Ex: Windows 10 vs. Ubuntu 22.04 vs. macOS Ventura)
    *   **Vers√£o do Python:** `python --version` em ambos.
    *   **Depend√™ncias Instaladas:**
        *   No notebook que funciona, execute: `pip freeze > requirements_working.txt`
        *   No notebook que n√£o funciona, execute: `pip freeze > requirements_broken.txt`
        *   Compare os dois arquivos (`diff requirements_working.txt requirements_broken.txt`). Preste aten√ß√£o especial √† vers√£o do `Pillow` e a presen√ßa de pacotes como `pywin32` (Windows) ou qualquer coisa relacionada a screenshots em Linux/macOS.

3.  **Solu√ß√µes para o `PIL.ImageGrab`:**

    *   **Para Linux (Ubuntu/Debian):**
        *   Instale `scrot` (ou `gnome-screenshot`): `sudo apt-get update && sudo apt-get install scrot` (ou `sudo apt-get install gnome-screenshot`).
        *   Certifique-se de que o `Pillow` foi instalado *ap√≥s* a instala√ß√£o dessas ferramentas ou reinstale-o para que ele possa detectar as depend√™ncias.
        *   Pode ser necess√°rio tamb√©m `sudo apt-get install python3-tk python3-pil.imagetk` em algumas distribui√ß√µes.

    *   **Para macOS:**
        *   Pode ser necess√°rio instalar `pyobjc`: `pip install pyobjc`.

    *   **Solu√ß√£o Alternativa (Desativar ou Condicional):**
        Se voc√™ n√£o precisa desesperadamente da funcionalidade de colar imagem da √°rea de transfer√™ncia e quer que a aplica√ß√£o funcione em qualquer SO sem depend√™ncias externas complexas:
        Voc√™ j√° tem um `try-except` que lida com o erro. A aplica√ß√£o deve continuar funcionando mesmo se a funcionalidade de colar n√£o funcionar. No entanto, se o erro estiver parando a execu√ß√£o principal, isso √© um problema.

        **Para evitar o erro na barra lateral em ambientes n√£o-Windows:**
        Voc√™ pode adicionar uma verifica√ß√£o de sistema operacional:
        ```python
        import platform
        # ...

        def check_clipboard():
            # S√≥ tenta importar ImageGrab se for Windows
            if platform.system() == "Windows":
                try:
                    from PIL import ImageGrab
                    img = ImageGrab.grabclipboard()
                    # ... (restante do c√≥digo se img n√£o for None)
                    if img is not None and isinstance(img, Image.Image):
                        # ... seu c√≥digo para processar a imagem da √°rea de transfer√™ncia
                        # ...
                        return ClipboardFile(img_byte_arr), img
                    return None, None
                except Exception as e:
                    # A mensagem j√° √© exibida no Streamlit, ent√£o n√£o precisa de outro print aqui
                    st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
                    return None, None
            else:
                # Se n√£o for Windows, n√£o tenta acessar a √°rea de transfer√™ncia
                # E n√£o mostra erro na barra lateral
                # st.sidebar.info("Colar da √°rea de transfer√™ncia dispon√≠vel apenas no Windows.")
                return None, None
        ```
        Isso impediria que o erro espec√≠fico de `ImageGrab` fosse tentado em sistemas n√£o-Windows.

4.  **Verifique a Chave da API Gemini:**
    *   Certifique-se de que o arquivo `.env` existe no diret√≥rio raiz da aplica√ß√£o no notebook problem√°tico.
    *   Certifique-se de que `API_KEY_GEMINI` est√° definida corretamente dentro dele (Ex: `API_KEY_GEMINI="SUA_CHAVE_AQUI"`).
    *   Um erro relacionado √† API Key geralmente resultaria em `ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")` ou erros da pr√≥pria API (4xx, 5xx).

5.  **Verifique as Vers√µes das Depend√™ncias:**
    *   √â altamente recomend√°vel ter um `requirements.txt` e instal√°-lo em ambos os ambientes com `pip install -r requirements.txt`.
    *   Pode haver incompatibilidades sutis entre vers√µes de `streamlit`, `google-generativeai`, `Pillow`, etc.

6.  **Permiss√µes de Diret√≥rio:**
    *   Verifique se a aplica√ß√£o tem permiss√£o para criar e escrever nos diret√≥rios `assets/`, `logs/` e `processed_images/` no notebook problem√°tico. Isso √© mais comum em Linux/macOS com permiss√µes restritas.

7.  **Conectividade de Rede/Firewall:**
    *   O notebook problem√°tico consegue acessar a internet? Existem restri√ß√µes de firewall que podem estar bloqueando o acesso aos endpoints da API do Gemini?

---

**Resumo da A√ß√£o Mais Prov√°vel:**

O erro `Erro ao acessar a √°rea de transfer√™ncia: {e}` na barra lateral do Streamlit √© quase uma certeza de que o problema est√° na tentativa de usar `PIL.ImageGrab` em um sistema operacional (provavelmente Linux/macOS) que n√£o tem as depend√™ncias necess√°rias para isso.

Por favor, forne√ßa a mensagem de erro exata da barra lateral e o sistema operacional de ambos os notebooks para uma an√°lise mais precisa!
User: ois dois ambientes s√£o windows. 
2025-06-28 13:47:41,202 [INFO] Resposta da IA (texto): Certo, isso muda o cen√°rio! Se ambos os ambientes s√£o Windows, ent√£o o problema n√£o √© a incompatibilidade do `PIL.ImageGrab` com Linux/macOS. No entanto, o `PIL.ImageGrab` continua sendo o principal suspeito, mas por raz√µes diferentes.

Quando `PIL.ImageGrab` falha no Windows, geralmente aponta para um problema com a biblioteca `pywin32`, que o Pillow usa para interagir com as APIs do sistema operacional Windows.

Vamos desvendar isso.

---

**Causas Mais Comuns do `PIL.ImageGrab` Falhar no Windows:**

1.  **`pywin32` Ausente ou Corrompido:** A biblioteca `pywin32` √© essencial para que o Pillow (que inclui `PIL.ImageGrab`) acesse a √°rea de transfer√™ncia do Windows de forma nativa. Se ela n√£o estiver instalada ou estiver corrompida, a fun√ß√£o falhar√°.
2.  **Ambientes Python Diferentes:** Mesmo que ambos sejam Windows, voc√™ pode ter ambientes Python (globais ou virtuais) ligeiramente diferentes em cada m√°quina, onde uma tem `pywin32` e a outra n√£o, ou tem uma vers√£o incompat√≠vel.
3.  **Conflitos de Vers√£o:** Raramente, pode haver um conflito entre a vers√£o do `Pillow` e a vers√£o do `pywin32`, ou at√© mesmo com a vers√£o do Python.
4.  **Conte√∫do da √Årea de Transfer√™ncia:** Embora menos prov√°vel dado seu relato, o `ImageGrab.grabclipboard()` retornar√° `None` se o conte√∫do da √°rea de transfer√™ncia n√£o for uma imagem (ex: texto, arquivos). No entanto, o erro que voc√™ v√™ sugere que o *acesso* √† √°rea de transfer√™ncia est√° falhando, n√£o apenas que ela est√° vazia.

---

**Passos Detalhados de Diagn√≥stico e Solu√ß√£o:**

**1. Obtenha a Mensagem de Erro Exata:**
   Este √© o passo mais crucial. Quando voc√™ executa a aplica√ß√£o no notebook que est√° dando problema, qual √© a *mensagem de erro completa* que aparece na barra lateral do Streamlit, ap√≥s "Erro ao acessar a √°rea de transfer√™ncia: "?
   Ex: `"Erro ao acessar a √°rea de transfer√™ncia: [WinError 5] Acesso negado."` ou `"Erro ao acessar a √°rea de transfer√™ncia: No module named 'win32clipboard'"` etc.
   Esta mensagem detalhada nos dir√° exatamente o que est√° acontecendo.

**2. Compare as Depend√™ncias `pip`:**

   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que N√ÉO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos:** Abra `requirements_working.txt` e `requirements_broken.txt` e procure por:
        *   `Pillow` (P-I-L-L-O-W): Verifique as vers√µes.
        *   `pywin32`: **Este √© o mais importante.** Ele deve estar presente no `requirements_working.txt`. Se n√£o estiver no `requirements_broken.txt` ou se a vers√£o for diferente, encontramos o prov√°vel culpado.
        *   `streamlit`: Verifique as vers√µes.

**3. Instale/Reinstale `pywin32` (no notebook problem√°tico):**

   Se `pywin32` estiver faltando ou com uma vers√£o diferente, tente o seguinte:

   *   **Verifique o ambiente virtual:** Se voc√™ estiver usando um ambiente virtual (altamente recomendado), ative-o primeiro.
        ```bash
        # Exemplo para venv:
        cd C:\caminho\para\seu\projeto
        .\.venv\Scripts\activate  # ou .\venv\Scripts\activate
        ```
   *   **Instale `pywin32`:**
        ```bash
        pip install pywin32
        ```
   *   **Ou, se j√° estiver instalado, force a reinstala√ß√£o:**
        ```bash
        pip install --upgrade --force-reinstall pywin32
        ```
   *   **Reinicie a aplica√ß√£o Streamlit.**

**4. Teste a Funcionalidade `ImageGrab` Isoladamente:**

   Para confirmar que o problema √© realmente com `ImageGrab` e n√£o algo mais amplo com o Streamlit ou seu c√≥digo:

   *   Crie um pequeno script Python (`test_clipboard.py`) no notebook problem√°tico:
        ```python
        import sys
        from PIL import ImageGrab, Image
        import io
        import time

        try:
            # Tenta pegar imagem da √°rea de transfer√™ncia
            img = ImageGrab.grabclipboard()

            if img is not None and isinstance(img, Image.Image):
                print("Imagem encontrada na √°rea de transfer√™ncia!")
                # Opcional: Salvar a imagem para verificar
                # img.save("clipboard_test.png")
                # print("Imagem salva como clipboard_test.png")
                
                # Tenta converter para bytes (simular o que voc√™ faz no Streamlit)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format='PNG')
                img_byte_arr.seek(0)
                print(f"Tamanho dos bytes: {len(img_byte_arr.getvalue())} bytes")
                
            elif img is None:
                print("Nenhuma imagem na √°rea de transfer√™ncia ou o conte√∫do n√£o √© uma imagem.")
            else:
                print(f"Tipo de conte√∫do inesperado na √°rea de transfer√™ncia: {type(img)}")

        except ImportError as e:
            print(f"Erro: Depend√™ncia faltando para PIL.ImageGrab: {e}")
            print("Tente 'pip install pywin32' e certifique-se de que est√° no ambiente correto.")
        except Exception as e:
            print(f"Erro inesperado ao acessar a √°rea de transfer√™ncia: {e}")
            print("Verifique permiss√µes ou poss√≠veis conflitos.")

        print("\nTeste conclu√≠do.")
        ```
   *   **Copie uma imagem** (n√£o texto) para a √°rea de transfer√™ncia no Windows.
   *   Execute este script no terminal (ativando seu ambiente virtual, se aplic√°vel): `python test_clipboard.py`
   *   Observe a sa√≠da. Isso confirmar√° se o `ImageGrab` est√° funcionando fora do Streamlit e qual erro exato ele gera.

**5. Verifique a Vers√£o do Python e Arquitetura:**

   Embora menos comum hoje em dia, um Python de 32 bits pode ter problemas com algumas bibliotecas de 64 bits.

   *   Execute `python -c "import platform; print(platform.architecture())"` em ambos os notebooks. Verifique se ambos s√£o `('64bit', 'WindowsPE')`.

**6. Outras Verifica√ß√µes (Menos Prov√°veis):**

*   **Antiv√≠rus/Firewall:** Muito raramente, um software de seguran√ßa pode interferir no acesso a certas APIs do sistema. Tente desativ√°-lo temporariamente (se for seguro fazer isso) para testar.
*   **Permiss√µes de Usu√°rio:** Certifique-se de que o usu√°rio que executa a aplica√ß√£o Streamlit tem as permiss√µes necess√°rias para interagir com o sistema.

---

**Em Resumo:**

A principal a√ß√£o √© obter a **mensagem de erro exata** da barra lateral do Streamlit e, em seguida, **comparar as depend√™ncias `pip freeze`** para identificar se `pywin32` est√° faltando ou tem uma vers√£o diferente. O teste isolado com `test_clipboard.py` tamb√©m √© muito √∫til para isolar o problema.

Assim que voc√™ tiver a mensagem de erro exata, poderemos refinar ainda mais o diagn√≥stico!
2025-06-28 13:47:41,852 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:47:41,855 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:47:41,857 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:47:41,866 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:47:41,867 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:47:41,868 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:47:41,869 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:47:41,870 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:47:41,873 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:47:41,877 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:47:41,879 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:47:41,881 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:47:41,883 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:47:41,886 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:47:41,887 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:50:03,479 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:50:03,480 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:50:03,481 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:50:03,482 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:50:03,484 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:50:03,485 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:50:03,487 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:50:03,488 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:50:03,489 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:50:03,490 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:50:03,492 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:50:03,494 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:50:03,498 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:50:03,504 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:50:03,507 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:50:03,703 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:50:03,704 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:50:03,705 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:50:03,706 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:50:03,707 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:50:03,708 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:50:03,710 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:50:03,712 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:50:03,715 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:50:03,723 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:50:03,724 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:50:03,726 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:50:03,727 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:50:03,728 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:50:03,730 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:50:03,733 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: resuma a aplica√ß√£o enviada
Assistant: Esta aplica√ß√£o √© um **Assistente de IA Multimodal** vers√°til, constru√≠do principalmente com **Streamlit** para uma interface de usu√°rio interativa e que se integra fortemente com a **API do Google Gemini**.

Aqui est√° um resumo detalhado de suas funcionalidades e arquitetura:

### 1. Funcionalidades Principais:

*   **Chat Interativo:**
    *   Permite conversas textuais com o modelo Gemini.
    *   O contexto do chat √© enriquecido com o pr√≥prio c√≥digo-fonte da aplica√ß√£o, tornando o assistente "c√¥nscio" de suas pr√≥prias funcionalidades.
*   **An√°lise de Imagens:**
    *   **Upload de Imagens:** Usu√°rios podem carregar arquivos de imagem (PNG, JPG, JPEG) diretamente via `st.file_uploader`.
    *   **Colar da √Årea de Transfer√™ncia:** Suporta colar imagens diretamente da √°rea de transfer√™ncia (clipboard), facilitando a an√°lise r√°pida de capturas de tela.
    *   Integra√ß√£o com o Gemini para descrever, analisar ou responder perguntas sobre as imagens fornecidas.
*   **Gera√ß√£o de Imagens:**
    *   Os usu√°rios podem fornecer um prompt de texto para gerar novas imagens usando o modelo `gemini-2.0-flash-exp-image-generation`.
    *   As imagens geradas s√£o exibidas no hist√≥rico do chat.
*   **Hist√≥rico de Conversas:** Mant√©m um hist√≥rico das mensagens e imagens trocadas na sess√£o do chat, exibindo-as de forma clara na interface.
*   **Processamento em Lote de Imagens (Funcionalidade Backend/Offline):**
    *   Um componente `ImageProcessor` permite analisar programaticamente um diret√≥rio de imagens (`assets/`).
    *   Para cada imagem, gera um resumo detalhado usando o Gemini.
    *   Salva os resumos e as imagens processadas em um documento **Microsoft Word (.docx)** e um arquivo **Markdown (.md)**.
    *   Move as imagens processadas para um diret√≥rio separado (`processed_images/`).
    *   Mant√©m um hist√≥rico JSON das an√°lises realizadas.

### 2. Arquitetura e Componentes-Chave:

*   **`chat_streamlit.py`:** O cora√ß√£o da interface de usu√°rio. Gerencia o estado da sess√£o (mensagens, imagens), interage com o usu√°rio, e orquestra as chamadas para os servi√ßos de IA e manipula√ß√£o de imagens.
*   **`main.py`:** O ponto de entrada para a funcionalidade de processamento em lote, utilizando o `ImageProcessor`.
*   **`config/config.py`:** Define vari√°veis de configura√ß√£o globais, como caminhos de diret√≥rios (ativos, logs, prompts, processados), nomes de arquivos de sa√≠da e configura√ß√µes de rate limiting.
*   **`core/`:**
    *   **`gemini_handler.py`:** Uma camada de abstra√ß√£o para interagir com a API do Gemini, incorporando o `RateLimiter`.
    *   **`rate_limiter.py`:** Uma classe fundamental que implementa um limitador de taxa robusto (usando `deque` e `Lock` para thread-safety), garantindo que as chamadas √† API do Gemini respeitem os limites de requisi√ß√µes por per√≠odo de tempo. √â usado tanto no chat interativo quanto no processamento em lote.
    *   **`logger_config.py`:** Configura um sistema de log centralizado para a aplica√ß√£o, registrando eventos em arquivo e no console.
    *   **`signal_handler.py`:** Lida com sinais do sistema (e.g., Ctrl+C) para permitir um encerramento gracioso da aplica√ß√£o.
*   **`services/`:**
    *   **`gpt_services.py`:** A classe principal para intera√ß√£o com a API Google Generative AI (Gemini), lidando com gera√ß√£o de conte√∫do de texto e de imagens (an√°lise multimodal).
    *   **`image_processor.py`:** Orquestra o fluxo de trabalho de processamento em lote de imagens: carrega prompts, chama o `gpt_handler` para an√°lise, e usa os servi√ßos de documento/markdown.
    *   **`document_service.py`:** Respons√°vel por criar e manipular documentos `.docx`, adicionando t√≠tulos, imagens e os resumos gerados pela IA.
    *   **`markdown_service.py`:** Similar ao `DocumentService`, mas focado em gerar arquivos `.md`.
    *   **`image_services.py`:** Um servi√ßo dedicado especificamente para a *gera√ß√£o* de imagens usando o modelo `gemini-2.0-flash-exp-image-generation`.
    *   **`search_files.py`:** Uma utilidade para ler o conte√∫do de todos os arquivos Python do projeto, que √© ent√£o injetado no prompt inicial do chat para fornecer contexto √† IA sobre sua pr√≥pria estrutura.
*   **`utils/file_utils.py`:** Fun√ß√µes auxiliares para manipula√ß√£o de arquivos, como listar imagens em um diret√≥rio.
*   **`test_image_generation.py`:** Um script de teste para a funcionalidade de gera√ß√£o de imagens.

### 3. Principais Destaques:

*   **Interatividade e Usabilidade:** A interface Streamlit torna o assistente f√°cil de usar para usu√°rios finais.
*   **Multimodalidade:** Capacidade de processar e interagir tanto com texto quanto com imagens.
*   **Autoconsci√™ncia (Contexto de C√≥digo):** A inje√ß√£o do pr√≥prio c√≥digo-fonte no prompt da IA √© uma caracter√≠stica inteligente que permite ao modelo entender melhor suas capacidades e a estrutura da aplica√ß√£o.
*   **Gerenciamento de API (Rate Limiting):** A implementa√ß√£o robusta de rate limiting √© crucial para aplica√ß√µes que dependem de APIs pagas ou com limites de uso.
*   **Relat√≥rios Automatizados:** A capacidade de gerar documentos Word e Markdown com as an√°lises √© valiosa para documenta√ß√£o e compartilhamento de resultados.

Em resumo, a aplica√ß√£o √© uma solu√ß√£o abrangente para intera√ß√£o com IA, combinando um frontend de chat din√¢mico com capacidades de processamento de imagem em segundo plano e gera√ß√£o de relat√≥rios, tudo isso enquanto gerencia eficientemente o uso da API.
User: quais lugares dessa aplica√ß√£o ainda usa a gemini-2.0 ?
2025-06-28 13:50:12,143 [INFO] Resposta da IA (texto): A aplica√ß√£o ainda usa `gemini-2.0` exclusivamente para a funcionalidade de **gera√ß√£o de imagens**.

Em todos os outros contextos (chat interativo e processamento em lote de imagens para an√°lise), o modelo utilizado √© o `gemini-2.5-flash`.

Os lugares espec√≠ficos onde `gemini-2.0` √© usado para gera√ß√£o de imagens s√£o:

1.  **`chat_streamlit.py`** (na fun√ß√£o `generate_image`):
    ```python
    def generate_image(prompt):
        # ...
        client = genai.Client(api_key=api_key)
        try:
            response = client.models.generate_content(
                model='gemini-2.0-flash-exp-image-generation', # <--- Aqui
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_modalities=['Text', 'Image']
                )
            )
            # ...
    ```

2.  **`services\image_services.py`** (na fun√ß√£o `generate_image`):
    ```python
    def generate_image(prompt: str) -> Image.Image | None:
        try:
            model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation') # <--- Aqui
            response = model.generate_content(prompt)
            # ...
    ```

3.  **`test_image_generation.py`** (no script de teste):
    ```python
    response = client.models.generate_content(
        model='gemini-2.0-flash-exp-image-generation', # <--- Aqui
        contents=prompt,
        config=types.GenerateContentConfig(
            response_modalities=['Text', 'Image']
        )
    )
    ```

Isso significa que, para tarefas de conversa√ß√£o e an√°lise de imagens existentes, o modelo mais recente (`gemini-2.5-flash`) √© empregado, enquanto para a cria√ß√£o de novas imagens a partir do zero, o modelo `gemini-2.0-flash-exp-image-generation` √© o respons√°vel.
2025-06-28 13:50:12,309 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:50:12,310 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:50:12,311 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:50:12,312 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:50:12,313 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:50:12,314 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:50:12,315 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:50:12,317 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:50:12,318 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:50:12,323 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:50:12,324 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:50:12,325 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:50:12,327 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:50:12,330 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:50:12,333 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:52:27,983 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:52:27,985 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:52:27,985 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:52:27,986 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:52:27,987 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:52:27,988 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:52:27,989 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:52:27,990 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:52:27,992 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:52:27,994 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:52:27,995 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:52:27,996 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:52:27,997 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:52:27,998 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:52:27,999 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:52:28,193 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:52:28,194 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:52:28,195 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:52:28,197 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:52:28,201 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:52:28,203 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:52:28,204 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:52:28,205 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:52:28,207 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:52:28,208 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:52:28,210 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:52:28,211 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:52:28,212 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:52:28,213 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:52:28,215 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:52:28,218 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: resuma a aplica√ß√£o enviada
Assistant: Esta aplica√ß√£o √© um **Assistente de IA Multimodal** vers√°til, constru√≠do principalmente com **Streamlit** para uma interface de usu√°rio interativa e que se integra fortemente com a **API do Google Gemini**.

Aqui est√° um resumo detalhado de suas funcionalidades e arquitetura:

### 1. Funcionalidades Principais:

*   **Chat Interativo:**
    *   Permite conversas textuais com o modelo Gemini.
    *   O contexto do chat √© enriquecido com o pr√≥prio c√≥digo-fonte da aplica√ß√£o, tornando o assistente "c√¥nscio" de suas pr√≥prias funcionalidades.
*   **An√°lise de Imagens:**
    *   **Upload de Imagens:** Usu√°rios podem carregar arquivos de imagem (PNG, JPG, JPEG) diretamente via `st.file_uploader`.
    *   **Colar da √Årea de Transfer√™ncia:** Suporta colar imagens diretamente da √°rea de transfer√™ncia (clipboard), facilitando a an√°lise r√°pida de capturas de tela.
    *   Integra√ß√£o com o Gemini para descrever, analisar ou responder perguntas sobre as imagens fornecidas.
*   **Gera√ß√£o de Imagens:**
    *   Os usu√°rios podem fornecer um prompt de texto para gerar novas imagens usando o modelo `gemini-2.0-flash-exp-image-generation`.
    *   As imagens geradas s√£o exibidas no hist√≥rico do chat.
*   **Hist√≥rico de Conversas:** Mant√©m um hist√≥rico das mensagens e imagens trocadas na sess√£o do chat, exibindo-as de forma clara na interface.
*   **Processamento em Lote de Imagens (Funcionalidade Backend/Offline):**
    *   Um componente `ImageProcessor` permite analisar programaticamente um diret√≥rio de imagens (`assets/`).
    *   Para cada imagem, gera um resumo detalhado usando o Gemini.
    *   Salva os resumos e as imagens processadas em um documento **Microsoft Word (.docx)** e um arquivo **Markdown (.md)**.
    *   Move as imagens processadas para um diret√≥rio separado (`processed_images/`).
    *   Mant√©m um hist√≥rico JSON das an√°lises realizadas.

### 2. Arquitetura e Componentes-Chave:

*   **`chat_streamlit.py`:** O cora√ß√£o da interface de usu√°rio. Gerencia o estado da sess√£o (mensagens, imagens), interage com o usu√°rio, e orquestra as chamadas para os servi√ßos de IA e manipula√ß√£o de imagens.
*   **`main.py`:** O ponto de entrada para a funcionalidade de processamento em lote, utilizando o `ImageProcessor`.
*   **`config/config.py`:** Define vari√°veis de configura√ß√£o globais, como caminhos de diret√≥rios (ativos, logs, prompts, processados), nomes de arquivos de sa√≠da e configura√ß√µes de rate limiting.
*   **`core/`:**
    *   **`gemini_handler.py`:** Uma camada de abstra√ß√£o para interagir com a API do Gemini, incorporando o `RateLimiter`.
    *   **`rate_limiter.py`:** Uma classe fundamental que implementa um limitador de taxa robusto (usando `deque` e `Lock` para thread-safety), garantindo que as chamadas √† API do Gemini respeitem os limites de requisi√ß√µes por per√≠odo de tempo. √â usado tanto no chat interativo quanto no processamento em lote.
    *   **`logger_config.py`:** Configura um sistema de log centralizado para a aplica√ß√£o, registrando eventos em arquivo e no console.
    *   **`signal_handler.py`:** Lida com sinais do sistema (e.g., Ctrl+C) para permitir um encerramento gracioso da aplica√ß√£o.
*   **`services/`:**
    *   **`gpt_services.py`:** A classe principal para intera√ß√£o com a API Google Generative AI (Gemini), lidando com gera√ß√£o de conte√∫do de texto e de imagens (an√°lise multimodal).
    *   **`image_processor.py`:** Orquestra o fluxo de trabalho de processamento em lote de imagens: carrega prompts, chama o `gpt_handler` para an√°lise, e usa os servi√ßos de documento/markdown.
    *   **`document_service.py`:** Respons√°vel por criar e manipular documentos `.docx`, adicionando t√≠tulos, imagens e os resumos gerados pela IA.
    *   **`markdown_service.py`:** Similar ao `DocumentService`, mas focado em gerar arquivos `.md`.
    *   **`image_services.py`:** Um servi√ßo dedicado especificamente para a *gera√ß√£o* de imagens usando o modelo `gemini-2.0-flash-exp-image-generation`.
    *   **`search_files.py`:** Uma utilidade para ler o conte√∫do de todos os arquivos Python do projeto, que √© ent√£o injetado no prompt inicial do chat para fornecer contexto √† IA sobre sua pr√≥pria estrutura.
*   **`utils/file_utils.py`:** Fun√ß√µes auxiliares para manipula√ß√£o de arquivos, como listar imagens em um diret√≥rio.
*   **`test_image_generation.py`:** Um script de teste para a funcionalidade de gera√ß√£o de imagens.

### 3. Principais Destaques:

*   **Interatividade e Usabilidade:** A interface Streamlit torna o assistente f√°cil de usar para usu√°rios finais.
*   **Multimodalidade:** Capacidade de processar e interagir tanto com texto quanto com imagens.
*   **Autoconsci√™ncia (Contexto de C√≥digo):** A inje√ß√£o do pr√≥prio c√≥digo-fonte no prompt da IA √© uma caracter√≠stica inteligente que permite ao modelo entender melhor suas capacidades e a estrutura da aplica√ß√£o.
*   **Gerenciamento de API (Rate Limiting):** A implementa√ß√£o robusta de rate limiting √© crucial para aplica√ß√µes que dependem de APIs pagas ou com limites de uso.
*   **Relat√≥rios Automatizados:** A capacidade de gerar documentos Word e Markdown com as an√°lises √© valiosa para documenta√ß√£o e compartilhamento de resultados.

Em resumo, a aplica√ß√£o √© uma solu√ß√£o abrangente para intera√ß√£o com IA, combinando um frontend de chat din√¢mico com capacidades de processamento de imagem em segundo plano e gera√ß√£o de relat√≥rios, tudo isso enquanto gerencia eficientemente o uso da API.
User: quais lugares dessa aplica√ß√£o ainda usa a gemini-2.0 ?
Assistant: A aplica√ß√£o ainda usa `gemini-2.0` exclusivamente para a funcionalidade de **gera√ß√£o de imagens**.

Em todos os outros contextos (chat interativo e processamento em lote de imagens para an√°lise), o modelo utilizado √© o `gemini-2.5-flash`.

Os lugares espec√≠ficos onde `gemini-2.0` √© usado para gera√ß√£o de imagens s√£o:

1.  **`chat_streamlit.py`** (na fun√ß√£o `generate_image`):
    ```python
    def generate_image(prompt):
        # ...
        client = genai.Client(api_key=api_key)
        try:
            response = client.models.generate_content(
                model='gemini-2.0-flash-exp-image-generation', # <--- Aqui
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_modalities=['Text', 'Image']
                )
            )
            # ...
    ```

2.  **`services\image_services.py`** (na fun√ß√£o `generate_image`):
    ```python
    def generate_image(prompt: str) -> Image.Image | None:
        try:
            model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation') # <--- Aqui
            response = model.generate_content(prompt)
            # ...
    ```

3.  **`test_image_generation.py`** (no script de teste):
    ```python
    response = client.models.generate_content(
        model='gemini-2.0-flash-exp-image-generation', # <--- Aqui
        contents=prompt,
        config=types.GenerateContentConfig(
            response_modalities=['Text', 'Image']
        )
    )
    ```

Isso significa que, para tarefas de conversa√ß√£o e an√°lise de imagens existentes, o modelo mais recente (`gemini-2.5-flash`) √© empregado, enquanto para a cria√ß√£o de novas imagens a partir do zero, o modelo `gemini-2.0-flash-exp-image-generation` √© o respons√°vel.
User: e aonde usa a (2.0-flash)
2025-06-28 13:52:38,125 [INFO] Resposta da IA (texto): Voc√™ est√° certo em querer a precis√£o! Na verdade, a aplica√ß√£o utiliza o modelo `gemini-2.0-flash-exp-image-generation` especificamente para a funcionalidade de **gera√ß√£o de imagens**.

O termo `gemini-2.0-flash` √© a forma abreviada para se referir a esse modelo de gera√ß√£o de imagens, que √© uma vers√£o experimental (`exp`) otimizada para essa tarefa.

Portanto, os lugares onde voc√™ encontrar√° o uso do modelo de **gera√ß√£o de imagens** (`gemini-2.0-flash-exp-image-generation`, ou coloquialmente "2.0-flash" para gera√ß√£o) s√£o exatamente os mesmos que mencionei na resposta anterior:

1.  **`chat_streamlit.py`** (na fun√ß√£o `generate_image`):
    ```python
    def generate_image(prompt):
        # ...
        client = genai.Client(api_key=api_key)
        try:
            response = client.models.generate_content(
                model='gemini-2.0-flash-exp-image-generation', # <--- Aqui
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_modalities=['Text', 'Image']
                )
            )
            # ...
    ```

2.  **`services\image_services.py`** (na fun√ß√£o `generate_image`):
    ```python
    def generate_image(prompt: str) -> Image.Image | None:
        try:
            model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation') # <--- Aqui
            response = model.generate_content(prompt)
            # ...
    ```

3.  **`test_image_generation.py`** (no script de teste):
    ```python
    response = client.models.generate_content(
        model='gemini-2.0-flash-exp-image-generation', # <--- Aqui
        contents=prompt,
        config=types.GenerateContentConfig(
            response_modalities=['Text', 'Image']
        )
    )
    ```

**Para todas as outras intera√ß√µes (chat conversacional e an√°lise de imagens existentes), a aplica√ß√£o utiliza o modelo `gemini-2.5-flash`.**

Essa distin√ß√£o √© importante: o `gemini-2.5-flash` √© o modelo mais recente e mais geral para conversa√ß√£o e compreens√£o multimodal (an√°lise de imagem e texto), enquanto o `gemini-2.0-flash-exp-image-generation` √© um modelo especializado apenas para criar novas imagens a partir de prompts de texto.
2025-06-28 13:52:38,284 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:52:38,285 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:52:38,286 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:52:38,286 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:52:38,288 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:52:38,289 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:52:38,290 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:52:38,292 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:52:38,293 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:52:38,294 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:52:38,295 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:52:38,296 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:52:38,297 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:52:38,298 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:52:38,299 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:59:12,298 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:59:12,300 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:59:12,301 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:59:12,306 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:59:12,308 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:59:12,309 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:59:12,313 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:59:12,315 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:59:12,316 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:59:12,321 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:59:12,323 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:59:12,326 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:59:12,327 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:59:12,328 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:59:12,330 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:59:15,853 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:59:15,854 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:59:15,855 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:59:15,856 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:59:15,857 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:59:15,859 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:59:15,865 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:59:15,868 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:59:15,869 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:59:15,872 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:59:15,873 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:59:15,882 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:59:15,883 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:59:15,884 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:59:15,886 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:59:19,184 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:59:19,185 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:59:19,186 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:59:19,188 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:59:19,189 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:59:19,190 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:59:19,191 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:59:19,193 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:59:19,194 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:59:19,195 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:59:19,197 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:59:19,198 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:59:19,199 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:59:19,200 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:59:19,201 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:59:19,949 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:59:22,969 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:59:24,076 [INFO] AFC remote call 1 is done.
2025-06-28 14:01:49,459 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:01:49,460 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:01:49,461 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:01:49,462 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:01:49,463 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:01:49,464 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:01:49,465 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:01:49,466 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:01:49,467 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:01:49,469 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:01:49,470 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:01:49,471 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:01:49,473 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:01:49,474 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:01:49,483 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:39,802 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:39,866 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:39,870 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:39,882 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:39,884 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:39,934 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:39,935 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:40,097 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:40,123 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:40,126 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:40,127 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:40,130 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:40,131 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:40,138 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:40,143 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:40,147 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:40,149 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:40,165 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:40,195 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:40,210 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:40,214 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:40,214 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:40,216 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:40,230 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:40,262 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:40,263 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:40,264 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:40,266 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:40,267 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:40,269 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:40,270 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:40,270 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:40,279 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:40,289 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:40,333 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:40,333 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:40,334 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:40,335 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:40,337 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:40,337 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:40,339 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:40,349 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:40,524 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:40,525 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:40,529 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:40,541 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:40,756 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:40,759 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:40,764 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:40,765 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:40,766 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:40,768 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:40,780 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:40,781 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:40,807 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:40,836 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:40,840 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:40,847 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:40,848 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:40,851 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:40,865 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:40,865 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:40,869 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:40,900 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:40,926 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,095 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:41,096 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,097 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:41,098 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,103 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:41,153 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,166 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,182 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,184 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:41,184 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:41,190 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,196 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:41,198 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:41,199 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:41,202 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:41,203 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:41,245 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:41,245 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,266 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:41,267 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:41,268 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:41,275 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:41,280 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:41,284 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:41,284 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:41,286 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:41,468 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,469 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:41,475 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:41,476 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:41,477 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:41,477 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,478 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,479 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:41,480 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:41,481 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:41,482 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:41,482 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:41,483 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,484 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,485 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:41,485 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:41,486 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:41,487 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:41,488 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:41,490 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,492 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:41,493 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:41,496 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:41,497 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:41,498 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,499 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:41,500 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:41,500 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:41,501 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:41,502 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:41,502 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:41,503 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,504 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,505 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:41,506 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:41,507 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:41,508 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:41,512 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:41,513 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:41,513 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:41,514 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,515 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,515 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:41,515 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:41,516 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:41,517 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:41,517 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:41,774 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,775 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,775 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:41,776 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:41,777 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:41,916 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:41,916 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:41,918 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,918 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:41,918 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:41,919 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,919 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:41,920 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:41,921 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:41,921 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,922 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:41,923 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:41,925 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:42,070 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:42,071 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:42,072 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:42,072 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:42,073 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:42,073 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:42,074 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:42,075 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:42,075 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:42,076 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:42,076 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:42,077 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:42,077 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:42,078 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:42,079 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:42,079 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:42,081 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:42,084 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:42,085 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:42,226 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:42,227 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:42,228 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:42,228 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:42,355 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:42,355 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:42,356 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:42,356 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:42,357 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:42,357 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:42,358 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:42,358 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:42,359 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:42,361 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:42,362 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:42,617 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:42,617 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:42,618 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:42,619 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:42,746 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:04:53,466 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:04:53,468 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:04:53,469 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:04:53,469 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:04:53,470 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:04:53,471 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:04:53,473 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:04:53,474 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:04:53,478 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:04:53,482 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:04:53,483 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:04:53,484 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:04:53,485 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:04:53,486 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:04:53,490 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:04:54,261 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:04:57,530 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 14:04:58,629 [INFO] AFC remote call 1 is done.
2025-06-28 14:05:21,464 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:05:21,465 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:05:21,466 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:05:21,467 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:05:21,468 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:05:21,470 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:05:21,472 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:05:21,475 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:05:21,479 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:05:21,482 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:05:21,484 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:05:21,491 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:05:21,492 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:05:21,493 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:05:21,494 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:05:21,719 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:05:21,720 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:05:21,721 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:05:21,722 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:05:21,723 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:05:21,724 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:05:21,727 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:05:21,730 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:05:21,736 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:05:21,737 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:05:21,738 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:05:21,739 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:05:21,740 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:05:21,742 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:05:21,743 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:05:21,746 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque estou recebendo esse erro nessa aplica√ß√£o sendo que no outro notebook a mesma aplica√ß√£o roda perfeitamente ?
Assistant: √â frustrante quando o mesmo c√≥digo se comporta de forma diferente em m√°quinas distintas! Isso quase sempre aponta para **diferen√ßas no ambiente de execu√ß√£o**, e n√£o no c√≥digo em si.

Analisando seu c√≥digo, o principal suspeito para um erro que ocorre em um notebook e n√£o em outro, especialmente se um for Windows e o outro Linux/macOS, √© o uso de `PIL.ImageGrab`.

---

**O SUSPEITO PRINCIPAL: `PIL.ImageGrab` (√Årea de Transfer√™ncia)**

No seu arquivo `chat_streamlit.py`, voc√™ tem a fun√ß√£o `check_clipboard()` que usa `PIL.ImageGrab`:

```python
# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab # <--- AQUI EST√Å O PROV√ÅVEL PROBLEMA

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        # ... (restante da fun√ß√£o)
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}") # <--- ESSA √â A MENSAGEM DE ERRO QUE VOC√ä DEVE ESTAR VENDO
        return None, None
```

A biblioteca `PIL.ImageGrab` (parte do Pillow) √© altamente dependente do sistema operacional:

1.  **Windows:** Geralmente funciona sem problemas, pois o Windows possui APIs diretas para acesso √† √°rea de transfer√™ncia de imagens.
2.  **Linux:** Requer utilit√°rios de linha de comando como `scrot` ou `gnome-screenshot` instalados e configurados corretamente, e a biblioteca `Pillow` precisa ser compilada com suporte a essas ferramentas. Muitas distribui√ß√µes Linux n√£o v√™m com isso pronto para o `ImageGrab`.
3.  **macOS:** Tamb√©m tem suas pr√≥prias depend√™ncias, geralmente exigindo bibliotecas como `pyobjc` para interagir com o ambiente Cocoa.

**Se o seu "outro notebook" roda Windows e o notebook com problema roda Linux ou macOS, este √© 99% a causa.** A mensagem de erro `"Erro ao acessar a √°rea de transfer√™ncia: {e}"` √© exatamente o que voc√™ veria na barra lateral do Streamlit se essa parte falhasse.

---

**Passos para Diagnosticar e Resolver:**

1.  **Obtenha a Mensagem de Erro Exata:**
    *   No notebook onde a aplica√ß√£o n√£o roda, verifique a barra lateral do Streamlit. Voc√™ ver√° a mensagem exata do erro que est√° ocorrendo, come√ßando com `"Erro ao acessar a √°rea de transfer√™ncia: ..."`. O que vem depois do `:` √© crucial.
    *   Se n√£o for `PIL.ImageGrab`, o erro estar√° em outro lugar.

2.  **Compare os Ambientes:**
    *   **Sistema Operacional:** Qual SO est√° rodando em cada notebook? (Ex: Windows 10 vs. Ubuntu 22.04 vs. macOS Ventura)
    *   **Vers√£o do Python:** `python --version` em ambos.
    *   **Depend√™ncias Instaladas:**
        *   No notebook que funciona, execute: `pip freeze > requirements_working.txt`
        *   No notebook que n√£o funciona, execute: `pip freeze > requirements_broken.txt`
        *   Compare os dois arquivos (`diff requirements_working.txt requirements_broken.txt`). Preste aten√ß√£o especial √† vers√£o do `Pillow` e a presen√ßa de pacotes como `pywin32` (Windows) ou qualquer coisa relacionada a screenshots em Linux/macOS.

3.  **Solu√ß√µes para o `PIL.ImageGrab`:**

    *   **Para Linux (Ubuntu/Debian):**
        *   Instale `scrot` (ou `gnome-screenshot`): `sudo apt-get update && sudo apt-get install scrot` (ou `sudo apt-get install gnome-screenshot`).
        *   Certifique-se de que o `Pillow` foi instalado *ap√≥s* a instala√ß√£o dessas ferramentas ou reinstale-o para que ele possa detectar as depend√™ncias.
        *   Pode ser necess√°rio tamb√©m `sudo apt-get install python3-tk python3-pil.imagetk` em algumas distribui√ß√µes.

    *   **Para macOS:**
        *   Pode ser necess√°rio instalar `pyobjc`: `pip install pyobjc`.

    *   **Solu√ß√£o Alternativa (Desativar ou Condicional):**
        Se voc√™ n√£o precisa desesperadamente da funcionalidade de colar imagem da √°rea de transfer√™ncia e quer que a aplica√ß√£o funcione em qualquer SO sem depend√™ncias externas complexas:
        Voc√™ j√° tem um `try-except` que lida com o erro. A aplica√ß√£o deve continuar funcionando mesmo se a funcionalidade de colar n√£o funcionar. No entanto, se o erro estiver parando a execu√ß√£o principal, isso √© um problema.

        **Para evitar o erro na barra lateral em ambientes n√£o-Windows:**
        Voc√™ pode adicionar uma verifica√ß√£o de sistema operacional:
        ```python
        import platform
        # ...

        def check_clipboard():
            # S√≥ tenta importar ImageGrab se for Windows
            if platform.system() == "Windows":
                try:
                    from PIL import ImageGrab
                    img = ImageGrab.grabclipboard()
                    # ... (restante do c√≥digo se img n√£o for None)
                    if img is not None and isinstance(img, Image.Image):
                        # ... seu c√≥digo para processar a imagem da √°rea de transfer√™ncia
                        # ...
                        return ClipboardFile(img_byte_arr), img
                    return None, None
                except Exception as e:
                    # A mensagem j√° √© exibida no Streamlit, ent√£o n√£o precisa de outro print aqui
                    st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
                    return None, None
            else:
                # Se n√£o for Windows, n√£o tenta acessar a √°rea de transfer√™ncia
                # E n√£o mostra erro na barra lateral
                # st.sidebar.info("Colar da √°rea de transfer√™ncia dispon√≠vel apenas no Windows.")
                return None, None
        ```
        Isso impediria que o erro espec√≠fico de `ImageGrab` fosse tentado em sistemas n√£o-Windows.

4.  **Verifique a Chave da API Gemini:**
    *   Certifique-se de que o arquivo `.env` existe no diret√≥rio raiz da aplica√ß√£o no notebook problem√°tico.
    *   Certifique-se de que `API_KEY_GEMINI` est√° definida corretamente dentro dele (Ex: `API_KEY_GEMINI="SUA_CHAVE_AQUI"`).
    *   Um erro relacionado √† API Key geralmente resultaria em `ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")` ou erros da pr√≥pria API (4xx, 5xx).

5.  **Verifique as Vers√µes das Depend√™ncias:**
    *   √â altamente recomend√°vel ter um `requirements.txt` e instal√°-lo em ambos os ambientes com `pip install -r requirements.txt`.
    *   Pode haver incompatibilidades sutis entre vers√µes de `streamlit`, `google-generativeai`, `Pillow`, etc.

6.  **Permiss√µes de Diret√≥rio:**
    *   Verifique se a aplica√ß√£o tem permiss√£o para criar e escrever nos diret√≥rios `assets/`, `logs/` e `processed_images/` no notebook problem√°tico. Isso √© mais comum em Linux/macOS com permiss√µes restritas.

7.  **Conectividade de Rede/Firewall:**
    *   O notebook problem√°tico consegue acessar a internet? Existem restri√ß√µes de firewall que podem estar bloqueando o acesso aos endpoints da API do Gemini?

---

**Resumo da A√ß√£o Mais Prov√°vel:**

O erro `Erro ao acessar a √°rea de transfer√™ncia: {e}` na barra lateral do Streamlit √© quase uma certeza de que o problema est√° na tentativa de usar `PIL.ImageGrab` em um sistema operacional (provavelmente Linux/macOS) que n√£o tem as depend√™ncias necess√°rias para isso.

Por favor, forne√ßa a mensagem de erro exata da barra lateral e o sistema operacional de ambos os notebooks para uma an√°lise mais precisa!
User: ois dois ambientes s√£o windows. 
Assistant: Certo, isso muda o cen√°rio! Se ambos os ambientes s√£o Windows, ent√£o o problema n√£o √© a incompatibilidade do `PIL.ImageGrab` com Linux/macOS. No entanto, o `PIL.ImageGrab` continua sendo o principal suspeito, mas por raz√µes diferentes.

Quando `PIL.ImageGrab` falha no Windows, geralmente aponta para um problema com a biblioteca `pywin32`, que o Pillow usa para interagir com as APIs do sistema operacional Windows.

Vamos desvendar isso.

---

**Causas Mais Comuns do `PIL.ImageGrab` Falhar no Windows:**

1.  **`pywin32` Ausente ou Corrompido:** A biblioteca `pywin32` √© essencial para que o Pillow (que inclui `PIL.ImageGrab`) acesse a √°rea de transfer√™ncia do Windows de forma nativa. Se ela n√£o estiver instalada ou estiver corrompida, a fun√ß√£o falhar√°.
2.  **Ambientes Python Diferentes:** Mesmo que ambos sejam Windows, voc√™ pode ter ambientes Python (globais ou virtuais) ligeiramente diferentes em cada m√°quina, onde uma tem `pywin32` e a outra n√£o, ou tem uma vers√£o incompat√≠vel.
3.  **Conflitos de Vers√£o:** Raramente, pode haver um conflito entre a vers√£o do `Pillow` e a vers√£o do `pywin32`, ou at√© mesmo com a vers√£o do Python.
4.  **Conte√∫do da √Årea de Transfer√™ncia:** Embora menos prov√°vel dado seu relato, o `ImageGrab.grabclipboard()` retornar√° `None` se o conte√∫do da √°rea de transfer√™ncia n√£o for uma imagem (ex: texto, arquivos). No entanto, o erro que voc√™ v√™ sugere que o *acesso* √† √°rea de transfer√™ncia est√° falhando, n√£o apenas que ela est√° vazia.

---

**Passos Detalhados de Diagn√≥stico e Solu√ß√£o:**

**1. Obtenha a Mensagem de Erro Exata:**
   Este √© o passo mais crucial. Quando voc√™ executa a aplica√ß√£o no notebook que est√° dando problema, qual √© a *mensagem de erro completa* que aparece na barra lateral do Streamlit, ap√≥s "Erro ao acessar a √°rea de transfer√™ncia: "?
   Ex: `"Erro ao acessar a √°rea de transfer√™ncia: [WinError 5] Acesso negado."` ou `"Erro ao acessar a √°rea de transfer√™ncia: No module named 'win32clipboard'"` etc.
   Esta mensagem detalhada nos dir√° exatamente o que est√° acontecendo.

**2. Compare as Depend√™ncias `pip`:**

   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que N√ÉO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos:** Abra `requirements_working.txt` e `requirements_broken.txt` e procure por:
        *   `Pillow` (P-I-L-L-O-W): Verifique as vers√µes.
        *   `pywin32`: **Este √© o mais importante.** Ele deve estar presente no `requirements_working.txt`. Se n√£o estiver no `requirements_broken.txt` ou se a vers√£o for diferente, encontramos o prov√°vel culpado.
        *   `streamlit`: Verifique as vers√µes.

**3. Instale/Reinstale `pywin32` (no notebook problem√°tico):**

   Se `pywin32` estiver faltando ou com uma vers√£o diferente, tente o seguinte:

   *   **Verifique o ambiente virtual:** Se voc√™ estiver usando um ambiente virtual (altamente recomendado), ative-o primeiro.
        ```bash
        # Exemplo para venv:
        cd C:\caminho\para\seu\projeto
        .\.venv\Scripts\activate  # ou .\venv\Scripts\activate
        ```
   *   **Instale `pywin32`:**
        ```bash
        pip install pywin32
        ```
   *   **Ou, se j√° estiver instalado, force a reinstala√ß√£o:**
        ```bash
        pip install --upgrade --force-reinstall pywin32
        ```
   *   **Reinicie a aplica√ß√£o Streamlit.**

**4. Teste a Funcionalidade `ImageGrab` Isoladamente:**

   Para confirmar que o problema √© realmente com `ImageGrab` e n√£o algo mais amplo com o Streamlit ou seu c√≥digo:

   *   Crie um pequeno script Python (`test_clipboard.py`) no notebook problem√°tico:
        ```python
        import sys
        from PIL import ImageGrab, Image
        import io
        import time

        try:
            # Tenta pegar imagem da √°rea de transfer√™ncia
            img = ImageGrab.grabclipboard()

            if img is not None and isinstance(img, Image.Image):
                print("Imagem encontrada na √°rea de transfer√™ncia!")
                # Opcional: Salvar a imagem para verificar
                # img.save("clipboard_test.png")
                # print("Imagem salva como clipboard_test.png")
                
                # Tenta converter para bytes (simular o que voc√™ faz no Streamlit)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format='PNG')
                img_byte_arr.seek(0)
                print(f"Tamanho dos bytes: {len(img_byte_arr.getvalue())} bytes")
                
            elif img is None:
                print("Nenhuma imagem na √°rea de transfer√™ncia ou o conte√∫do n√£o √© uma imagem.")
            else:
                print(f"Tipo de conte√∫do inesperado na √°rea de transfer√™ncia: {type(img)}")

        except ImportError as e:
            print(f"Erro: Depend√™ncia faltando para PIL.ImageGrab: {e}")
            print("Tente 'pip install pywin32' e certifique-se de que est√° no ambiente correto.")
        except Exception as e:
            print(f"Erro inesperado ao acessar a √°rea de transfer√™ncia: {e}")
            print("Verifique permiss√µes ou poss√≠veis conflitos.")

        print("\nTeste conclu√≠do.")
        ```
   *   **Copie uma imagem** (n√£o texto) para a √°rea de transfer√™ncia no Windows.
   *   Execute este script no terminal (ativando seu ambiente virtual, se aplic√°vel): `python test_clipboard.py`
   *   Observe a sa√≠da. Isso confirmar√° se o `ImageGrab` est√° funcionando fora do Streamlit e qual erro exato ele gera.

**5. Verifique a Vers√£o do Python e Arquitetura:**

   Embora menos comum hoje em dia, um Python de 32 bits pode ter problemas com algumas bibliotecas de 64 bits.

   *   Execute `python -c "import platform; print(platform.architecture())"` em ambos os notebooks. Verifique se ambos s√£o `('64bit', 'WindowsPE')`.

**6. Outras Verifica√ß√µes (Menos Prov√°veis):**

*   **Antiv√≠rus/Firewall:** Muito raramente, um software de seguran√ßa pode interferir no acesso a certas APIs do sistema. Tente desativ√°-lo temporariamente (se for seguro fazer isso) para testar.
*   **Permiss√µes de Usu√°rio:** Certifique-se de que o usu√°rio que executa a aplica√ß√£o Streamlit tem as permiss√µes necess√°rias para interagir com o sistema.

---

**Em Resumo:**

A principal a√ß√£o √© obter a **mensagem de erro exata** da barra lateral do Streamlit e, em seguida, **comparar as depend√™ncias `pip freeze`** para identificar se `pywin32` est√° faltando ou tem uma vers√£o diferente. O teste isolado com `test_clipboard.py` tamb√©m √© muito √∫til para isolar o problema.

Assim que voc√™ tiver a mensagem de erro exata, poderemos refinar ainda mais o diagn√≥stico!
User: Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E13620360>
2025-06-28 14:05:48,121 [INFO] Resposta da IA (texto): Ok, essa nova informa√ß√£o muda completamente o diagn√≥stico! O erro `Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x...>` **n√£o tem a ver com a √°rea de transfer√™ncia (`PIL.ImageGrab`)**.

Este erro significa que a biblioteca `Pillow` (PIL) tentou abrir algo que n√£o reconheceu como uma imagem v√°lida. A mensagem `_io.BytesIO object` indica que ela recebeu um fluxo de bytes, mas esses bytes n√£o formam uma imagem (JPEG, PNG, etc.) que ela possa decodificar.

Vamos focar na fun√ß√£o `generate_image` no `chat_streamlit.py`:

```python
# chat_streamlit.py

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # ... (verifica√ß√£o da API_KEY) ...

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text) # Se for s√≥ texto, imprime e continua o loop ou retorna None
            elif part.inline_data is not None:
                # <-- AQUI √â O PONTO DO ERRO
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image # Retorna a imagem e sai da fun√ß√£o

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # <-- √â aqui que o erro √© capturado e exibido
        return None
```

A linha `image = Image.open(io.BytesIO(part.inline_data.data))` √© a que est√° falhando. Isso significa que, por algum motivo, o `part.inline_data.data` que o Gemini est√° retornando **n√£o √© um conjunto de bytes de uma imagem v√°lida** no notebook problem√°tico, ou est√° vazio.

---

**Poss√≠veis Causas e Como Depurar:**

1.  **O Gemini n√£o est√° gerando uma imagem para aquele prompt/requisi√ß√£o espec√≠fica:**
    *   √Äs vezes, a API de gera√ß√£o de imagem do Gemini pode n√£o conseguir gerar uma imagem para um prompt, ou pode bloquear a gera√ß√£o por motivos de seguran√ßa/conte√∫do. Nesses casos, `part.inline_data` pode ser `None` ou `part.inline_data.data` pode estar vazio ou conter dados de erro/texto em vez de dados de imagem.
    *   Se o `response.prompt_feedback` tiver um `block_reason` (motivo de bloqueio), isso pode indicar o problema.

2.  **Problema de conectividade/API key:**
    *   Embora voc√™ j√° tenha verificado, √© bom garantir que a `API_KEY_GEMINI` esteja configurada *exatamente* da mesma forma em ambos os notebooks. Um problema na chave pode levar a respostas malformadas da API, mesmo que n√£o seja um erro √≥bvio de autentica√ß√£o.
    *   H√° alguma restri√ß√£o de rede/firewall no notebook problem√°tico que possa estar afetando a comunica√ß√£o com a API do Gemini, fazendo com que as respostas venham incompletas ou corrompidas?

3.  **Vers√£o da biblioteca `google-generativeai` ou `Pillow`:**
    *   Mesmo sendo Windows, pode haver pequenas diferen√ßas nas vers√µes dessas bibliotecas que causem um comportamento diferente.

---

**Passos para Diagnosticar e Resolver:**

**1. Teste o `test_image_generation.py` no notebook problem√°tico:**

   Voc√™ tem um script de teste simples (`test_image_generation.py`) que faz uma chamada direta para a API de gera√ß√£o de imagem.
   *   Copie e execute o `test_image_generation.py` no notebook que est√° apresentando o erro.
   *   Ele deve tentar gerar "ed-image.png".
   *   **Resultado esperado:**
        *   **Se `test_image_generation.py` tamb√©m falhar com o mesmo erro (`cannot identify image file`),** ent√£o o problema √© mais fundamental na comunica√ß√£o com o Gemini ou na forma como os dados s√£o recebidos/processados pelo Pillow *naquele ambiente*.
        *   **Se `test_image_generation.py` funcionar e gerar a imagem corretamente,** ent√£o o problema est√° em algo mais espec√≠fico no fluxo do `chat_streamlit.py` ao chamar `generate_image`.

**2. Adicione Logs Detalhados na Fun√ß√£o `generate_image` (em `chat_streamlit.py`):**

   Vamos adicionar mais `print` statements (ou usar `logger.info` se preferir ver no logfile) para entender o que est√° vindo da API antes de chamar `Image.open()`.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Certifique-se que io est√° importado

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI n√£o encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug print

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG ---
           print(f"\nDEBUG: Objeto de resposta Gemini completo: {response}")
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Raz√£o de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Gera√ß√£o de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # N√£o h√° imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA n√£o retornou candidatos de gera√ß√£o. Tente um prompt diferente.")
               return None

           found_image = False
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo: {type(part)}")
               if part.text is not None:
                   print(f"DEBUG: Conte√∫do de texto recebido: '{part.text}'")
                   # Se houver texto, voc√™ pode decidir se quer us√°-lo ou continuar procurando por imagem
               elif part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Tenta abrir a imagem
                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data √© None ou vazio, apesar de inline_data n√£o ser None.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline.")
           
           if not found_image:
               st.warning("O Gemini n√£o retornou uma imagem para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este √© o erro que voc√™ est√° vendo
           return None
   ```
   Execute a aplica√ß√£o com estes prints e observe a sa√≠da no terminal onde voc√™ executa `streamlit run chat_streamlit.py`. Isso nos dar√° pistas vitais sobre o que o Gemini est√° realmente retornando.

**3. Compare as Depend√™ncias Novamente:**

   Embora menos prov√°vel, vale a pena uma revis√£o minuciosa.
   *   No notebook que FUNCIONA, execute: `pip freeze > requirements_working.txt`
   *   No notebook que N√ÉO FUNCIONA, execute: `pip freeze > requirements_broken.txt`
   *   Compare os arquivos, prestando aten√ß√£o especial a:
        *   `Pillow` (ou PIL)
        *   `google-generativeai`
        *   `streamlit`
   *   Se houver diferen√ßa, tente instalar as vers√µes do `requirements_working.txt` no notebook problem√°tico.

**4. Reinstale `Pillow`:**

   Mesmo que as vers√µes sejam as mesmas, uma instala√ß√£o corrompida pode ser o problema.
   *   Ative seu ambiente virtual (se estiver usando).
   *   Execute: `pip install --upgrade --force-reinstall Pillow`
   *   Reinicie a aplica√ß√£o Streamlit.

---

**Conclus√£o Preliminar:**

O erro `cannot identify image file` na fun√ß√£o `generate_image` sugere fortemente que os dados que o Gemini retorna para `part.inline_data.data` **n√£o s√£o dados de imagem v√°lidos** no ambiente problem√°tico. Os passos de depura√ß√£o acima, especialmente a execu√ß√£o do `test_image_generation.py` e os `print`s detalhados na fun√ß√£o `generate_image` no `chat_streamlit.py`, nos dar√£o as informa√ß√µes necess√°rias para identificar a causa raiz.
2025-06-28 14:05:48,351 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:05:48,352 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:05:48,353 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:05:48,354 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:05:48,355 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:05:48,356 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:05:48,358 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:05:48,359 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:05:48,360 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:05:48,361 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:05:48,363 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:05:48,364 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:05:48,365 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:05:48,370 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:05:48,372 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:07:06,186 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:07:06,188 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:07:06,189 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:07:06,190 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:07:06,191 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:07:06,192 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:07:06,193 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:07:06,198 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:07:06,199 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:07:06,201 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:07:06,202 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:07:06,204 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:07:06,205 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:07:06,206 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:07:06,208 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:07:06,726 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:07:06,727 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:07:06,728 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:07:06,729 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:07:06,729 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:07:06,730 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:07:06,731 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:07:06,733 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:07:06,734 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:07:06,736 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:07:06,738 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:07:06,739 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:07:06,741 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:07:06,742 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:07:06,744 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:07:06,750 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque estou recebendo esse erro nessa aplica√ß√£o sendo que no outro notebook a mesma aplica√ß√£o roda perfeitamente ?
Assistant: √â frustrante quando o mesmo c√≥digo se comporta de forma diferente em m√°quinas distintas! Isso quase sempre aponta para **diferen√ßas no ambiente de execu√ß√£o**, e n√£o no c√≥digo em si.

Analisando seu c√≥digo, o principal suspeito para um erro que ocorre em um notebook e n√£o em outro, especialmente se um for Windows e o outro Linux/macOS, √© o uso de `PIL.ImageGrab`.

---

**O SUSPEITO PRINCIPAL: `PIL.ImageGrab` (√Årea de Transfer√™ncia)**

No seu arquivo `chat_streamlit.py`, voc√™ tem a fun√ß√£o `check_clipboard()` que usa `PIL.ImageGrab`:

```python
# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab # <--- AQUI EST√Å O PROV√ÅVEL PROBLEMA

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        # ... (restante da fun√ß√£o)
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}") # <--- ESSA √â A MENSAGEM DE ERRO QUE VOC√ä DEVE ESTAR VENDO
        return None, None
```

A biblioteca `PIL.ImageGrab` (parte do Pillow) √© altamente dependente do sistema operacional:

1.  **Windows:** Geralmente funciona sem problemas, pois o Windows possui APIs diretas para acesso √† √°rea de transfer√™ncia de imagens.
2.  **Linux:** Requer utilit√°rios de linha de comando como `scrot` ou `gnome-screenshot` instalados e configurados corretamente, e a biblioteca `Pillow` precisa ser compilada com suporte a essas ferramentas. Muitas distribui√ß√µes Linux n√£o v√™m com isso pronto para o `ImageGrab`.
3.  **macOS:** Tamb√©m tem suas pr√≥prias depend√™ncias, geralmente exigindo bibliotecas como `pyobjc` para interagir com o ambiente Cocoa.

**Se o seu "outro notebook" roda Windows e o notebook com problema roda Linux ou macOS, este √© 99% a causa.** A mensagem de erro `"Erro ao acessar a √°rea de transfer√™ncia: {e}"` √© exatamente o que voc√™ veria na barra lateral do Streamlit se essa parte falhasse.

---

**Passos para Diagnosticar e Resolver:**

1.  **Obtenha a Mensagem de Erro Exata:**
    *   No notebook onde a aplica√ß√£o n√£o roda, verifique a barra lateral do Streamlit. Voc√™ ver√° a mensagem exata do erro que est√° ocorrendo, come√ßando com `"Erro ao acessar a √°rea de transfer√™ncia: ..."`. O que vem depois do `:` √© crucial.
    *   Se n√£o for `PIL.ImageGrab`, o erro estar√° em outro lugar.

2.  **Compare os Ambientes:**
    *   **Sistema Operacional:** Qual SO est√° rodando em cada notebook? (Ex: Windows 10 vs. Ubuntu 22.04 vs. macOS Ventura)
    *   **Vers√£o do Python:** `python --version` em ambos.
    *   **Depend√™ncias Instaladas:**
        *   No notebook que funciona, execute: `pip freeze > requirements_working.txt`
        *   No notebook que n√£o funciona, execute: `pip freeze > requirements_broken.txt`
        *   Compare os dois arquivos (`diff requirements_working.txt requirements_broken.txt`). Preste aten√ß√£o especial √† vers√£o do `Pillow` e a presen√ßa de pacotes como `pywin32` (Windows) ou qualquer coisa relacionada a screenshots em Linux/macOS.

3.  **Solu√ß√µes para o `PIL.ImageGrab`:**

    *   **Para Linux (Ubuntu/Debian):**
        *   Instale `scrot` (ou `gnome-screenshot`): `sudo apt-get update && sudo apt-get install scrot` (ou `sudo apt-get install gnome-screenshot`).
        *   Certifique-se de que o `Pillow` foi instalado *ap√≥s* a instala√ß√£o dessas ferramentas ou reinstale-o para que ele possa detectar as depend√™ncias.
        *   Pode ser necess√°rio tamb√©m `sudo apt-get install python3-tk python3-pil.imagetk` em algumas distribui√ß√µes.

    *   **Para macOS:**
        *   Pode ser necess√°rio instalar `pyobjc`: `pip install pyobjc`.

    *   **Solu√ß√£o Alternativa (Desativar ou Condicional):**
        Se voc√™ n√£o precisa desesperadamente da funcionalidade de colar imagem da √°rea de transfer√™ncia e quer que a aplica√ß√£o funcione em qualquer SO sem depend√™ncias externas complexas:
        Voc√™ j√° tem um `try-except` que lida com o erro. A aplica√ß√£o deve continuar funcionando mesmo se a funcionalidade de colar n√£o funcionar. No entanto, se o erro estiver parando a execu√ß√£o principal, isso √© um problema.

        **Para evitar o erro na barra lateral em ambientes n√£o-Windows:**
        Voc√™ pode adicionar uma verifica√ß√£o de sistema operacional:
        ```python
        import platform
        # ...

        def check_clipboard():
            # S√≥ tenta importar ImageGrab se for Windows
            if platform.system() == "Windows":
                try:
                    from PIL import ImageGrab
                    img = ImageGrab.grabclipboard()
                    # ... (restante do c√≥digo se img n√£o for None)
                    if img is not None and isinstance(img, Image.Image):
                        # ... seu c√≥digo para processar a imagem da √°rea de transfer√™ncia
                        # ...
                        return ClipboardFile(img_byte_arr), img
                    return None, None
                except Exception as e:
                    # A mensagem j√° √© exibida no Streamlit, ent√£o n√£o precisa de outro print aqui
                    st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
                    return None, None
            else:
                # Se n√£o for Windows, n√£o tenta acessar a √°rea de transfer√™ncia
                # E n√£o mostra erro na barra lateral
                # st.sidebar.info("Colar da √°rea de transfer√™ncia dispon√≠vel apenas no Windows.")
                return None, None
        ```
        Isso impediria que o erro espec√≠fico de `ImageGrab` fosse tentado em sistemas n√£o-Windows.

4.  **Verifique a Chave da API Gemini:**
    *   Certifique-se de que o arquivo `.env` existe no diret√≥rio raiz da aplica√ß√£o no notebook problem√°tico.
    *   Certifique-se de que `API_KEY_GEMINI` est√° definida corretamente dentro dele (Ex: `API_KEY_GEMINI="SUA_CHAVE_AQUI"`).
    *   Um erro relacionado √† API Key geralmente resultaria em `ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")` ou erros da pr√≥pria API (4xx, 5xx).

5.  **Verifique as Vers√µes das Depend√™ncias:**
    *   √â altamente recomend√°vel ter um `requirements.txt` e instal√°-lo em ambos os ambientes com `pip install -r requirements.txt`.
    *   Pode haver incompatibilidades sutis entre vers√µes de `streamlit`, `google-generativeai`, `Pillow`, etc.

6.  **Permiss√µes de Diret√≥rio:**
    *   Verifique se a aplica√ß√£o tem permiss√£o para criar e escrever nos diret√≥rios `assets/`, `logs/` e `processed_images/` no notebook problem√°tico. Isso √© mais comum em Linux/macOS com permiss√µes restritas.

7.  **Conectividade de Rede/Firewall:**
    *   O notebook problem√°tico consegue acessar a internet? Existem restri√ß√µes de firewall que podem estar bloqueando o acesso aos endpoints da API do Gemini?

---

**Resumo da A√ß√£o Mais Prov√°vel:**

O erro `Erro ao acessar a √°rea de transfer√™ncia: {e}` na barra lateral do Streamlit √© quase uma certeza de que o problema est√° na tentativa de usar `PIL.ImageGrab` em um sistema operacional (provavelmente Linux/macOS) que n√£o tem as depend√™ncias necess√°rias para isso.

Por favor, forne√ßa a mensagem de erro exata da barra lateral e o sistema operacional de ambos os notebooks para uma an√°lise mais precisa!
User: ois dois ambientes s√£o windows. 
Assistant: Certo, isso muda o cen√°rio! Se ambos os ambientes s√£o Windows, ent√£o o problema n√£o √© a incompatibilidade do `PIL.ImageGrab` com Linux/macOS. No entanto, o `PIL.ImageGrab` continua sendo o principal suspeito, mas por raz√µes diferentes.

Quando `PIL.ImageGrab` falha no Windows, geralmente aponta para um problema com a biblioteca `pywin32`, que o Pillow usa para interagir com as APIs do sistema operacional Windows.

Vamos desvendar isso.

---

**Causas Mais Comuns do `PIL.ImageGrab` Falhar no Windows:**

1.  **`pywin32` Ausente ou Corrompido:** A biblioteca `pywin32` √© essencial para que o Pillow (que inclui `PIL.ImageGrab`) acesse a √°rea de transfer√™ncia do Windows de forma nativa. Se ela n√£o estiver instalada ou estiver corrompida, a fun√ß√£o falhar√°.
2.  **Ambientes Python Diferentes:** Mesmo que ambos sejam Windows, voc√™ pode ter ambientes Python (globais ou virtuais) ligeiramente diferentes em cada m√°quina, onde uma tem `pywin32` e a outra n√£o, ou tem uma vers√£o incompat√≠vel.
3.  **Conflitos de Vers√£o:** Raramente, pode haver um conflito entre a vers√£o do `Pillow` e a vers√£o do `pywin32`, ou at√© mesmo com a vers√£o do Python.
4.  **Conte√∫do da √Årea de Transfer√™ncia:** Embora menos prov√°vel dado seu relato, o `ImageGrab.grabclipboard()` retornar√° `None` se o conte√∫do da √°rea de transfer√™ncia n√£o for uma imagem (ex: texto, arquivos). No entanto, o erro que voc√™ v√™ sugere que o *acesso* √† √°rea de transfer√™ncia est√° falhando, n√£o apenas que ela est√° vazia.

---

**Passos Detalhados de Diagn√≥stico e Solu√ß√£o:**

**1. Obtenha a Mensagem de Erro Exata:**
   Este √© o passo mais crucial. Quando voc√™ executa a aplica√ß√£o no notebook que est√° dando problema, qual √© a *mensagem de erro completa* que aparece na barra lateral do Streamlit, ap√≥s "Erro ao acessar a √°rea de transfer√™ncia: "?
   Ex: `"Erro ao acessar a √°rea de transfer√™ncia: [WinError 5] Acesso negado."` ou `"Erro ao acessar a √°rea de transfer√™ncia: No module named 'win32clipboard'"` etc.
   Esta mensagem detalhada nos dir√° exatamente o que est√° acontecendo.

**2. Compare as Depend√™ncias `pip`:**

   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que N√ÉO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos:** Abra `requirements_working.txt` e `requirements_broken.txt` e procure por:
        *   `Pillow` (P-I-L-L-O-W): Verifique as vers√µes.
        *   `pywin32`: **Este √© o mais importante.** Ele deve estar presente no `requirements_working.txt`. Se n√£o estiver no `requirements_broken.txt` ou se a vers√£o for diferente, encontramos o prov√°vel culpado.
        *   `streamlit`: Verifique as vers√µes.

**3. Instale/Reinstale `pywin32` (no notebook problem√°tico):**

   Se `pywin32` estiver faltando ou com uma vers√£o diferente, tente o seguinte:

   *   **Verifique o ambiente virtual:** Se voc√™ estiver usando um ambiente virtual (altamente recomendado), ative-o primeiro.
        ```bash
        # Exemplo para venv:
        cd C:\caminho\para\seu\projeto
        .\.venv\Scripts\activate  # ou .\venv\Scripts\activate
        ```
   *   **Instale `pywin32`:**
        ```bash
        pip install pywin32
        ```
   *   **Ou, se j√° estiver instalado, force a reinstala√ß√£o:**
        ```bash
        pip install --upgrade --force-reinstall pywin32
        ```
   *   **Reinicie a aplica√ß√£o Streamlit.**

**4. Teste a Funcionalidade `ImageGrab` Isoladamente:**

   Para confirmar que o problema √© realmente com `ImageGrab` e n√£o algo mais amplo com o Streamlit ou seu c√≥digo:

   *   Crie um pequeno script Python (`test_clipboard.py`) no notebook problem√°tico:
        ```python
        import sys
        from PIL import ImageGrab, Image
        import io
        import time

        try:
            # Tenta pegar imagem da √°rea de transfer√™ncia
            img = ImageGrab.grabclipboard()

            if img is not None and isinstance(img, Image.Image):
                print("Imagem encontrada na √°rea de transfer√™ncia!")
                # Opcional: Salvar a imagem para verificar
                # img.save("clipboard_test.png")
                # print("Imagem salva como clipboard_test.png")
                
                # Tenta converter para bytes (simular o que voc√™ faz no Streamlit)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format='PNG')
                img_byte_arr.seek(0)
                print(f"Tamanho dos bytes: {len(img_byte_arr.getvalue())} bytes")
                
            elif img is None:
                print("Nenhuma imagem na √°rea de transfer√™ncia ou o conte√∫do n√£o √© uma imagem.")
            else:
                print(f"Tipo de conte√∫do inesperado na √°rea de transfer√™ncia: {type(img)}")

        except ImportError as e:
            print(f"Erro: Depend√™ncia faltando para PIL.ImageGrab: {e}")
            print("Tente 'pip install pywin32' e certifique-se de que est√° no ambiente correto.")
        except Exception as e:
            print(f"Erro inesperado ao acessar a √°rea de transfer√™ncia: {e}")
            print("Verifique permiss√µes ou poss√≠veis conflitos.")

        print("\nTeste conclu√≠do.")
        ```
   *   **Copie uma imagem** (n√£o texto) para a √°rea de transfer√™ncia no Windows.
   *   Execute este script no terminal (ativando seu ambiente virtual, se aplic√°vel): `python test_clipboard.py`
   *   Observe a sa√≠da. Isso confirmar√° se o `ImageGrab` est√° funcionando fora do Streamlit e qual erro exato ele gera.

**5. Verifique a Vers√£o do Python e Arquitetura:**

   Embora menos comum hoje em dia, um Python de 32 bits pode ter problemas com algumas bibliotecas de 64 bits.

   *   Execute `python -c "import platform; print(platform.architecture())"` em ambos os notebooks. Verifique se ambos s√£o `('64bit', 'WindowsPE')`.

**6. Outras Verifica√ß√µes (Menos Prov√°veis):**

*   **Antiv√≠rus/Firewall:** Muito raramente, um software de seguran√ßa pode interferir no acesso a certas APIs do sistema. Tente desativ√°-lo temporariamente (se for seguro fazer isso) para testar.
*   **Permiss√µes de Usu√°rio:** Certifique-se de que o usu√°rio que executa a aplica√ß√£o Streamlit tem as permiss√µes necess√°rias para interagir com o sistema.

---

**Em Resumo:**

A principal a√ß√£o √© obter a **mensagem de erro exata** da barra lateral do Streamlit e, em seguida, **comparar as depend√™ncias `pip freeze`** para identificar se `pywin32` est√° faltando ou tem uma vers√£o diferente. O teste isolado com `test_clipboard.py` tamb√©m √© muito √∫til para isolar o problema.

Assim que voc√™ tiver a mensagem de erro exata, poderemos refinar ainda mais o diagn√≥stico!
User: Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E13620360>
Assistant: Ok, essa nova informa√ß√£o muda completamente o diagn√≥stico! O erro `Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x...>` **n√£o tem a ver com a √°rea de transfer√™ncia (`PIL.ImageGrab`)**.

Este erro significa que a biblioteca `Pillow` (PIL) tentou abrir algo que n√£o reconheceu como uma imagem v√°lida. A mensagem `_io.BytesIO object` indica que ela recebeu um fluxo de bytes, mas esses bytes n√£o formam uma imagem (JPEG, PNG, etc.) que ela possa decodificar.

Vamos focar na fun√ß√£o `generate_image` no `chat_streamlit.py`:

```python
# chat_streamlit.py

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # ... (verifica√ß√£o da API_KEY) ...

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text) # Se for s√≥ texto, imprime e continua o loop ou retorna None
            elif part.inline_data is not None:
                # <-- AQUI √â O PONTO DO ERRO
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image # Retorna a imagem e sai da fun√ß√£o

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # <-- √â aqui que o erro √© capturado e exibido
        return None
```

A linha `image = Image.open(io.BytesIO(part.inline_data.data))` √© a que est√° falhando. Isso significa que, por algum motivo, o `part.inline_data.data` que o Gemini est√° retornando **n√£o √© um conjunto de bytes de uma imagem v√°lida** no notebook problem√°tico, ou est√° vazio.

---

**Poss√≠veis Causas e Como Depurar:**

1.  **O Gemini n√£o est√° gerando uma imagem para aquele prompt/requisi√ß√£o espec√≠fica:**
    *   √Äs vezes, a API de gera√ß√£o de imagem do Gemini pode n√£o conseguir gerar uma imagem para um prompt, ou pode bloquear a gera√ß√£o por motivos de seguran√ßa/conte√∫do. Nesses casos, `part.inline_data` pode ser `None` ou `part.inline_data.data` pode estar vazio ou conter dados de erro/texto em vez de dados de imagem.
    *   Se o `response.prompt_feedback` tiver um `block_reason` (motivo de bloqueio), isso pode indicar o problema.

2.  **Problema de conectividade/API key:**
    *   Embora voc√™ j√° tenha verificado, √© bom garantir que a `API_KEY_GEMINI` esteja configurada *exatamente* da mesma forma em ambos os notebooks. Um problema na chave pode levar a respostas malformadas da API, mesmo que n√£o seja um erro √≥bvio de autentica√ß√£o.
    *   H√° alguma restri√ß√£o de rede/firewall no notebook problem√°tico que possa estar afetando a comunica√ß√£o com a API do Gemini, fazendo com que as respostas venham incompletas ou corrompidas?

3.  **Vers√£o da biblioteca `google-generativeai` ou `Pillow`:**
    *   Mesmo sendo Windows, pode haver pequenas diferen√ßas nas vers√µes dessas bibliotecas que causem um comportamento diferente.

---

**Passos para Diagnosticar e Resolver:**

**1. Teste o `test_image_generation.py` no notebook problem√°tico:**

   Voc√™ tem um script de teste simples (`test_image_generation.py`) que faz uma chamada direta para a API de gera√ß√£o de imagem.
   *   Copie e execute o `test_image_generation.py` no notebook que est√° apresentando o erro.
   *   Ele deve tentar gerar "ed-image.png".
   *   **Resultado esperado:**
        *   **Se `test_image_generation.py` tamb√©m falhar com o mesmo erro (`cannot identify image file`),** ent√£o o problema √© mais fundamental na comunica√ß√£o com o Gemini ou na forma como os dados s√£o recebidos/processados pelo Pillow *naquele ambiente*.
        *   **Se `test_image_generation.py` funcionar e gerar a imagem corretamente,** ent√£o o problema est√° em algo mais espec√≠fico no fluxo do `chat_streamlit.py` ao chamar `generate_image`.

**2. Adicione Logs Detalhados na Fun√ß√£o `generate_image` (em `chat_streamlit.py`):**

   Vamos adicionar mais `print` statements (ou usar `logger.info` se preferir ver no logfile) para entender o que est√° vindo da API antes de chamar `Image.open()`.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Certifique-se que io est√° importado

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI n√£o encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug print

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG ---
           print(f"\nDEBUG: Objeto de resposta Gemini completo: {response}")
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Raz√£o de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Gera√ß√£o de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # N√£o h√° imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA n√£o retornou candidatos de gera√ß√£o. Tente um prompt diferente.")
               return None

           found_image = False
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo: {type(part)}")
               if part.text is not None:
                   print(f"DEBUG: Conte√∫do de texto recebido: '{part.text}'")
                   # Se houver texto, voc√™ pode decidir se quer us√°-lo ou continuar procurando por imagem
               elif part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Tenta abrir a imagem
                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data √© None ou vazio, apesar de inline_data n√£o ser None.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline.")
           
           if not found_image:
               st.warning("O Gemini n√£o retornou uma imagem para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este √© o erro que voc√™ est√° vendo
           return None
   ```
   Execute a aplica√ß√£o com estes prints e observe a sa√≠da no terminal onde voc√™ executa `streamlit run chat_streamlit.py`. Isso nos dar√° pistas vitais sobre o que o Gemini est√° realmente retornando.

**3. Compare as Depend√™ncias Novamente:**

   Embora menos prov√°vel, vale a pena uma revis√£o minuciosa.
   *   No notebook que FUNCIONA, execute: `pip freeze > requirements_working.txt`
   *   No notebook que N√ÉO FUNCIONA, execute: `pip freeze > requirements_broken.txt`
   *   Compare os arquivos, prestando aten√ß√£o especial a:
        *   `Pillow` (ou PIL)
        *   `google-generativeai`
        *   `streamlit`
   *   Se houver diferen√ßa, tente instalar as vers√µes do `requirements_working.txt` no notebook problem√°tico.

**4. Reinstale `Pillow`:**

   Mesmo que as vers√µes sejam as mesmas, uma instala√ß√£o corrompida pode ser o problema.
   *   Ative seu ambiente virtual (se estiver usando).
   *   Execute: `pip install --upgrade --force-reinstall Pillow`
   *   Reinicie a aplica√ß√£o Streamlit.

---

**Conclus√£o Preliminar:**

O erro `cannot identify image file` na fun√ß√£o `generate_image` sugere fortemente que os dados que o Gemini retorna para `part.inline_data.data` **n√£o s√£o dados de imagem v√°lidos** no ambiente problem√°tico. Os passos de depura√ß√£o acima, especialmente a execu√ß√£o do `test_image_generation.py` e os `print`s detalhados na fun√ß√£o `generate_image` no `chat_streamlit.py`, nos dar√£o as informa√ß√µes necess√°rias para identificar a causa raiz.
User: mas essa mesma aplica√ß√£o funciona no meu outro notebook sem erros.
2025-06-28 14:07:31,291 [INFO] Resposta da IA (texto): Entendido! Se o problema persiste no segundo notebook, mesmo ambos sendo Windows, e o erro √© `cannot identify image file <_io.BytesIO object at 0x...>`, isso significa que a quest√£o n√£o √© a compatibilidade do sistema operacional com `PIL.ImageGrab`, mas sim algo espec√≠fico na forma como os **dados da imagem gerada pelo Gemini** est√£o chegando ou sendo processados no notebook problem√°tico.

Este erro (`cannot identify image file`) ocorre quando a biblioteca `Pillow` (que √© usada para abrir a imagem com `Image.open`) recebe um fluxo de bytes (`_io.BytesIO`) que ela n√£o consegue reconhecer como um formato de imagem v√°lido (PNG, JPG, etc.).

Vamos focar na fun√ß√£o `generate_image` no `chat_streamlit.py` e nos dados que ela recebe da API do Gemini.

### Causas Prov√°veis do Erro `cannot identify image file` (mesmo em Windows):

1.  **Dados de Imagem Inv√°lidos/Incompletos da API do Gemini:**
    *   **API Key com Problema:** Embora voc√™ tenha verificado, √†s vezes uma API Key pode ter restri√ß√µes, ou a pr√≥pria chamada de API pode estar retornando uma resposta malformada ou um erro sutil que n√£o √© um `HTTP 200 OK` com dados de imagem.
    *   **Bloqueio de Conte√∫do:** O prompt pode estar sendo bloqueado pelos filtros de seguran√ßa do Gemini, e em vez de uma imagem, a API retorna uma resposta vazia, uma mensagem de erro em texto, ou um `prompt_feedback` indicando um bloqueio, mas o c√≥digo tenta processar `inline_data.data` como se fosse uma imagem.
    *   **Problema de Conectividade de Rede:** No notebook problem√°tico, pode haver alguma instabilidade de rede, firewall ou proxy que est√° corrompendo ou truncando os dados da imagem durante a transfer√™ncia da API do Gemini para a sua aplica√ß√£o. Os bytes chegam incompletos ou inv√°lidos, e o `Pillow` n√£o consegue decodific√°-los.

2.  **Problema na Instala√ß√£o ou Vers√£o do `Pillow` (ou `google-generativeai`):**
    *   **Instala√ß√£o Corrompida:** Apesar de ser Windows, a instala√ß√£o do Pillow pode ter algum arquivo corrompido ou faltando no ambiente Python do notebook problem√°tico.
    *   **Diferen√ßa de Vers√£o:** Se as vers√µes do `Pillow` ou `google-generativeai` forem ligeiramente diferentes entre os dois notebooks, pode haver uma incompatibilidade sutil. Ex: Uma vers√£o antiga do Pillow pode n√£o suportar um formato espec√≠fico que o Gemini est√° retornando, ou uma vers√£o diferente do `google-generativeai` pode lidar com a resposta da API de forma distinta.

### Passos de Diagn√≥stico e Solu√ß√£o Detalhados:

**1. Re-execute o `test_image_generation.py` no notebook problem√°tico.**
   Este √© o teste mais importante para isolar o problema. Ele usa a mesma l√≥gica de chamada de API e de abertura de imagem.
   *   Execute `python test_image_generation.py` no terminal do notebook que est√° com problema.
   *   **Observe a sa√≠da:**
        *   Se ele **tamb√©m falhar com o mesmo erro** (`cannot identify image file`), ent√£o o problema √© fundamental na forma como os dados chegam da API ou como o `Pillow` os interpreta *naquele ambiente*.
        *   Se ele **funcionar e gerar a imagem**, ent√£o o problema √© mais espec√≠fico no fluxo do `chat_streamlit.py` (embora isso seja menos prov√°vel com base no erro que voc√™ relatou).

**2. Adicione Logs Detalhados na Fun√ß√£o `generate_image` (em `chat_streamlit.py`)**

   Isso nos permitir√° ver exatamente o que o Gemini est√° retornando antes que o `Pillow` tente abrir a imagem.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Verifique se 'io' est√° importado no topo do arquivo

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI n√£o encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug no Streamlit
           print(f"\nDEBUG: Chamando API do Gemini para prompt: '{prompt}'") # Debug no terminal

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG (principalmente no terminal) ---
           print(f"\nDEBUG: Resposta completa do Gemini recebida. Tipo: {type(response)}")
           
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Raz√£o de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Gera√ß√£o de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # N√£o h√° imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA n√£o retornou candidatos de gera√ß√£o. Tente um prompt diferente.")
               return None

           found_image = False
           # Assumimos que sempre pegamos o primeiro candidato (response.candidates[0])
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo do objeto part: {type(part)}")
               
               if hasattr(part, 'text') and part.text is not None:
                   print(f"DEBUG: Conte√∫do de texto recebido: '{part.text}'")
                   # Se houver texto e voc√™ esperar apenas imagem, pode ser um problema
               
               if hasattr(part, 'inline_data') and part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Para ter certeza, grave os bytes em um arquivo tempor√°rio para inspe√ß√£o
                       # with open("debug_image_data.bin", "wb") as f:
                       #     f.write(part.inline_data.data)
                       # print("DEBUG: Dados brutos da imagem salvos em 'debug_image_data.bin'.")

                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image # Retorna a imagem e encerra a fun√ß√£o
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}. Verifique o console para mais detalhes.")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data √© None ou vazio, apesar de inline_data existir.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline relevantes.")
           
           if not found_image:
               st.warning("O Gemini n√£o retornou uma imagem v√°lida para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este √© o erro que voc√™ est√° vendo
           print(f"DEBUG: Exce√ß√£o principal capturada: {e}") # Log mais detalhado no terminal
           return None
   ```
   Execute a aplica√ß√£o Streamlit com estas altera√ß√µes (`streamlit run chat_streamlit.py`) no notebook problem√°tico. Os `print`s extras aparecer√£o no terminal onde voc√™ iniciou o Streamlit e nos dar√£o informa√ß√µes cruciais sobre o que o Gemini est√° realmente enviando.

**3. Compare as Vers√µes das Depend√™ncias (`pip freeze`)**

   Mesmo em Windows, a vers√£o das bibliotecas pode ser um problema.
   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que N√ÉO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos `requirements_working.txt` e `requirements_broken.txt` meticulosamente.** Preste aten√ß√£o especial √†s vers√µes de:
        *   `Pillow` (P-I-L-L-O-W)
        *   `google-generativeai`
        *   `streamlit`
   *   **A√ß√£o:** Se houver diferen√ßas, tente instalar as vers√µes do notebook que funciona no notebook problem√°tico. Por exemplo, se `Pillow==9.5.0` funciona e `Pillow==10.0.0` falha, tente:
        ```bash
        pip install Pillow==[VERSAO_DO_FUNCIONANDO] google-generativeai==[VERSAO_DO_FUNCIONANDO]
        ```
        Sempre ative seu ambiente virtual antes de instalar!

**4. Verifique a Conectividade de Rede e Firewall**

   *   **Teste de Ping:** Abra o Prompt de Comando (CMD) ou PowerShell no notebook problem√°tico e tente:
        ```bash
        ping generativelanguage.googleapis.com
        ```
        Voc√™ deve ver pacotes sendo enviados e recebidos. Se houver perda de pacotes ou o destino n√£o for alcan√ßado, pode ser um problema de rede.
   *   **Firewall/Antiv√≠rus:** Temporariamente (com cautela, se for seguro) desative o firewall do Windows ou o antiv√≠rus para ver se eles est√£o bloqueando a comunica√ß√£o com a API do Gemini. Se funcionar, voc√™ precisar√° adicionar uma exce√ß√£o.
   *   **Proxy:** Se voc√™ estiver em uma rede corporativa, verifique se h√° um proxy que precisa ser configurado para chamadas HTTP/HTTPS.

**5. Reinstale as Bibliotecas Chave (For√ßado)**

   Se a compara√ß√£o do `pip freeze` n√£o revelar grandes diferen√ßas ou se voc√™ quiser ter certeza de uma instala√ß√£o limpa:
   *   Ative seu ambiente virtual.
   *   Execute:
        ```bash
        pip install --upgrade --force-reinstall Pillow google-generativeai
        ```
   *   Reinicie a aplica√ß√£o Streamlit.

**6. Consist√™ncia do Ambiente Virtual**

   Certifique-se de que em ambos os notebooks voc√™ est√° usando ambientes virtuais e que os ativou corretamente antes de instalar as depend√™ncias e antes de executar o `streamlit run`. Usar o ambiente global do Python pode levar a conflitos de depend√™ncias.

---

A mensagem de erro √© bem clara sobre onde o problema est√° (`Image.open(io.BytesIO(part.inline_data.data))`). A chave √© descobrir *por que* os `bytes` em `part.inline_data.data` n√£o est√£o representando uma imagem v√°lida no notebook problem√°tico. Os passos de debug com `print`s na fun√ß√£o `generate_image` e o teste com `test_image_generation.py` s√£o os mais promissores para desvendar isso.
2025-06-28 14:07:31,485 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:07:31,486 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:07:31,488 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:07:31,489 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:07:31,490 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:07:31,491 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:07:31,493 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:07:31,494 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:07:31,496 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:07:31,499 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:07:31,500 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:07:31,502 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:07:31,503 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:07:31,504 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:07:31,509 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:09:14,013 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:09:14,018 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:09:14,020 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:09:14,021 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:09:14,028 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:09:14,029 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:09:14,030 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:09:14,032 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:09:14,033 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:09:14,040 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:09:14,048 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:09:14,049 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:09:14,055 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:09:14,061 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:09:14,062 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:09:14,413 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:09:14,414 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:09:14,415 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:09:14,416 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:09:14,417 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:09:14,418 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:09:14,420 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:09:14,422 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:09:14,423 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:09:14,425 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:09:14,429 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:09:14,430 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:09:14,432 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:09:14,433 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:09:14,434 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:09:15,208 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:09:19,714 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 14:09:20,818 [INFO] AFC remote call 1 is done.
2025-06-28 14:09:57,569 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:09:57,571 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:09:57,572 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:09:57,573 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:09:57,574 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:09:57,575 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:09:57,578 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:09:57,579 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:09:57,580 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:09:57,581 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:09:57,587 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:09:57,589 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:09:57,590 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:09:57,591 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:09:57,593 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:09:57,850 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:09:57,851 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:09:57,852 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:09:57,853 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:09:57,854 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:09:57,856 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:09:57,857 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:09:57,859 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:09:57,860 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:09:57,861 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:09:57,862 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:09:57,863 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:09:57,865 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:09:57,866 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:09:57,867 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:09:57,870 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque estou recebendo esse erro nessa aplica√ß√£o sendo que no outro notebook a mesma aplica√ß√£o roda perfeitamente ?
Assistant: √â frustrante quando o mesmo c√≥digo se comporta de forma diferente em m√°quinas distintas! Isso quase sempre aponta para **diferen√ßas no ambiente de execu√ß√£o**, e n√£o no c√≥digo em si.

Analisando seu c√≥digo, o principal suspeito para um erro que ocorre em um notebook e n√£o em outro, especialmente se um for Windows e o outro Linux/macOS, √© o uso de `PIL.ImageGrab`.

---

**O SUSPEITO PRINCIPAL: `PIL.ImageGrab` (√Årea de Transfer√™ncia)**

No seu arquivo `chat_streamlit.py`, voc√™ tem a fun√ß√£o `check_clipboard()` que usa `PIL.ImageGrab`:

```python
# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab # <--- AQUI EST√Å O PROV√ÅVEL PROBLEMA

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        # ... (restante da fun√ß√£o)
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}") # <--- ESSA √â A MENSAGEM DE ERRO QUE VOC√ä DEVE ESTAR VENDO
        return None, None
```

A biblioteca `PIL.ImageGrab` (parte do Pillow) √© altamente dependente do sistema operacional:

1.  **Windows:** Geralmente funciona sem problemas, pois o Windows possui APIs diretas para acesso √† √°rea de transfer√™ncia de imagens.
2.  **Linux:** Requer utilit√°rios de linha de comando como `scrot` ou `gnome-screenshot` instalados e configurados corretamente, e a biblioteca `Pillow` precisa ser compilada com suporte a essas ferramentas. Muitas distribui√ß√µes Linux n√£o v√™m com isso pronto para o `ImageGrab`.
3.  **macOS:** Tamb√©m tem suas pr√≥prias depend√™ncias, geralmente exigindo bibliotecas como `pyobjc` para interagir com o ambiente Cocoa.

**Se o seu "outro notebook" roda Windows e o notebook com problema roda Linux ou macOS, este √© 99% a causa.** A mensagem de erro `"Erro ao acessar a √°rea de transfer√™ncia: {e}"` √© exatamente o que voc√™ veria na barra lateral do Streamlit se essa parte falhasse.

---

**Passos para Diagnosticar e Resolver:**

1.  **Obtenha a Mensagem de Erro Exata:**
    *   No notebook onde a aplica√ß√£o n√£o roda, verifique a barra lateral do Streamlit. Voc√™ ver√° a mensagem exata do erro que est√° ocorrendo, come√ßando com `"Erro ao acessar a √°rea de transfer√™ncia: ..."`. O que vem depois do `:` √© crucial.
    *   Se n√£o for `PIL.ImageGrab`, o erro estar√° em outro lugar.

2.  **Compare os Ambientes:**
    *   **Sistema Operacional:** Qual SO est√° rodando em cada notebook? (Ex: Windows 10 vs. Ubuntu 22.04 vs. macOS Ventura)
    *   **Vers√£o do Python:** `python --version` em ambos.
    *   **Depend√™ncias Instaladas:**
        *   No notebook que funciona, execute: `pip freeze > requirements_working.txt`
        *   No notebook que n√£o funciona, execute: `pip freeze > requirements_broken.txt`
        *   Compare os dois arquivos (`diff requirements_working.txt requirements_broken.txt`). Preste aten√ß√£o especial √† vers√£o do `Pillow` e a presen√ßa de pacotes como `pywin32` (Windows) ou qualquer coisa relacionada a screenshots em Linux/macOS.

3.  **Solu√ß√µes para o `PIL.ImageGrab`:**

    *   **Para Linux (Ubuntu/Debian):**
        *   Instale `scrot` (ou `gnome-screenshot`): `sudo apt-get update && sudo apt-get install scrot` (ou `sudo apt-get install gnome-screenshot`).
        *   Certifique-se de que o `Pillow` foi instalado *ap√≥s* a instala√ß√£o dessas ferramentas ou reinstale-o para que ele possa detectar as depend√™ncias.
        *   Pode ser necess√°rio tamb√©m `sudo apt-get install python3-tk python3-pil.imagetk` em algumas distribui√ß√µes.

    *   **Para macOS:**
        *   Pode ser necess√°rio instalar `pyobjc`: `pip install pyobjc`.

    *   **Solu√ß√£o Alternativa (Desativar ou Condicional):**
        Se voc√™ n√£o precisa desesperadamente da funcionalidade de colar imagem da √°rea de transfer√™ncia e quer que a aplica√ß√£o funcione em qualquer SO sem depend√™ncias externas complexas:
        Voc√™ j√° tem um `try-except` que lida com o erro. A aplica√ß√£o deve continuar funcionando mesmo se a funcionalidade de colar n√£o funcionar. No entanto, se o erro estiver parando a execu√ß√£o principal, isso √© um problema.

        **Para evitar o erro na barra lateral em ambientes n√£o-Windows:**
        Voc√™ pode adicionar uma verifica√ß√£o de sistema operacional:
        ```python
        import platform
        # ...

        def check_clipboard():
            # S√≥ tenta importar ImageGrab se for Windows
            if platform.system() == "Windows":
                try:
                    from PIL import ImageGrab
                    img = ImageGrab.grabclipboard()
                    # ... (restante do c√≥digo se img n√£o for None)
                    if img is not None and isinstance(img, Image.Image):
                        # ... seu c√≥digo para processar a imagem da √°rea de transfer√™ncia
                        # ...
                        return ClipboardFile(img_byte_arr), img
                    return None, None
                except Exception as e:
                    # A mensagem j√° √© exibida no Streamlit, ent√£o n√£o precisa de outro print aqui
                    st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
                    return None, None
            else:
                # Se n√£o for Windows, n√£o tenta acessar a √°rea de transfer√™ncia
                # E n√£o mostra erro na barra lateral
                # st.sidebar.info("Colar da √°rea de transfer√™ncia dispon√≠vel apenas no Windows.")
                return None, None
        ```
        Isso impediria que o erro espec√≠fico de `ImageGrab` fosse tentado em sistemas n√£o-Windows.

4.  **Verifique a Chave da API Gemini:**
    *   Certifique-se de que o arquivo `.env` existe no diret√≥rio raiz da aplica√ß√£o no notebook problem√°tico.
    *   Certifique-se de que `API_KEY_GEMINI` est√° definida corretamente dentro dele (Ex: `API_KEY_GEMINI="SUA_CHAVE_AQUI"`).
    *   Um erro relacionado √† API Key geralmente resultaria em `ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")` ou erros da pr√≥pria API (4xx, 5xx).

5.  **Verifique as Vers√µes das Depend√™ncias:**
    *   √â altamente recomend√°vel ter um `requirements.txt` e instal√°-lo em ambos os ambientes com `pip install -r requirements.txt`.
    *   Pode haver incompatibilidades sutis entre vers√µes de `streamlit`, `google-generativeai`, `Pillow`, etc.

6.  **Permiss√µes de Diret√≥rio:**
    *   Verifique se a aplica√ß√£o tem permiss√£o para criar e escrever nos diret√≥rios `assets/`, `logs/` e `processed_images/` no notebook problem√°tico. Isso √© mais comum em Linux/macOS com permiss√µes restritas.

7.  **Conectividade de Rede/Firewall:**
    *   O notebook problem√°tico consegue acessar a internet? Existem restri√ß√µes de firewall que podem estar bloqueando o acesso aos endpoints da API do Gemini?

---

**Resumo da A√ß√£o Mais Prov√°vel:**

O erro `Erro ao acessar a √°rea de transfer√™ncia: {e}` na barra lateral do Streamlit √© quase uma certeza de que o problema est√° na tentativa de usar `PIL.ImageGrab` em um sistema operacional (provavelmente Linux/macOS) que n√£o tem as depend√™ncias necess√°rias para isso.

Por favor, forne√ßa a mensagem de erro exata da barra lateral e o sistema operacional de ambos os notebooks para uma an√°lise mais precisa!
User: ois dois ambientes s√£o windows. 
Assistant: Certo, isso muda o cen√°rio! Se ambos os ambientes s√£o Windows, ent√£o o problema n√£o √© a incompatibilidade do `PIL.ImageGrab` com Linux/macOS. No entanto, o `PIL.ImageGrab` continua sendo o principal suspeito, mas por raz√µes diferentes.

Quando `PIL.ImageGrab` falha no Windows, geralmente aponta para um problema com a biblioteca `pywin32`, que o Pillow usa para interagir com as APIs do sistema operacional Windows.

Vamos desvendar isso.

---

**Causas Mais Comuns do `PIL.ImageGrab` Falhar no Windows:**

1.  **`pywin32` Ausente ou Corrompido:** A biblioteca `pywin32` √© essencial para que o Pillow (que inclui `PIL.ImageGrab`) acesse a √°rea de transfer√™ncia do Windows de forma nativa. Se ela n√£o estiver instalada ou estiver corrompida, a fun√ß√£o falhar√°.
2.  **Ambientes Python Diferentes:** Mesmo que ambos sejam Windows, voc√™ pode ter ambientes Python (globais ou virtuais) ligeiramente diferentes em cada m√°quina, onde uma tem `pywin32` e a outra n√£o, ou tem uma vers√£o incompat√≠vel.
3.  **Conflitos de Vers√£o:** Raramente, pode haver um conflito entre a vers√£o do `Pillow` e a vers√£o do `pywin32`, ou at√© mesmo com a vers√£o do Python.
4.  **Conte√∫do da √Årea de Transfer√™ncia:** Embora menos prov√°vel dado seu relato, o `ImageGrab.grabclipboard()` retornar√° `None` se o conte√∫do da √°rea de transfer√™ncia n√£o for uma imagem (ex: texto, arquivos). No entanto, o erro que voc√™ v√™ sugere que o *acesso* √† √°rea de transfer√™ncia est√° falhando, n√£o apenas que ela est√° vazia.

---

**Passos Detalhados de Diagn√≥stico e Solu√ß√£o:**

**1. Obtenha a Mensagem de Erro Exata:**
   Este √© o passo mais crucial. Quando voc√™ executa a aplica√ß√£o no notebook que est√° dando problema, qual √© a *mensagem de erro completa* que aparece na barra lateral do Streamlit, ap√≥s "Erro ao acessar a √°rea de transfer√™ncia: "?
   Ex: `"Erro ao acessar a √°rea de transfer√™ncia: [WinError 5] Acesso negado."` ou `"Erro ao acessar a √°rea de transfer√™ncia: No module named 'win32clipboard'"` etc.
   Esta mensagem detalhada nos dir√° exatamente o que est√° acontecendo.

**2. Compare as Depend√™ncias `pip`:**

   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que N√ÉO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos:** Abra `requirements_working.txt` e `requirements_broken.txt` e procure por:
        *   `Pillow` (P-I-L-L-O-W): Verifique as vers√µes.
        *   `pywin32`: **Este √© o mais importante.** Ele deve estar presente no `requirements_working.txt`. Se n√£o estiver no `requirements_broken.txt` ou se a vers√£o for diferente, encontramos o prov√°vel culpado.
        *   `streamlit`: Verifique as vers√µes.

**3. Instale/Reinstale `pywin32` (no notebook problem√°tico):**

   Se `pywin32` estiver faltando ou com uma vers√£o diferente, tente o seguinte:

   *   **Verifique o ambiente virtual:** Se voc√™ estiver usando um ambiente virtual (altamente recomendado), ative-o primeiro.
        ```bash
        # Exemplo para venv:
        cd C:\caminho\para\seu\projeto
        .\.venv\Scripts\activate  # ou .\venv\Scripts\activate
        ```
   *   **Instale `pywin32`:**
        ```bash
        pip install pywin32
        ```
   *   **Ou, se j√° estiver instalado, force a reinstala√ß√£o:**
        ```bash
        pip install --upgrade --force-reinstall pywin32
        ```
   *   **Reinicie a aplica√ß√£o Streamlit.**

**4. Teste a Funcionalidade `ImageGrab` Isoladamente:**

   Para confirmar que o problema √© realmente com `ImageGrab` e n√£o algo mais amplo com o Streamlit ou seu c√≥digo:

   *   Crie um pequeno script Python (`test_clipboard.py`) no notebook problem√°tico:
        ```python
        import sys
        from PIL import ImageGrab, Image
        import io
        import time

        try:
            # Tenta pegar imagem da √°rea de transfer√™ncia
            img = ImageGrab.grabclipboard()

            if img is not None and isinstance(img, Image.Image):
                print("Imagem encontrada na √°rea de transfer√™ncia!")
                # Opcional: Salvar a imagem para verificar
                # img.save("clipboard_test.png")
                # print("Imagem salva como clipboard_test.png")
                
                # Tenta converter para bytes (simular o que voc√™ faz no Streamlit)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format='PNG')
                img_byte_arr.seek(0)
                print(f"Tamanho dos bytes: {len(img_byte_arr.getvalue())} bytes")
                
            elif img is None:
                print("Nenhuma imagem na √°rea de transfer√™ncia ou o conte√∫do n√£o √© uma imagem.")
            else:
                print(f"Tipo de conte√∫do inesperado na √°rea de transfer√™ncia: {type(img)}")

        except ImportError as e:
            print(f"Erro: Depend√™ncia faltando para PIL.ImageGrab: {e}")
            print("Tente 'pip install pywin32' e certifique-se de que est√° no ambiente correto.")
        except Exception as e:
            print(f"Erro inesperado ao acessar a √°rea de transfer√™ncia: {e}")
            print("Verifique permiss√µes ou poss√≠veis conflitos.")

        print("\nTeste conclu√≠do.")
        ```
   *   **Copie uma imagem** (n√£o texto) para a √°rea de transfer√™ncia no Windows.
   *   Execute este script no terminal (ativando seu ambiente virtual, se aplic√°vel): `python test_clipboard.py`
   *   Observe a sa√≠da. Isso confirmar√° se o `ImageGrab` est√° funcionando fora do Streamlit e qual erro exato ele gera.

**5. Verifique a Vers√£o do Python e Arquitetura:**

   Embora menos comum hoje em dia, um Python de 32 bits pode ter problemas com algumas bibliotecas de 64 bits.

   *   Execute `python -c "import platform; print(platform.architecture())"` em ambos os notebooks. Verifique se ambos s√£o `('64bit', 'WindowsPE')`.

**6. Outras Verifica√ß√µes (Menos Prov√°veis):**

*   **Antiv√≠rus/Firewall:** Muito raramente, um software de seguran√ßa pode interferir no acesso a certas APIs do sistema. Tente desativ√°-lo temporariamente (se for seguro fazer isso) para testar.
*   **Permiss√µes de Usu√°rio:** Certifique-se de que o usu√°rio que executa a aplica√ß√£o Streamlit tem as permiss√µes necess√°rias para interagir com o sistema.

---

**Em Resumo:**

A principal a√ß√£o √© obter a **mensagem de erro exata** da barra lateral do Streamlit e, em seguida, **comparar as depend√™ncias `pip freeze`** para identificar se `pywin32` est√° faltando ou tem uma vers√£o diferente. O teste isolado com `test_clipboard.py` tamb√©m √© muito √∫til para isolar o problema.

Assim que voc√™ tiver a mensagem de erro exata, poderemos refinar ainda mais o diagn√≥stico!
User: Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E13620360>
Assistant: Ok, essa nova informa√ß√£o muda completamente o diagn√≥stico! O erro `Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x...>` **n√£o tem a ver com a √°rea de transfer√™ncia (`PIL.ImageGrab`)**.

Este erro significa que a biblioteca `Pillow` (PIL) tentou abrir algo que n√£o reconheceu como uma imagem v√°lida. A mensagem `_io.BytesIO object` indica que ela recebeu um fluxo de bytes, mas esses bytes n√£o formam uma imagem (JPEG, PNG, etc.) que ela possa decodificar.

Vamos focar na fun√ß√£o `generate_image` no `chat_streamlit.py`:

```python
# chat_streamlit.py

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # ... (verifica√ß√£o da API_KEY) ...

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text) # Se for s√≥ texto, imprime e continua o loop ou retorna None
            elif part.inline_data is not None:
                # <-- AQUI √â O PONTO DO ERRO
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image # Retorna a imagem e sai da fun√ß√£o

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # <-- √â aqui que o erro √© capturado e exibido
        return None
```

A linha `image = Image.open(io.BytesIO(part.inline_data.data))` √© a que est√° falhando. Isso significa que, por algum motivo, o `part.inline_data.data` que o Gemini est√° retornando **n√£o √© um conjunto de bytes de uma imagem v√°lida** no notebook problem√°tico, ou est√° vazio.

---

**Poss√≠veis Causas e Como Depurar:**

1.  **O Gemini n√£o est√° gerando uma imagem para aquele prompt/requisi√ß√£o espec√≠fica:**
    *   √Äs vezes, a API de gera√ß√£o de imagem do Gemini pode n√£o conseguir gerar uma imagem para um prompt, ou pode bloquear a gera√ß√£o por motivos de seguran√ßa/conte√∫do. Nesses casos, `part.inline_data` pode ser `None` ou `part.inline_data.data` pode estar vazio ou conter dados de erro/texto em vez de dados de imagem.
    *   Se o `response.prompt_feedback` tiver um `block_reason` (motivo de bloqueio), isso pode indicar o problema.

2.  **Problema de conectividade/API key:**
    *   Embora voc√™ j√° tenha verificado, √© bom garantir que a `API_KEY_GEMINI` esteja configurada *exatamente* da mesma forma em ambos os notebooks. Um problema na chave pode levar a respostas malformadas da API, mesmo que n√£o seja um erro √≥bvio de autentica√ß√£o.
    *   H√° alguma restri√ß√£o de rede/firewall no notebook problem√°tico que possa estar afetando a comunica√ß√£o com a API do Gemini, fazendo com que as respostas venham incompletas ou corrompidas?

3.  **Vers√£o da biblioteca `google-generativeai` ou `Pillow`:**
    *   Mesmo sendo Windows, pode haver pequenas diferen√ßas nas vers√µes dessas bibliotecas que causem um comportamento diferente.

---

**Passos para Diagnosticar e Resolver:**

**1. Teste o `test_image_generation.py` no notebook problem√°tico:**

   Voc√™ tem um script de teste simples (`test_image_generation.py`) que faz uma chamada direta para a API de gera√ß√£o de imagem.
   *   Copie e execute o `test_image_generation.py` no notebook que est√° apresentando o erro.
   *   Ele deve tentar gerar "ed-image.png".
   *   **Resultado esperado:**
        *   **Se `test_image_generation.py` tamb√©m falhar com o mesmo erro (`cannot identify image file`),** ent√£o o problema √© mais fundamental na comunica√ß√£o com o Gemini ou na forma como os dados s√£o recebidos/processados pelo Pillow *naquele ambiente*.
        *   **Se `test_image_generation.py` funcionar e gerar a imagem corretamente,** ent√£o o problema est√° em algo mais espec√≠fico no fluxo do `chat_streamlit.py` ao chamar `generate_image`.

**2. Adicione Logs Detalhados na Fun√ß√£o `generate_image` (em `chat_streamlit.py`):**

   Vamos adicionar mais `print` statements (ou usar `logger.info` se preferir ver no logfile) para entender o que est√° vindo da API antes de chamar `Image.open()`.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Certifique-se que io est√° importado

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI n√£o encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug print

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG ---
           print(f"\nDEBUG: Objeto de resposta Gemini completo: {response}")
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Raz√£o de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Gera√ß√£o de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # N√£o h√° imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA n√£o retornou candidatos de gera√ß√£o. Tente um prompt diferente.")
               return None

           found_image = False
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo: {type(part)}")
               if part.text is not None:
                   print(f"DEBUG: Conte√∫do de texto recebido: '{part.text}'")
                   # Se houver texto, voc√™ pode decidir se quer us√°-lo ou continuar procurando por imagem
               elif part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Tenta abrir a imagem
                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data √© None ou vazio, apesar de inline_data n√£o ser None.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline.")
           
           if not found_image:
               st.warning("O Gemini n√£o retornou uma imagem para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este √© o erro que voc√™ est√° vendo
           return None
   ```
   Execute a aplica√ß√£o com estes prints e observe a sa√≠da no terminal onde voc√™ executa `streamlit run chat_streamlit.py`. Isso nos dar√° pistas vitais sobre o que o Gemini est√° realmente retornando.

**3. Compare as Depend√™ncias Novamente:**

   Embora menos prov√°vel, vale a pena uma revis√£o minuciosa.
   *   No notebook que FUNCIONA, execute: `pip freeze > requirements_working.txt`
   *   No notebook que N√ÉO FUNCIONA, execute: `pip freeze > requirements_broken.txt`
   *   Compare os arquivos, prestando aten√ß√£o especial a:
        *   `Pillow` (ou PIL)
        *   `google-generativeai`
        *   `streamlit`
   *   Se houver diferen√ßa, tente instalar as vers√µes do `requirements_working.txt` no notebook problem√°tico.

**4. Reinstale `Pillow`:**

   Mesmo que as vers√µes sejam as mesmas, uma instala√ß√£o corrompida pode ser o problema.
   *   Ative seu ambiente virtual (se estiver usando).
   *   Execute: `pip install --upgrade --force-reinstall Pillow`
   *   Reinicie a aplica√ß√£o Streamlit.

---

**Conclus√£o Preliminar:**

O erro `cannot identify image file` na fun√ß√£o `generate_image` sugere fortemente que os dados que o Gemini retorna para `part.inline_data.data` **n√£o s√£o dados de imagem v√°lidos** no ambiente problem√°tico. Os passos de depura√ß√£o acima, especialmente a execu√ß√£o do `test_image_generation.py` e os `print`s detalhados na fun√ß√£o `generate_image` no `chat_streamlit.py`, nos dar√£o as informa√ß√µes necess√°rias para identificar a causa raiz.
User: mas essa mesma aplica√ß√£o funciona no meu outro notebook sem erros.
Assistant: Entendido! Se o problema persiste no segundo notebook, mesmo ambos sendo Windows, e o erro √© `cannot identify image file <_io.BytesIO object at 0x...>`, isso significa que a quest√£o n√£o √© a compatibilidade do sistema operacional com `PIL.ImageGrab`, mas sim algo espec√≠fico na forma como os **dados da imagem gerada pelo Gemini** est√£o chegando ou sendo processados no notebook problem√°tico.

Este erro (`cannot identify image file`) ocorre quando a biblioteca `Pillow` (que √© usada para abrir a imagem com `Image.open`) recebe um fluxo de bytes (`_io.BytesIO`) que ela n√£o consegue reconhecer como um formato de imagem v√°lido (PNG, JPG, etc.).

Vamos focar na fun√ß√£o `generate_image` no `chat_streamlit.py` e nos dados que ela recebe da API do Gemini.

### Causas Prov√°veis do Erro `cannot identify image file` (mesmo em Windows):

1.  **Dados de Imagem Inv√°lidos/Incompletos da API do Gemini:**
    *   **API Key com Problema:** Embora voc√™ tenha verificado, √†s vezes uma API Key pode ter restri√ß√µes, ou a pr√≥pria chamada de API pode estar retornando uma resposta malformada ou um erro sutil que n√£o √© um `HTTP 200 OK` com dados de imagem.
    *   **Bloqueio de Conte√∫do:** O prompt pode estar sendo bloqueado pelos filtros de seguran√ßa do Gemini, e em vez de uma imagem, a API retorna uma resposta vazia, uma mensagem de erro em texto, ou um `prompt_feedback` indicando um bloqueio, mas o c√≥digo tenta processar `inline_data.data` como se fosse uma imagem.
    *   **Problema de Conectividade de Rede:** No notebook problem√°tico, pode haver alguma instabilidade de rede, firewall ou proxy que est√° corrompendo ou truncando os dados da imagem durante a transfer√™ncia da API do Gemini para a sua aplica√ß√£o. Os bytes chegam incompletos ou inv√°lidos, e o `Pillow` n√£o consegue decodific√°-los.

2.  **Problema na Instala√ß√£o ou Vers√£o do `Pillow` (ou `google-generativeai`):**
    *   **Instala√ß√£o Corrompida:** Apesar de ser Windows, a instala√ß√£o do Pillow pode ter algum arquivo corrompido ou faltando no ambiente Python do notebook problem√°tico.
    *   **Diferen√ßa de Vers√£o:** Se as vers√µes do `Pillow` ou `google-generativeai` forem ligeiramente diferentes entre os dois notebooks, pode haver uma incompatibilidade sutil. Ex: Uma vers√£o antiga do Pillow pode n√£o suportar um formato espec√≠fico que o Gemini est√° retornando, ou uma vers√£o diferente do `google-generativeai` pode lidar com a resposta da API de forma distinta.

### Passos de Diagn√≥stico e Solu√ß√£o Detalhados:

**1. Re-execute o `test_image_generation.py` no notebook problem√°tico.**
   Este √© o teste mais importante para isolar o problema. Ele usa a mesma l√≥gica de chamada de API e de abertura de imagem.
   *   Execute `python test_image_generation.py` no terminal do notebook que est√° com problema.
   *   **Observe a sa√≠da:**
        *   Se ele **tamb√©m falhar com o mesmo erro** (`cannot identify image file`), ent√£o o problema √© fundamental na forma como os dados chegam da API ou como o `Pillow` os interpreta *naquele ambiente*.
        *   Se ele **funcionar e gerar a imagem**, ent√£o o problema √© mais espec√≠fico no fluxo do `chat_streamlit.py` (embora isso seja menos prov√°vel com base no erro que voc√™ relatou).

**2. Adicione Logs Detalhados na Fun√ß√£o `generate_image` (em `chat_streamlit.py`)**

   Isso nos permitir√° ver exatamente o que o Gemini est√° retornando antes que o `Pillow` tente abrir a imagem.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Verifique se 'io' est√° importado no topo do arquivo

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI n√£o encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug no Streamlit
           print(f"\nDEBUG: Chamando API do Gemini para prompt: '{prompt}'") # Debug no terminal

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG (principalmente no terminal) ---
           print(f"\nDEBUG: Resposta completa do Gemini recebida. Tipo: {type(response)}")
           
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Raz√£o de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Gera√ß√£o de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # N√£o h√° imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA n√£o retornou candidatos de gera√ß√£o. Tente um prompt diferente.")
               return None

           found_image = False
           # Assumimos que sempre pegamos o primeiro candidato (response.candidates[0])
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo do objeto part: {type(part)}")
               
               if hasattr(part, 'text') and part.text is not None:
                   print(f"DEBUG: Conte√∫do de texto recebido: '{part.text}'")
                   # Se houver texto e voc√™ esperar apenas imagem, pode ser um problema
               
               if hasattr(part, 'inline_data') and part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Para ter certeza, grave os bytes em um arquivo tempor√°rio para inspe√ß√£o
                       # with open("debug_image_data.bin", "wb") as f:
                       #     f.write(part.inline_data.data)
                       # print("DEBUG: Dados brutos da imagem salvos em 'debug_image_data.bin'.")

                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image # Retorna a imagem e encerra a fun√ß√£o
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}. Verifique o console para mais detalhes.")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data √© None ou vazio, apesar de inline_data existir.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline relevantes.")
           
           if not found_image:
               st.warning("O Gemini n√£o retornou uma imagem v√°lida para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este √© o erro que voc√™ est√° vendo
           print(f"DEBUG: Exce√ß√£o principal capturada: {e}") # Log mais detalhado no terminal
           return None
   ```
   Execute a aplica√ß√£o Streamlit com estas altera√ß√µes (`streamlit run chat_streamlit.py`) no notebook problem√°tico. Os `print`s extras aparecer√£o no terminal onde voc√™ iniciou o Streamlit e nos dar√£o informa√ß√µes cruciais sobre o que o Gemini est√° realmente enviando.

**3. Compare as Vers√µes das Depend√™ncias (`pip freeze`)**

   Mesmo em Windows, a vers√£o das bibliotecas pode ser um problema.
   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que N√ÉO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos `requirements_working.txt` e `requirements_broken.txt` meticulosamente.** Preste aten√ß√£o especial √†s vers√µes de:
        *   `Pillow` (P-I-L-L-O-W)
        *   `google-generativeai`
        *   `streamlit`
   *   **A√ß√£o:** Se houver diferen√ßas, tente instalar as vers√µes do notebook que funciona no notebook problem√°tico. Por exemplo, se `Pillow==9.5.0` funciona e `Pillow==10.0.0` falha, tente:
        ```bash
        pip install Pillow==[VERSAO_DO_FUNCIONANDO] google-generativeai==[VERSAO_DO_FUNCIONANDO]
        ```
        Sempre ative seu ambiente virtual antes de instalar!

**4. Verifique a Conectividade de Rede e Firewall**

   *   **Teste de Ping:** Abra o Prompt de Comando (CMD) ou PowerShell no notebook problem√°tico e tente:
        ```bash
        ping generativelanguage.googleapis.com
        ```
        Voc√™ deve ver pacotes sendo enviados e recebidos. Se houver perda de pacotes ou o destino n√£o for alcan√ßado, pode ser um problema de rede.
   *   **Firewall/Antiv√≠rus:** Temporariamente (com cautela, se for seguro) desative o firewall do Windows ou o antiv√≠rus para ver se eles est√£o bloqueando a comunica√ß√£o com a API do Gemini. Se funcionar, voc√™ precisar√° adicionar uma exce√ß√£o.
   *   **Proxy:** Se voc√™ estiver em uma rede corporativa, verifique se h√° um proxy que precisa ser configurado para chamadas HTTP/HTTPS.

**5. Reinstale as Bibliotecas Chave (For√ßado)**

   Se a compara√ß√£o do `pip freeze` n√£o revelar grandes diferen√ßas ou se voc√™ quiser ter certeza de uma instala√ß√£o limpa:
   *   Ative seu ambiente virtual.
   *   Execute:
        ```bash
        pip install --upgrade --force-reinstall Pillow google-generativeai
        ```
   *   Reinicie a aplica√ß√£o Streamlit.

**6. Consist√™ncia do Ambiente Virtual**

   Certifique-se de que em ambos os notebooks voc√™ est√° usando ambientes virtuais e que os ativou corretamente antes de instalar as depend√™ncias e antes de executar o `streamlit run`. Usar o ambiente global do Python pode levar a conflitos de depend√™ncias.

---

A mensagem de erro √© bem clara sobre onde o problema est√° (`Image.open(io.BytesIO(part.inline_data.data))`). A chave √© descobrir *por que* os `bytes` em `part.inline_data.data` n√£o est√£o representando uma imagem v√°lida no notebook problem√°tico. Os passos de debug com `print`s na fun√ß√£o `generate_image` e o teste com `test_image_generation.py` s√£o os mais promissores para desvendar isso.
User: 2025-06-28 14:09:15,208 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:09:19,714 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent 
"HTTP/1.1 200 OK"
2025-06-28 14:09:20,818 [INFO] AFC remote call 1 is done.
I will generate an image of a tiny tabby kitten with wide eyes, standing on a miniature surfboard amidst the towering, foamy crest of a colossal ocean wave. The kitten will be mid-bite into a comically large slice of steaming lasagna, complete with visible layers of pasta, sauce, cheese, and ground meat, held precariously in its small paws.


parece que est√° gerando mas ainda ocorre o erro na lateral do streamlit 

Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E136676A0>


2025-06-28 14:10:36,822 [INFO] Resposta da IA (texto): Essa √© uma informa√ß√£o crucial!

O fato de voc√™ ver:

```
I will generate an image of a tiny tabby kitten with wide eyes, standing on a miniature surfboard amidst the towering, foamy crest of a colossal ocean wave. The kitten will be mid-bite into a comically large slice of steaming lasagna, complete with visible layers of pasta, sauce, cheese, and ground meat, held precariously in its small paws.
```
e
```
2025-06-28 14:09:19,714 [INFO] HTTP Request: POST ... HTTP/1.1 200 OK
```

Indica que:

1.  A comunica√ß√£o com a API do Gemini est√° **funcionando** (`HTTP/1.1 200 OK`).
2.  A API do Gemini est√° **respondendo** com conte√∫do.
3.  O conte√∫do inclui uma **parte de texto** (`part.text`) que descreve a imagem que ser√° gerada.

No entanto, o erro `cannot identify image file <_io.BytesIO object at 0x...>` ainda significa que, quando a biblioteca `Pillow` (PIL) tenta abrir os bytes que deveriam ser a imagem, ela **n√£o os reconhece como uma imagem v√°lida**.

### O que isso significa?

A API de gera√ß√£o de imagem do Gemini (como `gemini-2.0-flash-exp-image-generation`) geralmente retorna uma resposta multi-parte:
*   Pode ter uma parte `text` (como o que voc√™ viu, descrevendo o que ser√° gerado).
*   E *depois*, uma parte `inline_data` que cont√©m os bytes da imagem gerada.

Se voc√™ est√° vendo o texto mas o erro na imagem persiste, isso pode indicar que:

1.  **A parte da imagem est√° faltando:** O Gemini retornou o texto de "confirmar" o prompt, mas por algum motivo (talvez o prompt seja muito complexo/invi√°vel, ou algum filtro de seguran√ßa) **n√£o conseguiu gerar ou enviar a imagem em si**. Ent√£o, `part.inline_data.data` estaria vazio ou `None`, mas o seu c√≥digo *ainda tenta* process√°-lo de alguma forma que leva ao erro.
2.  **Os dados da imagem est√£o corrompidos:** A API tentou enviar a imagem, mas os bytes chegaram corrompidos (talvez por um problema de rede muito sutil *ap√≥s* o `200 OK`, durante a transfer√™ncia do corpo da resposta), e o `Pillow` n√£o consegue abri-los.
3.  **L√≥gica no loop `for part in response.candidates[0].content.parts:`:** Sua l√≥gica atual est√° apenas `print`ando o `part.text` e, em seguida, esperando encontrar o `part.inline_data`. Se o `part.inline_data` est√° vazio ou corrompido, a fun√ß√£o chega ao `Image.open` com dados ruins.

### A√ß√µes para Diagnosticar e Resolver:

Vamos refinar a fun√ß√£o `generate_image` para ser mais robusta e para que os logs capturem *exatamente* o que est√° acontecendo com os dados da imagem.

**1. Adicione logs detalhados e um salvamento de bytes para inspe√ß√£o (no `chat_streamlit.py`):**

Vamos ajustar a fun√ß√£o `generate_image` para capturar e inspecionar os bytes da imagem.

```python
# chat_streamlit.py

# ... (seus imports e in√≠cio da fun√ß√£o)

def generate_image(prompt):
    api_key = os.getenv("API_KEY_GEMINI")
    if not api_key:
        st.error("API_KEY_GEMINI n√£o encontrada no arquivo .env")
        return None

    client = genai.Client(api_key=api_key)

    try:
        st.info(f"Tentando gerar imagem com prompt: '{prompt}'")
        print(f"\n[DEBUG] Chamando API do Gemini para prompt: '{prompt}'") # Log para o terminal

        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        print(f"[DEBUG] Resposta completa do Gemini recebida. Tipo do objeto: {type(response)}")
        
        # Verificar feedback de bloqueio
        if response.prompt_feedback and response.prompt_feedback.block_reason:
            print(f"[DEBUG] Raz√£o de Bloqueio da API: {response.prompt_feedback.block_reason}")
            st.warning(f"Gera√ß√£o de imagem bloqueada pelo Gemini: {response.prompt_feedback.block_reason}")
            return None
        
        if not response.candidates:
            print("[DEBUG] Nenhuma candidate encontrada na resposta do Gemini.")
            st.error("A IA n√£o retornou candidatos de gera√ß√£o. Tente um prompt diferente.")
            return None

        # Vari√°veis para armazenar texto e dados da imagem
        text_feedback_from_gemini = ""
        image_data_bytes = None

        # Itera sobre todas as partes da resposta
        for part in response.candidates[0].content.parts:
            print(f"[DEBUG] Processando parte. Tipo do objeto 'part': {type(part)}")
            
            # Se a parte for texto
            if hasattr(part, 'text') and part.text is not None:
                text_feedback_from_gemini += part.text + "\n"
                print(f"[DEBUG] Conte√∫do de texto recebido: '{part.text}'")
            
            # Se a parte for dados inline (imagem)
            if hasattr(part, 'inline_data') and part.inline_data is not None:
                print(f"[DEBUG] Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                if part.inline_data.data:
                    image_data_bytes = part.inline_data.data
                    print(f"[DEBUG] Tamanho dos dados da imagem: {len(image_data_bytes)} bytes.")
                    
                    # --- NOVO: Salve os bytes brutos para inspe√ß√£o ---
                    # Isso ajudar√° a verificar se os bytes s√£o realmente de uma imagem.
                    # Eles ser√£o salvos no mesmo diret√≥rio do script.
                    with open("temp_gemini_image_debug.bin", "wb") as f:
                        f.write(image_data_bytes)
                    print("[DEBUG] Dados brutos da imagem salvos em 'temp_gemini_image_debug.bin' para an√°lise.")
                    
                    # Interrompa o loop assim que encontrar os dados da imagem (assumindo que s√≥ h√° uma)
                    break
                else:
                    print("[DEBUG] part.inline_data.data √© None ou vazio, apesar de inline_data existir.")
            else:
                print("[DEBUG] Parte sem texto ou dados inline relevantes.")
        
        # Tentar abrir a imagem se os bytes foram encontrados
        if image_data_bytes:
            try:
                image = Image.open(io.BytesIO(image_data_bytes))
                st.session_state.generated_image = image
                
                # Se o Gemini forneceu algum texto al√©m da imagem, adicione-o ao chat history
                # junto com a imagem, se for relevante.
                # (Ajuste onde isso √© adicionado ao chat history se o seu fluxo for diferente)
                if text_feedback_from_gemini.strip():
                     st.session_state.messages.append({"role": "assistant", "image": image, "content": text_feedback_from_gemini.strip()})
                else:
                    st.session_state.messages.append({"role": "assistant", "image": image, "content": f"Imagem gerada com o prompt: {prompt}"})
                
                st.session_state.generated_image = None # Limpa para n√£o exibir em cima
                return image # Retorna a imagem e encerra a fun√ß√£o
            except Exception as img_err:
                print(f"[DEBUG] ERRO da Pillow ao abrir imagem: {img_err}. Provavelmente dados de imagem corrompidos/inv√°lidos.")
                st.error(f"Erro interno ao processar a imagem gerada: {img_err}. A imagem recebida pode estar corrompida.")
                return None
        else:
            # Se n√£o encontrou dados de imagem
            msg = "O Gemini n√£o retornou uma imagem v√°lida para o prompt fornecido."
            if text_feedback_from_gemini.strip():
                msg += f" Feedback da IA: {text_feedback_from_gemini.strip()}"
            st.warning(msg)
            print(f"[DEBUG] {msg}")
            return None

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # Este √© o erro que voc√™ est√° vendo
        print(f"[DEBUG] Exce√ß√£o principal capturada em generate_image: {e}") # Log mais detalhado no terminal
        return None
```

**2. Execute a Aplica√ß√£o e Analise os Logs e o Arquivo `temp_gemini_image_debug.bin`:**

*   Salve as altera√ß√µes no seu `chat_streamlit.py`.
*   Execute `streamlit run chat_streamlit.py` no terminal do notebook problem√°tico.
*   Tente gerar uma imagem com o prompt que causou o problema.
*   **Observe a sa√≠da no terminal:** Preste aten√ß√£o aos `[DEBUG]` prints que voc√™ adicionou, especialmente o `Tamanho dos dados da imagem`.
    *   Se o tamanho for **zero ou muito pequeno** (ex: 0 a 100 bytes), isso indica que o Gemini n√£o enviou os dados da imagem, ou enviou um "nada" corrompido.
    *   Se o tamanho for **razo√°vel** (ex: dezenas ou centenas de KB), isso indica que os dados chegaram, mas o `Pillow` n√£o os reconhece.
*   **Inspecione o arquivo `temp_gemini_image_debug.bin`:**
    *   Ap√≥s o erro, localize este arquivo no diret√≥rio do seu script.
    *   Tente abri-lo com um visualizador de imagens (renomeie para `.png` ou `.jpg` se quiser tentar).
    *   Se ele n√£o abrir ou estiver corrompido, voc√™ confirmou que o problema est√° nos bytes que o Gemini enviou para voc√™.
    *   Se for um arquivo de imagem v√°lido (o que seria surpreendente), o problema seria ent√£o na instala√ß√£o do `Pillow` ou em alguma incompatibilidade no seu ambiente.

### O que esperar com base no seu `200 OK` e `text` part:

√â muito prov√°vel que voc√™ veja o `[DEBUG] Tamanho dos dados da imagem: X bytes`, onde X √© **zero** ou um n√∫mero muito pequeno e **n√£o h√° imagem v√°lida no `temp_gemini_image_debug.bin`**.

Isso significaria que a API do Gemini, por algum motivo interno (complexidade do prompt, filtros de seguran√ßa etc.), n√£o conseguiu materializar a imagem, apesar de ter confirmado o prompt com a parte textual. Se esse for o caso, n√£o h√° muito que voc√™ possa fazer a n√≠vel de c√≥digo para "consertar" uma imagem que n√£o foi gerada corretamente pela API. A solu√ß√£o seria:

*   **Tentar prompts mais simples ou diferentes.**
*   **Verificar a documenta√ß√£o da API Gemini** para poss√≠veis restri√ß√µes ou limita√ß√µes de conte√∫do.
*   **Reportar o problema ao Google** se voc√™ acreditar que √© um bug na API para o tipo de prompt que est√° usando.

Os logs detalhados e a inspe√ß√£o do arquivo bin√°rio ser√£o o "veredicto final" sobre a natureza do problema.
2025-06-28 14:10:37,036 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:10:37,037 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:10:37,038 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:10:37,039 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:10:37,039 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:10:37,040 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:10:37,041 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:10:37,042 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:10:37,043 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:10:37,044 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:10:37,045 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:10:37,047 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:10:37,051 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:10:37,052 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:10:37,053 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:12:52,157 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:12:52,158 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:12:52,160 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:12:52,161 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:12:52,165 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:12:52,167 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:12:52,169 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:12:52,180 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:12:52,181 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:12:52,182 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:12:52,183 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:12:52,184 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:12:52,186 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:12:52,191 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:12:52,192 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:12:52,439 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:12:52,440 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:12:52,441 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:12:52,443 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:12:52,443 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:12:52,444 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:12:52,445 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:12:52,447 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:12:52,448 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:12:52,450 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:12:52,451 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:12:52,455 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:12:52,457 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:12:52,459 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:12:52,460 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:12:52,464 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque estou recebendo esse erro nessa aplica√ß√£o sendo que no outro notebook a mesma aplica√ß√£o roda perfeitamente ?
Assistant: √â frustrante quando o mesmo c√≥digo se comporta de forma diferente em m√°quinas distintas! Isso quase sempre aponta para **diferen√ßas no ambiente de execu√ß√£o**, e n√£o no c√≥digo em si.

Analisando seu c√≥digo, o principal suspeito para um erro que ocorre em um notebook e n√£o em outro, especialmente se um for Windows e o outro Linux/macOS, √© o uso de `PIL.ImageGrab`.

---

**O SUSPEITO PRINCIPAL: `PIL.ImageGrab` (√Årea de Transfer√™ncia)**

No seu arquivo `chat_streamlit.py`, voc√™ tem a fun√ß√£o `check_clipboard()` que usa `PIL.ImageGrab`:

```python
# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab # <--- AQUI EST√Å O PROV√ÅVEL PROBLEMA

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        # ... (restante da fun√ß√£o)
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}") # <--- ESSA √â A MENSAGEM DE ERRO QUE VOC√ä DEVE ESTAR VENDO
        return None, None
```

A biblioteca `PIL.ImageGrab` (parte do Pillow) √© altamente dependente do sistema operacional:

1.  **Windows:** Geralmente funciona sem problemas, pois o Windows possui APIs diretas para acesso √† √°rea de transfer√™ncia de imagens.
2.  **Linux:** Requer utilit√°rios de linha de comando como `scrot` ou `gnome-screenshot` instalados e configurados corretamente, e a biblioteca `Pillow` precisa ser compilada com suporte a essas ferramentas. Muitas distribui√ß√µes Linux n√£o v√™m com isso pronto para o `ImageGrab`.
3.  **macOS:** Tamb√©m tem suas pr√≥prias depend√™ncias, geralmente exigindo bibliotecas como `pyobjc` para interagir com o ambiente Cocoa.

**Se o seu "outro notebook" roda Windows e o notebook com problema roda Linux ou macOS, este √© 99% a causa.** A mensagem de erro `"Erro ao acessar a √°rea de transfer√™ncia: {e}"` √© exatamente o que voc√™ veria na barra lateral do Streamlit se essa parte falhasse.

---

**Passos para Diagnosticar e Resolver:**

1.  **Obtenha a Mensagem de Erro Exata:**
    *   No notebook onde a aplica√ß√£o n√£o roda, verifique a barra lateral do Streamlit. Voc√™ ver√° a mensagem exata do erro que est√° ocorrendo, come√ßando com `"Erro ao acessar a √°rea de transfer√™ncia: ..."`. O que vem depois do `:` √© crucial.
    *   Se n√£o for `PIL.ImageGrab`, o erro estar√° em outro lugar.

2.  **Compare os Ambientes:**
    *   **Sistema Operacional:** Qual SO est√° rodando em cada notebook? (Ex: Windows 10 vs. Ubuntu 22.04 vs. macOS Ventura)
    *   **Vers√£o do Python:** `python --version` em ambos.
    *   **Depend√™ncias Instaladas:**
        *   No notebook que funciona, execute: `pip freeze > requirements_working.txt`
        *   No notebook que n√£o funciona, execute: `pip freeze > requirements_broken.txt`
        *   Compare os dois arquivos (`diff requirements_working.txt requirements_broken.txt`). Preste aten√ß√£o especial √† vers√£o do `Pillow` e a presen√ßa de pacotes como `pywin32` (Windows) ou qualquer coisa relacionada a screenshots em Linux/macOS.

3.  **Solu√ß√µes para o `PIL.ImageGrab`:**

    *   **Para Linux (Ubuntu/Debian):**
        *   Instale `scrot` (ou `gnome-screenshot`): `sudo apt-get update && sudo apt-get install scrot` (ou `sudo apt-get install gnome-screenshot`).
        *   Certifique-se de que o `Pillow` foi instalado *ap√≥s* a instala√ß√£o dessas ferramentas ou reinstale-o para que ele possa detectar as depend√™ncias.
        *   Pode ser necess√°rio tamb√©m `sudo apt-get install python3-tk python3-pil.imagetk` em algumas distribui√ß√µes.

    *   **Para macOS:**
        *   Pode ser necess√°rio instalar `pyobjc`: `pip install pyobjc`.

    *   **Solu√ß√£o Alternativa (Desativar ou Condicional):**
        Se voc√™ n√£o precisa desesperadamente da funcionalidade de colar imagem da √°rea de transfer√™ncia e quer que a aplica√ß√£o funcione em qualquer SO sem depend√™ncias externas complexas:
        Voc√™ j√° tem um `try-except` que lida com o erro. A aplica√ß√£o deve continuar funcionando mesmo se a funcionalidade de colar n√£o funcionar. No entanto, se o erro estiver parando a execu√ß√£o principal, isso √© um problema.

        **Para evitar o erro na barra lateral em ambientes n√£o-Windows:**
        Voc√™ pode adicionar uma verifica√ß√£o de sistema operacional:
        ```python
        import platform
        # ...

        def check_clipboard():
            # S√≥ tenta importar ImageGrab se for Windows
            if platform.system() == "Windows":
                try:
                    from PIL import ImageGrab
                    img = ImageGrab.grabclipboard()
                    # ... (restante do c√≥digo se img n√£o for None)
                    if img is not None and isinstance(img, Image.Image):
                        # ... seu c√≥digo para processar a imagem da √°rea de transfer√™ncia
                        # ...
                        return ClipboardFile(img_byte_arr), img
                    return None, None
                except Exception as e:
                    # A mensagem j√° √© exibida no Streamlit, ent√£o n√£o precisa de outro print aqui
                    st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
                    return None, None
            else:
                # Se n√£o for Windows, n√£o tenta acessar a √°rea de transfer√™ncia
                # E n√£o mostra erro na barra lateral
                # st.sidebar.info("Colar da √°rea de transfer√™ncia dispon√≠vel apenas no Windows.")
                return None, None
        ```
        Isso impediria que o erro espec√≠fico de `ImageGrab` fosse tentado em sistemas n√£o-Windows.

4.  **Verifique a Chave da API Gemini:**
    *   Certifique-se de que o arquivo `.env` existe no diret√≥rio raiz da aplica√ß√£o no notebook problem√°tico.
    *   Certifique-se de que `API_KEY_GEMINI` est√° definida corretamente dentro dele (Ex: `API_KEY_GEMINI="SUA_CHAVE_AQUI"`).
    *   Um erro relacionado √† API Key geralmente resultaria em `ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")` ou erros da pr√≥pria API (4xx, 5xx).

5.  **Verifique as Vers√µes das Depend√™ncias:**
    *   √â altamente recomend√°vel ter um `requirements.txt` e instal√°-lo em ambos os ambientes com `pip install -r requirements.txt`.
    *   Pode haver incompatibilidades sutis entre vers√µes de `streamlit`, `google-generativeai`, `Pillow`, etc.

6.  **Permiss√µes de Diret√≥rio:**
    *   Verifique se a aplica√ß√£o tem permiss√£o para criar e escrever nos diret√≥rios `assets/`, `logs/` e `processed_images/` no notebook problem√°tico. Isso √© mais comum em Linux/macOS com permiss√µes restritas.

7.  **Conectividade de Rede/Firewall:**
    *   O notebook problem√°tico consegue acessar a internet? Existem restri√ß√µes de firewall que podem estar bloqueando o acesso aos endpoints da API do Gemini?

---

**Resumo da A√ß√£o Mais Prov√°vel:**

O erro `Erro ao acessar a √°rea de transfer√™ncia: {e}` na barra lateral do Streamlit √© quase uma certeza de que o problema est√° na tentativa de usar `PIL.ImageGrab` em um sistema operacional (provavelmente Linux/macOS) que n√£o tem as depend√™ncias necess√°rias para isso.

Por favor, forne√ßa a mensagem de erro exata da barra lateral e o sistema operacional de ambos os notebooks para uma an√°lise mais precisa!
User: ois dois ambientes s√£o windows. 
Assistant: Certo, isso muda o cen√°rio! Se ambos os ambientes s√£o Windows, ent√£o o problema n√£o √© a incompatibilidade do `PIL.ImageGrab` com Linux/macOS. No entanto, o `PIL.ImageGrab` continua sendo o principal suspeito, mas por raz√µes diferentes.

Quando `PIL.ImageGrab` falha no Windows, geralmente aponta para um problema com a biblioteca `pywin32`, que o Pillow usa para interagir com as APIs do sistema operacional Windows.

Vamos desvendar isso.

---

**Causas Mais Comuns do `PIL.ImageGrab` Falhar no Windows:**

1.  **`pywin32` Ausente ou Corrompido:** A biblioteca `pywin32` √© essencial para que o Pillow (que inclui `PIL.ImageGrab`) acesse a √°rea de transfer√™ncia do Windows de forma nativa. Se ela n√£o estiver instalada ou estiver corrompida, a fun√ß√£o falhar√°.
2.  **Ambientes Python Diferentes:** Mesmo que ambos sejam Windows, voc√™ pode ter ambientes Python (globais ou virtuais) ligeiramente diferentes em cada m√°quina, onde uma tem `pywin32` e a outra n√£o, ou tem uma vers√£o incompat√≠vel.
3.  **Conflitos de Vers√£o:** Raramente, pode haver um conflito entre a vers√£o do `Pillow` e a vers√£o do `pywin32`, ou at√© mesmo com a vers√£o do Python.
4.  **Conte√∫do da √Årea de Transfer√™ncia:** Embora menos prov√°vel dado seu relato, o `ImageGrab.grabclipboard()` retornar√° `None` se o conte√∫do da √°rea de transfer√™ncia n√£o for uma imagem (ex: texto, arquivos). No entanto, o erro que voc√™ v√™ sugere que o *acesso* √† √°rea de transfer√™ncia est√° falhando, n√£o apenas que ela est√° vazia.

---

**Passos Detalhados de Diagn√≥stico e Solu√ß√£o:**

**1. Obtenha a Mensagem de Erro Exata:**
   Este √© o passo mais crucial. Quando voc√™ executa a aplica√ß√£o no notebook que est√° dando problema, qual √© a *mensagem de erro completa* que aparece na barra lateral do Streamlit, ap√≥s "Erro ao acessar a √°rea de transfer√™ncia: "?
   Ex: `"Erro ao acessar a √°rea de transfer√™ncia: [WinError 5] Acesso negado."` ou `"Erro ao acessar a √°rea de transfer√™ncia: No module named 'win32clipboard'"` etc.
   Esta mensagem detalhada nos dir√° exatamente o que est√° acontecendo.

**2. Compare as Depend√™ncias `pip`:**

   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que N√ÉO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos:** Abra `requirements_working.txt` e `requirements_broken.txt` e procure por:
        *   `Pillow` (P-I-L-L-O-W): Verifique as vers√µes.
        *   `pywin32`: **Este √© o mais importante.** Ele deve estar presente no `requirements_working.txt`. Se n√£o estiver no `requirements_broken.txt` ou se a vers√£o for diferente, encontramos o prov√°vel culpado.
        *   `streamlit`: Verifique as vers√µes.

**3. Instale/Reinstale `pywin32` (no notebook problem√°tico):**

   Se `pywin32` estiver faltando ou com uma vers√£o diferente, tente o seguinte:

   *   **Verifique o ambiente virtual:** Se voc√™ estiver usando um ambiente virtual (altamente recomendado), ative-o primeiro.
        ```bash
        # Exemplo para venv:
        cd C:\caminho\para\seu\projeto
        .\.venv\Scripts\activate  # ou .\venv\Scripts\activate
        ```
   *   **Instale `pywin32`:**
        ```bash
        pip install pywin32
        ```
   *   **Ou, se j√° estiver instalado, force a reinstala√ß√£o:**
        ```bash
        pip install --upgrade --force-reinstall pywin32
        ```
   *   **Reinicie a aplica√ß√£o Streamlit.**

**4. Teste a Funcionalidade `ImageGrab` Isoladamente:**

   Para confirmar que o problema √© realmente com `ImageGrab` e n√£o algo mais amplo com o Streamlit ou seu c√≥digo:

   *   Crie um pequeno script Python (`test_clipboard.py`) no notebook problem√°tico:
        ```python
        import sys
        from PIL import ImageGrab, Image
        import io
        import time

        try:
            # Tenta pegar imagem da √°rea de transfer√™ncia
            img = ImageGrab.grabclipboard()

            if img is not None and isinstance(img, Image.Image):
                print("Imagem encontrada na √°rea de transfer√™ncia!")
                # Opcional: Salvar a imagem para verificar
                # img.save("clipboard_test.png")
                # print("Imagem salva como clipboard_test.png")
                
                # Tenta converter para bytes (simular o que voc√™ faz no Streamlit)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format='PNG')
                img_byte_arr.seek(0)
                print(f"Tamanho dos bytes: {len(img_byte_arr.getvalue())} bytes")
                
            elif img is None:
                print("Nenhuma imagem na √°rea de transfer√™ncia ou o conte√∫do n√£o √© uma imagem.")
            else:
                print(f"Tipo de conte√∫do inesperado na √°rea de transfer√™ncia: {type(img)}")

        except ImportError as e:
            print(f"Erro: Depend√™ncia faltando para PIL.ImageGrab: {e}")
            print("Tente 'pip install pywin32' e certifique-se de que est√° no ambiente correto.")
        except Exception as e:
            print(f"Erro inesperado ao acessar a √°rea de transfer√™ncia: {e}")
            print("Verifique permiss√µes ou poss√≠veis conflitos.")

        print("\nTeste conclu√≠do.")
        ```
   *   **Copie uma imagem** (n√£o texto) para a √°rea de transfer√™ncia no Windows.
   *   Execute este script no terminal (ativando seu ambiente virtual, se aplic√°vel): `python test_clipboard.py`
   *   Observe a sa√≠da. Isso confirmar√° se o `ImageGrab` est√° funcionando fora do Streamlit e qual erro exato ele gera.

**5. Verifique a Vers√£o do Python e Arquitetura:**

   Embora menos comum hoje em dia, um Python de 32 bits pode ter problemas com algumas bibliotecas de 64 bits.

   *   Execute `python -c "import platform; print(platform.architecture())"` em ambos os notebooks. Verifique se ambos s√£o `('64bit', 'WindowsPE')`.

**6. Outras Verifica√ß√µes (Menos Prov√°veis):**

*   **Antiv√≠rus/Firewall:** Muito raramente, um software de seguran√ßa pode interferir no acesso a certas APIs do sistema. Tente desativ√°-lo temporariamente (se for seguro fazer isso) para testar.
*   **Permiss√µes de Usu√°rio:** Certifique-se de que o usu√°rio que executa a aplica√ß√£o Streamlit tem as permiss√µes necess√°rias para interagir com o sistema.

---

**Em Resumo:**

A principal a√ß√£o √© obter a **mensagem de erro exata** da barra lateral do Streamlit e, em seguida, **comparar as depend√™ncias `pip freeze`** para identificar se `pywin32` est√° faltando ou tem uma vers√£o diferente. O teste isolado com `test_clipboard.py` tamb√©m √© muito √∫til para isolar o problema.

Assim que voc√™ tiver a mensagem de erro exata, poderemos refinar ainda mais o diagn√≥stico!
User: Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E13620360>
Assistant: Ok, essa nova informa√ß√£o muda completamente o diagn√≥stico! O erro `Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x...>` **n√£o tem a ver com a √°rea de transfer√™ncia (`PIL.ImageGrab`)**.

Este erro significa que a biblioteca `Pillow` (PIL) tentou abrir algo que n√£o reconheceu como uma imagem v√°lida. A mensagem `_io.BytesIO object` indica que ela recebeu um fluxo de bytes, mas esses bytes n√£o formam uma imagem (JPEG, PNG, etc.) que ela possa decodificar.

Vamos focar na fun√ß√£o `generate_image` no `chat_streamlit.py`:

```python
# chat_streamlit.py

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # ... (verifica√ß√£o da API_KEY) ...

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text) # Se for s√≥ texto, imprime e continua o loop ou retorna None
            elif part.inline_data is not None:
                # <-- AQUI √â O PONTO DO ERRO
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image # Retorna a imagem e sai da fun√ß√£o

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # <-- √â aqui que o erro √© capturado e exibido
        return None
```

A linha `image = Image.open(io.BytesIO(part.inline_data.data))` √© a que est√° falhando. Isso significa que, por algum motivo, o `part.inline_data.data` que o Gemini est√° retornando **n√£o √© um conjunto de bytes de uma imagem v√°lida** no notebook problem√°tico, ou est√° vazio.

---

**Poss√≠veis Causas e Como Depurar:**

1.  **O Gemini n√£o est√° gerando uma imagem para aquele prompt/requisi√ß√£o espec√≠fica:**
    *   √Äs vezes, a API de gera√ß√£o de imagem do Gemini pode n√£o conseguir gerar uma imagem para um prompt, ou pode bloquear a gera√ß√£o por motivos de seguran√ßa/conte√∫do. Nesses casos, `part.inline_data` pode ser `None` ou `part.inline_data.data` pode estar vazio ou conter dados de erro/texto em vez de dados de imagem.
    *   Se o `response.prompt_feedback` tiver um `block_reason` (motivo de bloqueio), isso pode indicar o problema.

2.  **Problema de conectividade/API key:**
    *   Embora voc√™ j√° tenha verificado, √© bom garantir que a `API_KEY_GEMINI` esteja configurada *exatamente* da mesma forma em ambos os notebooks. Um problema na chave pode levar a respostas malformadas da API, mesmo que n√£o seja um erro √≥bvio de autentica√ß√£o.
    *   H√° alguma restri√ß√£o de rede/firewall no notebook problem√°tico que possa estar afetando a comunica√ß√£o com a API do Gemini, fazendo com que as respostas venham incompletas ou corrompidas?

3.  **Vers√£o da biblioteca `google-generativeai` ou `Pillow`:**
    *   Mesmo sendo Windows, pode haver pequenas diferen√ßas nas vers√µes dessas bibliotecas que causem um comportamento diferente.

---

**Passos para Diagnosticar e Resolver:**

**1. Teste o `test_image_generation.py` no notebook problem√°tico:**

   Voc√™ tem um script de teste simples (`test_image_generation.py`) que faz uma chamada direta para a API de gera√ß√£o de imagem.
   *   Copie e execute o `test_image_generation.py` no notebook que est√° apresentando o erro.
   *   Ele deve tentar gerar "ed-image.png".
   *   **Resultado esperado:**
        *   **Se `test_image_generation.py` tamb√©m falhar com o mesmo erro (`cannot identify image file`),** ent√£o o problema √© mais fundamental na comunica√ß√£o com o Gemini ou na forma como os dados s√£o recebidos/processados pelo Pillow *naquele ambiente*.
        *   **Se `test_image_generation.py` funcionar e gerar a imagem corretamente,** ent√£o o problema est√° em algo mais espec√≠fico no fluxo do `chat_streamlit.py` ao chamar `generate_image`.

**2. Adicione Logs Detalhados na Fun√ß√£o `generate_image` (em `chat_streamlit.py`):**

   Vamos adicionar mais `print` statements (ou usar `logger.info` se preferir ver no logfile) para entender o que est√° vindo da API antes de chamar `Image.open()`.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Certifique-se que io est√° importado

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI n√£o encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug print

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG ---
           print(f"\nDEBUG: Objeto de resposta Gemini completo: {response}")
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Raz√£o de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Gera√ß√£o de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # N√£o h√° imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA n√£o retornou candidatos de gera√ß√£o. Tente um prompt diferente.")
               return None

           found_image = False
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo: {type(part)}")
               if part.text is not None:
                   print(f"DEBUG: Conte√∫do de texto recebido: '{part.text}'")
                   # Se houver texto, voc√™ pode decidir se quer us√°-lo ou continuar procurando por imagem
               elif part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Tenta abrir a imagem
                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data √© None ou vazio, apesar de inline_data n√£o ser None.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline.")
           
           if not found_image:
               st.warning("O Gemini n√£o retornou uma imagem para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este √© o erro que voc√™ est√° vendo
           return None
   ```
   Execute a aplica√ß√£o com estes prints e observe a sa√≠da no terminal onde voc√™ executa `streamlit run chat_streamlit.py`. Isso nos dar√° pistas vitais sobre o que o Gemini est√° realmente retornando.

**3. Compare as Depend√™ncias Novamente:**

   Embora menos prov√°vel, vale a pena uma revis√£o minuciosa.
   *   No notebook que FUNCIONA, execute: `pip freeze > requirements_working.txt`
   *   No notebook que N√ÉO FUNCIONA, execute: `pip freeze > requirements_broken.txt`
   *   Compare os arquivos, prestando aten√ß√£o especial a:
        *   `Pillow` (ou PIL)
        *   `google-generativeai`
        *   `streamlit`
   *   Se houver diferen√ßa, tente instalar as vers√µes do `requirements_working.txt` no notebook problem√°tico.

**4. Reinstale `Pillow`:**

   Mesmo que as vers√µes sejam as mesmas, uma instala√ß√£o corrompida pode ser o problema.
   *   Ative seu ambiente virtual (se estiver usando).
   *   Execute: `pip install --upgrade --force-reinstall Pillow`
   *   Reinicie a aplica√ß√£o Streamlit.

---

**Conclus√£o Preliminar:**

O erro `cannot identify image file` na fun√ß√£o `generate_image` sugere fortemente que os dados que o Gemini retorna para `part.inline_data.data` **n√£o s√£o dados de imagem v√°lidos** no ambiente problem√°tico. Os passos de depura√ß√£o acima, especialmente a execu√ß√£o do `test_image_generation.py` e os `print`s detalhados na fun√ß√£o `generate_image` no `chat_streamlit.py`, nos dar√£o as informa√ß√µes necess√°rias para identificar a causa raiz.
User: mas essa mesma aplica√ß√£o funciona no meu outro notebook sem erros.
Assistant: Entendido! Se o problema persiste no segundo notebook, mesmo ambos sendo Windows, e o erro √© `cannot identify image file <_io.BytesIO object at 0x...>`, isso significa que a quest√£o n√£o √© a compatibilidade do sistema operacional com `PIL.ImageGrab`, mas sim algo espec√≠fico na forma como os **dados da imagem gerada pelo Gemini** est√£o chegando ou sendo processados no notebook problem√°tico.

Este erro (`cannot identify image file`) ocorre quando a biblioteca `Pillow` (que √© usada para abrir a imagem com `Image.open`) recebe um fluxo de bytes (`_io.BytesIO`) que ela n√£o consegue reconhecer como um formato de imagem v√°lido (PNG, JPG, etc.).

Vamos focar na fun√ß√£o `generate_image` no `chat_streamlit.py` e nos dados que ela recebe da API do Gemini.

### Causas Prov√°veis do Erro `cannot identify image file` (mesmo em Windows):

1.  **Dados de Imagem Inv√°lidos/Incompletos da API do Gemini:**
    *   **API Key com Problema:** Embora voc√™ tenha verificado, √†s vezes uma API Key pode ter restri√ß√µes, ou a pr√≥pria chamada de API pode estar retornando uma resposta malformada ou um erro sutil que n√£o √© um `HTTP 200 OK` com dados de imagem.
    *   **Bloqueio de Conte√∫do:** O prompt pode estar sendo bloqueado pelos filtros de seguran√ßa do Gemini, e em vez de uma imagem, a API retorna uma resposta vazia, uma mensagem de erro em texto, ou um `prompt_feedback` indicando um bloqueio, mas o c√≥digo tenta processar `inline_data.data` como se fosse uma imagem.
    *   **Problema de Conectividade de Rede:** No notebook problem√°tico, pode haver alguma instabilidade de rede, firewall ou proxy que est√° corrompendo ou truncando os dados da imagem durante a transfer√™ncia da API do Gemini para a sua aplica√ß√£o. Os bytes chegam incompletos ou inv√°lidos, e o `Pillow` n√£o consegue decodific√°-los.

2.  **Problema na Instala√ß√£o ou Vers√£o do `Pillow` (ou `google-generativeai`):**
    *   **Instala√ß√£o Corrompida:** Apesar de ser Windows, a instala√ß√£o do Pillow pode ter algum arquivo corrompido ou faltando no ambiente Python do notebook problem√°tico.
    *   **Diferen√ßa de Vers√£o:** Se as vers√µes do `Pillow` ou `google-generativeai` forem ligeiramente diferentes entre os dois notebooks, pode haver uma incompatibilidade sutil. Ex: Uma vers√£o antiga do Pillow pode n√£o suportar um formato espec√≠fico que o Gemini est√° retornando, ou uma vers√£o diferente do `google-generativeai` pode lidar com a resposta da API de forma distinta.

### Passos de Diagn√≥stico e Solu√ß√£o Detalhados:

**1. Re-execute o `test_image_generation.py` no notebook problem√°tico.**
   Este √© o teste mais importante para isolar o problema. Ele usa a mesma l√≥gica de chamada de API e de abertura de imagem.
   *   Execute `python test_image_generation.py` no terminal do notebook que est√° com problema.
   *   **Observe a sa√≠da:**
        *   Se ele **tamb√©m falhar com o mesmo erro** (`cannot identify image file`), ent√£o o problema √© fundamental na forma como os dados chegam da API ou como o `Pillow` os interpreta *naquele ambiente*.
        *   Se ele **funcionar e gerar a imagem**, ent√£o o problema √© mais espec√≠fico no fluxo do `chat_streamlit.py` (embora isso seja menos prov√°vel com base no erro que voc√™ relatou).

**2. Adicione Logs Detalhados na Fun√ß√£o `generate_image` (em `chat_streamlit.py`)**

   Isso nos permitir√° ver exatamente o que o Gemini est√° retornando antes que o `Pillow` tente abrir a imagem.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Verifique se 'io' est√° importado no topo do arquivo

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI n√£o encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug no Streamlit
           print(f"\nDEBUG: Chamando API do Gemini para prompt: '{prompt}'") # Debug no terminal

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG (principalmente no terminal) ---
           print(f"\nDEBUG: Resposta completa do Gemini recebida. Tipo: {type(response)}")
           
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Raz√£o de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Gera√ß√£o de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # N√£o h√° imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA n√£o retornou candidatos de gera√ß√£o. Tente um prompt diferente.")
               return None

           found_image = False
           # Assumimos que sempre pegamos o primeiro candidato (response.candidates[0])
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo do objeto part: {type(part)}")
               
               if hasattr(part, 'text') and part.text is not None:
                   print(f"DEBUG: Conte√∫do de texto recebido: '{part.text}'")
                   # Se houver texto e voc√™ esperar apenas imagem, pode ser um problema
               
               if hasattr(part, 'inline_data') and part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Para ter certeza, grave os bytes em um arquivo tempor√°rio para inspe√ß√£o
                       # with open("debug_image_data.bin", "wb") as f:
                       #     f.write(part.inline_data.data)
                       # print("DEBUG: Dados brutos da imagem salvos em 'debug_image_data.bin'.")

                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image # Retorna a imagem e encerra a fun√ß√£o
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}. Verifique o console para mais detalhes.")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data √© None ou vazio, apesar de inline_data existir.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline relevantes.")
           
           if not found_image:
               st.warning("O Gemini n√£o retornou uma imagem v√°lida para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este √© o erro que voc√™ est√° vendo
           print(f"DEBUG: Exce√ß√£o principal capturada: {e}") # Log mais detalhado no terminal
           return None
   ```
   Execute a aplica√ß√£o Streamlit com estas altera√ß√µes (`streamlit run chat_streamlit.py`) no notebook problem√°tico. Os `print`s extras aparecer√£o no terminal onde voc√™ iniciou o Streamlit e nos dar√£o informa√ß√µes cruciais sobre o que o Gemini est√° realmente enviando.

**3. Compare as Vers√µes das Depend√™ncias (`pip freeze`)**

   Mesmo em Windows, a vers√£o das bibliotecas pode ser um problema.
   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que N√ÉO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos `requirements_working.txt` e `requirements_broken.txt` meticulosamente.** Preste aten√ß√£o especial √†s vers√µes de:
        *   `Pillow` (P-I-L-L-O-W)
        *   `google-generativeai`
        *   `streamlit`
   *   **A√ß√£o:** Se houver diferen√ßas, tente instalar as vers√µes do notebook que funciona no notebook problem√°tico. Por exemplo, se `Pillow==9.5.0` funciona e `Pillow==10.0.0` falha, tente:
        ```bash
        pip install Pillow==[VERSAO_DO_FUNCIONANDO] google-generativeai==[VERSAO_DO_FUNCIONANDO]
        ```
        Sempre ative seu ambiente virtual antes de instalar!

**4. Verifique a Conectividade de Rede e Firewall**

   *   **Teste de Ping:** Abra o Prompt de Comando (CMD) ou PowerShell no notebook problem√°tico e tente:
        ```bash
        ping generativelanguage.googleapis.com
        ```
        Voc√™ deve ver pacotes sendo enviados e recebidos. Se houver perda de pacotes ou o destino n√£o for alcan√ßado, pode ser um problema de rede.
   *   **Firewall/Antiv√≠rus:** Temporariamente (com cautela, se for seguro) desative o firewall do Windows ou o antiv√≠rus para ver se eles est√£o bloqueando a comunica√ß√£o com a API do Gemini. Se funcionar, voc√™ precisar√° adicionar uma exce√ß√£o.
   *   **Proxy:** Se voc√™ estiver em uma rede corporativa, verifique se h√° um proxy que precisa ser configurado para chamadas HTTP/HTTPS.

**5. Reinstale as Bibliotecas Chave (For√ßado)**

   Se a compara√ß√£o do `pip freeze` n√£o revelar grandes diferen√ßas ou se voc√™ quiser ter certeza de uma instala√ß√£o limpa:
   *   Ative seu ambiente virtual.
   *   Execute:
        ```bash
        pip install --upgrade --force-reinstall Pillow google-generativeai
        ```
   *   Reinicie a aplica√ß√£o Streamlit.

**6. Consist√™ncia do Ambiente Virtual**

   Certifique-se de que em ambos os notebooks voc√™ est√° usando ambientes virtuais e que os ativou corretamente antes de instalar as depend√™ncias e antes de executar o `streamlit run`. Usar o ambiente global do Python pode levar a conflitos de depend√™ncias.

---

A mensagem de erro √© bem clara sobre onde o problema est√° (`Image.open(io.BytesIO(part.inline_data.data))`). A chave √© descobrir *por que* os `bytes` em `part.inline_data.data` n√£o est√£o representando uma imagem v√°lida no notebook problem√°tico. Os passos de debug com `print`s na fun√ß√£o `generate_image` e o teste com `test_image_generation.py` s√£o os mais promissores para desvendar isso.
User: 2025-06-28 14:09:15,208 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:09:19,714 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent 
"HTTP/1.1 200 OK"
2025-06-28 14:09:20,818 [INFO] AFC remote call 1 is done.
I will generate an image of a tiny tabby kitten with wide eyes, standing on a miniature surfboard amidst the towering, foamy crest of a colossal ocean wave. The kitten will be mid-bite into a comically large slice of steaming lasagna, complete with visible layers of pasta, sauce, cheese, and ground meat, held precariously in its small paws.


parece que est√° gerando mas ainda ocorre o erro na lateral do streamlit 

Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E136676A0>


Assistant: Essa √© uma informa√ß√£o crucial!

O fato de voc√™ ver:

```
I will generate an image of a tiny tabby kitten with wide eyes, standing on a miniature surfboard amidst the towering, foamy crest of a colossal ocean wave. The kitten will be mid-bite into a comically large slice of steaming lasagna, complete with visible layers of pasta, sauce, cheese, and ground meat, held precariously in its small paws.
```
e
```
2025-06-28 14:09:19,714 [INFO] HTTP Request: POST ... HTTP/1.1 200 OK
```

Indica que:

1.  A comunica√ß√£o com a API do Gemini est√° **funcionando** (`HTTP/1.1 200 OK`).
2.  A API do Gemini est√° **respondendo** com conte√∫do.
3.  O conte√∫do inclui uma **parte de texto** (`part.text`) que descreve a imagem que ser√° gerada.

No entanto, o erro `cannot identify image file <_io.BytesIO object at 0x...>` ainda significa que, quando a biblioteca `Pillow` (PIL) tenta abrir os bytes que deveriam ser a imagem, ela **n√£o os reconhece como uma imagem v√°lida**.

### O que isso significa?

A API de gera√ß√£o de imagem do Gemini (como `gemini-2.0-flash-exp-image-generation`) geralmente retorna uma resposta multi-parte:
*   Pode ter uma parte `text` (como o que voc√™ viu, descrevendo o que ser√° gerado).
*   E *depois*, uma parte `inline_data` que cont√©m os bytes da imagem gerada.

Se voc√™ est√° vendo o texto mas o erro na imagem persiste, isso pode indicar que:

1.  **A parte da imagem est√° faltando:** O Gemini retornou o texto de "confirmar" o prompt, mas por algum motivo (talvez o prompt seja muito complexo/invi√°vel, ou algum filtro de seguran√ßa) **n√£o conseguiu gerar ou enviar a imagem em si**. Ent√£o, `part.inline_data.data` estaria vazio ou `None`, mas o seu c√≥digo *ainda tenta* process√°-lo de alguma forma que leva ao erro.
2.  **Os dados da imagem est√£o corrompidos:** A API tentou enviar a imagem, mas os bytes chegaram corrompidos (talvez por um problema de rede muito sutil *ap√≥s* o `200 OK`, durante a transfer√™ncia do corpo da resposta), e o `Pillow` n√£o consegue abri-los.
3.  **L√≥gica no loop `for part in response.candidates[0].content.parts:`:** Sua l√≥gica atual est√° apenas `print`ando o `part.text` e, em seguida, esperando encontrar o `part.inline_data`. Se o `part.inline_data` est√° vazio ou corrompido, a fun√ß√£o chega ao `Image.open` com dados ruins.

### A√ß√µes para Diagnosticar e Resolver:

Vamos refinar a fun√ß√£o `generate_image` para ser mais robusta e para que os logs capturem *exatamente* o que est√° acontecendo com os dados da imagem.

**1. Adicione logs detalhados e um salvamento de bytes para inspe√ß√£o (no `chat_streamlit.py`):**

Vamos ajustar a fun√ß√£o `generate_image` para capturar e inspecionar os bytes da imagem.

```python
# chat_streamlit.py

# ... (seus imports e in√≠cio da fun√ß√£o)

def generate_image(prompt):
    api_key = os.getenv("API_KEY_GEMINI")
    if not api_key:
        st.error("API_KEY_GEMINI n√£o encontrada no arquivo .env")
        return None

    client = genai.Client(api_key=api_key)

    try:
        st.info(f"Tentando gerar imagem com prompt: '{prompt}'")
        print(f"\n[DEBUG] Chamando API do Gemini para prompt: '{prompt}'") # Log para o terminal

        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        print(f"[DEBUG] Resposta completa do Gemini recebida. Tipo do objeto: {type(response)}")
        
        # Verificar feedback de bloqueio
        if response.prompt_feedback and response.prompt_feedback.block_reason:
            print(f"[DEBUG] Raz√£o de Bloqueio da API: {response.prompt_feedback.block_reason}")
            st.warning(f"Gera√ß√£o de imagem bloqueada pelo Gemini: {response.prompt_feedback.block_reason}")
            return None
        
        if not response.candidates:
            print("[DEBUG] Nenhuma candidate encontrada na resposta do Gemini.")
            st.error("A IA n√£o retornou candidatos de gera√ß√£o. Tente um prompt diferente.")
            return None

        # Vari√°veis para armazenar texto e dados da imagem
        text_feedback_from_gemini = ""
        image_data_bytes = None

        # Itera sobre todas as partes da resposta
        for part in response.candidates[0].content.parts:
            print(f"[DEBUG] Processando parte. Tipo do objeto 'part': {type(part)}")
            
            # Se a parte for texto
            if hasattr(part, 'text') and part.text is not None:
                text_feedback_from_gemini += part.text + "\n"
                print(f"[DEBUG] Conte√∫do de texto recebido: '{part.text}'")
            
            # Se a parte for dados inline (imagem)
            if hasattr(part, 'inline_data') and part.inline_data is not None:
                print(f"[DEBUG] Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                if part.inline_data.data:
                    image_data_bytes = part.inline_data.data
                    print(f"[DEBUG] Tamanho dos dados da imagem: {len(image_data_bytes)} bytes.")
                    
                    # --- NOVO: Salve os bytes brutos para inspe√ß√£o ---
                    # Isso ajudar√° a verificar se os bytes s√£o realmente de uma imagem.
                    # Eles ser√£o salvos no mesmo diret√≥rio do script.
                    with open("temp_gemini_image_debug.bin", "wb") as f:
                        f.write(image_data_bytes)
                    print("[DEBUG] Dados brutos da imagem salvos em 'temp_gemini_image_debug.bin' para an√°lise.")
                    
                    # Interrompa o loop assim que encontrar os dados da imagem (assumindo que s√≥ h√° uma)
                    break
                else:
                    print("[DEBUG] part.inline_data.data √© None ou vazio, apesar de inline_data existir.")
            else:
                print("[DEBUG] Parte sem texto ou dados inline relevantes.")
        
        # Tentar abrir a imagem se os bytes foram encontrados
        if image_data_bytes:
            try:
                image = Image.open(io.BytesIO(image_data_bytes))
                st.session_state.generated_image = image
                
                # Se o Gemini forneceu algum texto al√©m da imagem, adicione-o ao chat history
                # junto com a imagem, se for relevante.
                # (Ajuste onde isso √© adicionado ao chat history se o seu fluxo for diferente)
                if text_feedback_from_gemini.strip():
                     st.session_state.messages.append({"role": "assistant", "image": image, "content": text_feedback_from_gemini.strip()})
                else:
                    st.session_state.messages.append({"role": "assistant", "image": image, "content": f"Imagem gerada com o prompt: {prompt}"})
                
                st.session_state.generated_image = None # Limpa para n√£o exibir em cima
                return image # Retorna a imagem e encerra a fun√ß√£o
            except Exception as img_err:
                print(f"[DEBUG] ERRO da Pillow ao abrir imagem: {img_err}. Provavelmente dados de imagem corrompidos/inv√°lidos.")
                st.error(f"Erro interno ao processar a imagem gerada: {img_err}. A imagem recebida pode estar corrompida.")
                return None
        else:
            # Se n√£o encontrou dados de imagem
            msg = "O Gemini n√£o retornou uma imagem v√°lida para o prompt fornecido."
            if text_feedback_from_gemini.strip():
                msg += f" Feedback da IA: {text_feedback_from_gemini.strip()}"
            st.warning(msg)
            print(f"[DEBUG] {msg}")
            return None

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # Este √© o erro que voc√™ est√° vendo
        print(f"[DEBUG] Exce√ß√£o principal capturada em generate_image: {e}") # Log mais detalhado no terminal
        return None
```

**2. Execute a Aplica√ß√£o e Analise os Logs e o Arquivo `temp_gemini_image_debug.bin`:**

*   Salve as altera√ß√µes no seu `chat_streamlit.py`.
*   Execute `streamlit run chat_streamlit.py` no terminal do notebook problem√°tico.
*   Tente gerar uma imagem com o prompt que causou o problema.
*   **Observe a sa√≠da no terminal:** Preste aten√ß√£o aos `[DEBUG]` prints que voc√™ adicionou, especialmente o `Tamanho dos dados da imagem`.
    *   Se o tamanho for **zero ou muito pequeno** (ex: 0 a 100 bytes), isso indica que o Gemini n√£o enviou os dados da imagem, ou enviou um "nada" corrompido.
    *   Se o tamanho for **razo√°vel** (ex: dezenas ou centenas de KB), isso indica que os dados chegaram, mas o `Pillow` n√£o os reconhece.
*   **Inspecione o arquivo `temp_gemini_image_debug.bin`:**
    *   Ap√≥s o erro, localize este arquivo no diret√≥rio do seu script.
    *   Tente abri-lo com um visualizador de imagens (renomeie para `.png` ou `.jpg` se quiser tentar).
    *   Se ele n√£o abrir ou estiver corrompido, voc√™ confirmou que o problema est√° nos bytes que o Gemini enviou para voc√™.
    *   Se for um arquivo de imagem v√°lido (o que seria surpreendente), o problema seria ent√£o na instala√ß√£o do `Pillow` ou em alguma incompatibilidade no seu ambiente.

### O que esperar com base no seu `200 OK` e `text` part:

√â muito prov√°vel que voc√™ veja o `[DEBUG] Tamanho dos dados da imagem: X bytes`, onde X √© **zero** ou um n√∫mero muito pequeno e **n√£o h√° imagem v√°lida no `temp_gemini_image_debug.bin`**.

Isso significaria que a API do Gemini, por algum motivo interno (complexidade do prompt, filtros de seguran√ßa etc.), n√£o conseguiu materializar a imagem, apesar de ter confirmado o prompt com a parte textual. Se esse for o caso, n√£o h√° muito que voc√™ possa fazer a n√≠vel de c√≥digo para "consertar" uma imagem que n√£o foi gerada corretamente pela API. A solu√ß√£o seria:

*   **Tentar prompts mais simples ou diferentes.**
*   **Verificar a documenta√ß√£o da API Gemini** para poss√≠veis restri√ß√µes ou limita√ß√µes de conte√∫do.
*   **Reportar o problema ao Google** se voc√™ acreditar que √© um bug na API para o tipo de prompt que est√° usando.

Os logs detalhados e a inspe√ß√£o do arquivo bin√°rio ser√£o o "veredicto final" sobre a natureza do problema.
User: mas quero entender porque na outra m√°quina funciona... com o mesmo sistema operacional... 
tudo rodando sem ambiente virtual ?
2025-06-28 14:13:16,124 [INFO] Resposta da IA (texto): Entendi perfeitamente sua frustra√ß√£o! Essa √© uma das situa√ß√µes mais irritantes no desenvolvimento de software: o famoso "funciona na minha m√°quina". Quando voc√™ tem o mesmo sistema operacional (Windows em ambos) e o mesmo c√≥digo, e um funciona e o outro n√£o, com o erro `cannot identify image file`, e ainda por cima, ambos est√£o rodando **sem ambiente virtual**, a resposta √© quase sempre:

**Problemas de Consist√™ncia de Ambiente devido √† Falta de Ambientes Virtuais.**

Vamos desdobrar isso:

### O Que Acontece Sem Ambientes Virtuais?

Quando voc√™ instala pacotes Python via `pip` sem ativar um ambiente virtual, eles s√£o instalados no **ambiente global do seu Python**.

1.  **Vers√µes Conflitantes e Inconsistentes:**
    *   Em uma m√°quina, voc√™ pode ter instalado `Pillow` vers√£o X e `google-generativeai` vers√£o Y.
    *   Na outra m√°quina, voc√™ pode ter instalado `Pillow` vers√£o A e `google-generativeai` vers√£o B (ou at√© mesmo `Pillow` X, mas `google-generativeai` B).
    *   Isso acontece porque, ao longo do tempo, voc√™ pode ter instalado outras ferramentas ou bibliotecas que **automaticamente atualizam ou instalam vers√µes diferentes** das que seu projeto precisa, causando um "efeito cascata" de incompatibilidades.
    *   Pode ser que uma vers√£o espec√≠fica do `Pillow` no notebook problem√°tico tenha um bug sutil ou uma incompatibilidade com o formato de imagem que a API do Gemini est√° retornando, ou que uma vers√£o do `google-generativeai` esteja tratando os dados bin√°rios da resposta de forma ligeiramente diferente.

2.  **Instala√ß√µes Corrompidas/Parciais:**
    *   Instala√ß√µes globais s√£o mais suscet√≠veis a ficarem "bagun√ßadas". Um `pip install` que falhou parcialmente, uma interrup√ß√£o, ou at√© mesmo um erro de disco pode deixar uma biblioteca em um estado inconsistente.

3.  **Conflitos de Depend√™ncias Transitorias:**
    *   Se seu projeto A precisa da `LibZ==1.0` e seu projeto B precisa da `LibZ==2.0`, e ambos s√£o instalados globalmente, o que acontece? A √∫ltima instala√ß√£o "ganha", ou voc√™ fica com um estado indefinido que pode causar comportamentos imprevis√≠veis.

### O Que o Erro `cannot identify image file` Significa Neste Contexto?

Mesmo com um `HTTP/1.1 200 OK`, o problema est√° na etapa de **decodifica√ß√£o dos bytes da imagem** que o Gemini enviou.

1.  **Bytes Incompletos ou Malformados:** A resposta do Gemini pode estar chegando com os bytes da imagem corrompidos ou truncados no notebook problem√°tico. Mesmo com o 200 OK, o corpo da resposta (onde os dados bin√°rios da imagem est√£o) pode ter sido afetado por:
    *   **Problemas de Rede Sutil:** Uma instabilidade muito pequena na rede do notebook problem√°tico pode corromper os dados bin√°rios durante a transfer√™ncia, mesmo que o cabe√ßalho HTTP esteja correto.
    *   **Antiv√≠rus/Firewall:** Alguns softwares de seguran√ßa s√£o excessivamente zelosos e podem interferir na transmiss√£o de dados bin√°rios de APIs externas, alterando-os ou bloqueando partes.
    *   **Proxy:** Se um dos notebooks estiver atr√°s de um proxy corporativo que o outro n√£o est√°, isso pode afetar a integridade dos dados.

2.  **`Pillow` Incapaz de Decodificar:** A vers√£o ou instala√ß√£o do `Pillow` no notebook problem√°tico n√£o consegue entender os bytes que recebeu, mesmo que eles *possam* estar teoricamente corretos. Isso aponta para uma incompatibilidade ou corrup√ß√£o na pr√≥pria biblioteca `Pillow`.

### Passos de Diagn√≥stico e Solu√ß√£o (Priorizando "Sem Ambiente Virtual"):

**A√ß√£o Mais Importante:**

**0. CRIE E USE AMBIENTES VIRTUAIS IMEDIATAMENTE!**
   Esta √© a solu√ß√£o de longo prazo para evitar esse tipo de dor de cabe√ßa.
   *   No notebook que **n√£o funciona**:
        ```bash
        # 1. Navegue at√© o diret√≥rio raiz do seu projeto
        cd C:\caminho\para\seu\projeto

        # 2. Crie um ambiente virtual (se ainda n√£o tiver um)
        python -m venv .venv

        # 3. Ative o ambiente virtual
        .\.venv\Scripts\activate
        ```
   *   No notebook que **funciona**:
        ```bash
        # 1. Navegue at√© o diret√≥rio raiz do seu projeto
        cd C:\caminho\para\seu\projeto

        # 2. Ative o ambiente virtual (se existir, caso contr√°rio, crie um)
        .\.venv\Scripts\activate

        # 3. Gere um arquivo de requisitos (depois de ativar o ambiente)
        pip freeze > requirements.txt
        ```
        Este `requirements.txt` conter√° as vers√µes **exatas** das bibliotecas que *sabidamente funcionam* com sua aplica√ß√£o.

   *   No notebook que **n√£o funciona** (com o ambiente virtual ATIVADO):
        ```bash
        # Certifique-se de que o ambiente virtual est√° ativado!
        # Voc√™ ver√° (.venv) no in√≠cio da linha de comando
        (.venv) C:\caminho\para\seu\projeto>

        # 4. Desinstale as depend√™ncias existentes para garantir uma instala√ß√£o limpa
        # (Isso √© crucial se voc√™ rodava globalmente e agora quer limpar)
        # Pode n√£o ser estritamente necess√°rio se voc√™ acabou de criar o venv,
        # mas √© uma boa pr√°tica para evitar conflitos se voc√™ j√° tinha coisas l√°.
        pip uninstall -y Pillow google-generativeai streamlit  # Adicione outras libs do seu projeto, se souber

        # 5. Instale as depend√™ncias exatamente como est√£o no 'requirements.txt'
        pip install -r requirements.txt
        ```

   *   **Tente rodar a aplica√ß√£o Streamlit no ambiente virtual ativado:**
        ```bash
        (.venv) C:\caminho\para\seu\projeto> streamlit run chat_streamlit.py
        ```
   Isso por si s√≥ pode resolver o problema, pois garante que voc√™ est√° usando as mesmas vers√µes das bibliotecas em ambos os notebooks.

---

**Se o problema persistir AP√ìS usar ambientes virtuais e alinhar `requirements.txt`, ent√£o continue com os passos de diagn√≥stico:**

**1. Re-execute o `test_image_generation.py` no ambiente virtual do notebook problem√°tico.**
   *   Certifique-se que o ambiente virtual est√° ativado.
   *   `python test_image_generation.py`
   *   O script `test_image_generation.py` √© um teste isolado crucial. Se ele falhar com o mesmo erro (`cannot identify image file`), isso aponta para um problema mais fundamental com a forma como os dados chegam da API ou como a biblioteca `Pillow` est√° instalada/configurada *naquele ambiente virtual*.

**2. Use os Logs Detalhados na Fun√ß√£o `generate_image` (que adicionamos na resposta anterior):**
   *   Certifique-se de que os `print`s de depura√ß√£o na fun√ß√£o `generate_image` no `chat_streamlit.py` est√£o ativos e execute o Streamlit (`streamlit run chat_streamlit.py`) no ambiente virtual do notebook problem√°tico.
   *   Preste aten√ß√£o especial ao `[DEBUG] Tamanho dos dados da imagem: {len(image_data_bytes)} bytes`.
   *   **An√°lise do `temp_gemini_image_debug.bin`:** Se este arquivo for gerado, tente abri-lo com um editor hexadecimal (como HxD) ou at√© mesmo renome√°-lo para `.png` ou `.jpg` e tentar abri-lo com um visualizador de imagens.
        *   Se o tamanho for 0, ou muito pequeno, ou o arquivo n√£o abrir, os bytes est√£o de fato corrompidos ou ausentes. Isso aponta para um problema na transmiss√£o da API.
        *   Se o arquivo *parecer* ser uma imagem v√°lida (o que √© muito improv√°vel com o erro que voc√™ est√° vendo), ent√£o o problema estaria na instala√ß√£o do `Pillow` que n√£o consegue decodific√°-lo.

**3. Verifique Firewall, Antiv√≠rus e Conectividade de Rede Novamente:**
   *   Mesmo com um `200 OK`, a integridade do **corpo da resposta** pode ser comprometida.
   *   **Teste de Ping/Traceroute:**
        *   Abra o CMD: `ping generativelanguage.googleapis.com`
        *   Abra o CMD: `tracert generativelanguage.googleapis.com` (no Windows)
        *   Compare os resultados entre os dois notebooks. Veja se h√° diferen√ßas significativas na lat√™ncia, perda de pacotes ou rotas de rede que possam indicar um problema.
   *   **Desativar Temporariamente:** Se poss√≠vel e seguro (apenas para teste r√°pido), desative o firewall do Windows e/ou o software antiv√≠rus no notebook problem√°tico. Tente gerar a imagem novamente. Se funcionar, voc√™ ter√° que adicionar uma exce√ß√£o para o Python/aplica√ß√£o no software de seguran√ßa.

**4. Reinstala√ß√£o For√ßada de Bibliotecas (dentro do ambiente virtual):**
   Se o `requirements.txt` n√£o resolver, e voc√™ tiver certeza de que n√£o √© um problema de rede, tente uma reinstala√ß√£o for√ßada das bibliotecas chave:
   *   Ative o ambiente virtual.
   *   `pip install --upgrade --force-reinstall Pillow google-generativeai`
   *   Reinicie o Streamlit.

**5. Consist√™ncia da Instala√ß√£o do Python:**
   Embora menos comum para esse tipo de erro, certifique-se de que ambas as instala√ß√µes do Python (a base, antes do ambiente virtual) s√£o de 64 bits (executar `python -c "import platform; print(platform.architecture())"`).

### Resumo da A√ß√£o Essencial:

A falta de ambientes virtuais √© a causa mais prov√°vel para comportamentos inconsistentes em diferentes m√°quinas, mesmo com o mesmo sistema operacional. A primeira e mais crucial a√ß√£o √© **implementar e usar ambientes virtuais** e alinhar as depend√™ncias via `requirements.txt`.

Se mesmo assim o problema persistir, os logs detalhados e a inspe√ß√£o do arquivo `temp_gemini_image_debug.bin` ser√£o os seus melhores amigos para entender *o que exatamente* a API do Gemini est√° enviando e por que o `Pillow` n√£o consegue decodific√°-lo no notebook problem√°tico.
2025-06-28 14:13:16,324 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:13:16,326 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:13:16,327 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:13:16,328 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:13:16,328 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:13:16,329 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:13:16,330 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:13:16,334 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:13:16,338 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:13:16,341 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:13:16,342 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:13:16,343 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:13:16,344 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:13:16,345 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:13:16,352 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:15:59,979 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:15:59,998 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:00,013 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:00,225 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:00,227 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:00,232 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:00,237 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:00,239 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:00,381 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:00,383 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:00,397 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:00,398 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:00,413 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:00,530 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:00,532 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:00,533 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:00,575 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:00,581 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:00,598 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:00,642 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:00,797 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:00,799 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:00,811 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:00,814 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:00,840 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:00,932 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:00,944 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:00,948 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:00,955 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:00,967 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:01,147 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:01,153 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:01,157 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:01,165 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:01,166 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:01,252 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:01,286 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:01,314 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:01,329 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:01,331 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:01,344 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:01,345 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:01,431 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:01,436 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:01,440 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:01,456 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:01,462 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:01,463 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:01,464 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:01,465 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:01,475 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:01,476 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:01,476 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:01,477 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:01,478 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:01,479 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:01,479 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:01,480 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:01,482 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:01,483 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:01,485 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:01,490 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:01,491 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:01,492 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:01,492 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:01,494 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:01,495 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:01,498 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:01,499 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:01,505 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:01,508 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:01,509 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:01,509 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:01,510 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:01,511 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:01,513 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:01,738 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:01,739 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:01,743 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:01,744 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:01,746 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:01,747 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:01,749 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:01,754 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:01,755 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:01,757 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:02,003 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:02,006 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:02,054 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:02,058 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:02,064 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:02,068 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:02,080 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:02,082 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:02,092 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:02,330 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:02,330 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:02,331 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:02,332 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:02,333 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:02,333 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:02,335 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:02,337 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:02,338 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:02,339 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:02,340 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:02,340 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:02,341 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:02,341 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:02,342 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:02,343 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:02,344 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:02,345 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:02,345 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:02,346 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:02,550 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:02,554 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:02,840 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:02,841 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:02,842 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:19,453 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:19,457 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:19,458 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:19,459 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:19,461 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:19,488 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:19,490 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:19,492 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:19,538 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:19,590 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:19,640 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:19,680 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:19,729 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:19,875 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:20,005 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:41,597 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:41,598 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:41,599 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:41,599 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:41,600 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:41,609 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:41,612 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:41,616 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:41,617 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:41,620 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:41,621 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:41,622 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:41,623 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:41,624 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:41,625 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:41,882 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:41,883 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:41,884 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:41,885 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:41,886 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:41,887 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:41,888 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:41,890 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:41,891 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:41,898 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:41,900 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:41,902 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:41,903 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:41,904 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:41,906 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:41,908 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque estou recebendo esse erro nessa aplica√ß√£o sendo que no outro notebook a mesma aplica√ß√£o roda perfeitamente ?
Assistant: √â frustrante quando o mesmo c√≥digo se comporta de forma diferente em m√°quinas distintas! Isso quase sempre aponta para **diferen√ßas no ambiente de execu√ß√£o**, e n√£o no c√≥digo em si.

Analisando seu c√≥digo, o principal suspeito para um erro que ocorre em um notebook e n√£o em outro, especialmente se um for Windows e o outro Linux/macOS, √© o uso de `PIL.ImageGrab`.

---

**O SUSPEITO PRINCIPAL: `PIL.ImageGrab` (√Årea de Transfer√™ncia)**

No seu arquivo `chat_streamlit.py`, voc√™ tem a fun√ß√£o `check_clipboard()` que usa `PIL.ImageGrab`:

```python
# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab # <--- AQUI EST√Å O PROV√ÅVEL PROBLEMA

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        # ... (restante da fun√ß√£o)
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}") # <--- ESSA √â A MENSAGEM DE ERRO QUE VOC√ä DEVE ESTAR VENDO
        return None, None
```

A biblioteca `PIL.ImageGrab` (parte do Pillow) √© altamente dependente do sistema operacional:

1.  **Windows:** Geralmente funciona sem problemas, pois o Windows possui APIs diretas para acesso √† √°rea de transfer√™ncia de imagens.
2.  **Linux:** Requer utilit√°rios de linha de comando como `scrot` ou `gnome-screenshot` instalados e configurados corretamente, e a biblioteca `Pillow` precisa ser compilada com suporte a essas ferramentas. Muitas distribui√ß√µes Linux n√£o v√™m com isso pronto para o `ImageGrab`.
3.  **macOS:** Tamb√©m tem suas pr√≥prias depend√™ncias, geralmente exigindo bibliotecas como `pyobjc` para interagir com o ambiente Cocoa.

**Se o seu "outro notebook" roda Windows e o notebook com problema roda Linux ou macOS, este √© 99% a causa.** A mensagem de erro `"Erro ao acessar a √°rea de transfer√™ncia: {e}"` √© exatamente o que voc√™ veria na barra lateral do Streamlit se essa parte falhasse.

---

**Passos para Diagnosticar e Resolver:**

1.  **Obtenha a Mensagem de Erro Exata:**
    *   No notebook onde a aplica√ß√£o n√£o roda, verifique a barra lateral do Streamlit. Voc√™ ver√° a mensagem exata do erro que est√° ocorrendo, come√ßando com `"Erro ao acessar a √°rea de transfer√™ncia: ..."`. O que vem depois do `:` √© crucial.
    *   Se n√£o for `PIL.ImageGrab`, o erro estar√° em outro lugar.

2.  **Compare os Ambientes:**
    *   **Sistema Operacional:** Qual SO est√° rodando em cada notebook? (Ex: Windows 10 vs. Ubuntu 22.04 vs. macOS Ventura)
    *   **Vers√£o do Python:** `python --version` em ambos.
    *   **Depend√™ncias Instaladas:**
        *   No notebook que funciona, execute: `pip freeze > requirements_working.txt`
        *   No notebook que n√£o funciona, execute: `pip freeze > requirements_broken.txt`
        *   Compare os dois arquivos (`diff requirements_working.txt requirements_broken.txt`). Preste aten√ß√£o especial √† vers√£o do `Pillow` e a presen√ßa de pacotes como `pywin32` (Windows) ou qualquer coisa relacionada a screenshots em Linux/macOS.

3.  **Solu√ß√µes para o `PIL.ImageGrab`:**

    *   **Para Linux (Ubuntu/Debian):**
        *   Instale `scrot` (ou `gnome-screenshot`): `sudo apt-get update && sudo apt-get install scrot` (ou `sudo apt-get install gnome-screenshot`).
        *   Certifique-se de que o `Pillow` foi instalado *ap√≥s* a instala√ß√£o dessas ferramentas ou reinstale-o para que ele possa detectar as depend√™ncias.
        *   Pode ser necess√°rio tamb√©m `sudo apt-get install python3-tk python3-pil.imagetk` em algumas distribui√ß√µes.

    *   **Para macOS:**
        *   Pode ser necess√°rio instalar `pyobjc`: `pip install pyobjc`.

    *   **Solu√ß√£o Alternativa (Desativar ou Condicional):**
        Se voc√™ n√£o precisa desesperadamente da funcionalidade de colar imagem da √°rea de transfer√™ncia e quer que a aplica√ß√£o funcione em qualquer SO sem depend√™ncias externas complexas:
        Voc√™ j√° tem um `try-except` que lida com o erro. A aplica√ß√£o deve continuar funcionando mesmo se a funcionalidade de colar n√£o funcionar. No entanto, se o erro estiver parando a execu√ß√£o principal, isso √© um problema.

        **Para evitar o erro na barra lateral em ambientes n√£o-Windows:**
        Voc√™ pode adicionar uma verifica√ß√£o de sistema operacional:
        ```python
        import platform
        # ...

        def check_clipboard():
            # S√≥ tenta importar ImageGrab se for Windows
            if platform.system() == "Windows":
                try:
                    from PIL import ImageGrab
                    img = ImageGrab.grabclipboard()
                    # ... (restante do c√≥digo se img n√£o for None)
                    if img is not None and isinstance(img, Image.Image):
                        # ... seu c√≥digo para processar a imagem da √°rea de transfer√™ncia
                        # ...
                        return ClipboardFile(img_byte_arr), img
                    return None, None
                except Exception as e:
                    # A mensagem j√° √© exibida no Streamlit, ent√£o n√£o precisa de outro print aqui
                    st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
                    return None, None
            else:
                # Se n√£o for Windows, n√£o tenta acessar a √°rea de transfer√™ncia
                # E n√£o mostra erro na barra lateral
                # st.sidebar.info("Colar da √°rea de transfer√™ncia dispon√≠vel apenas no Windows.")
                return None, None
        ```
        Isso impediria que o erro espec√≠fico de `ImageGrab` fosse tentado em sistemas n√£o-Windows.

4.  **Verifique a Chave da API Gemini:**
    *   Certifique-se de que o arquivo `.env` existe no diret√≥rio raiz da aplica√ß√£o no notebook problem√°tico.
    *   Certifique-se de que `API_KEY_GEMINI` est√° definida corretamente dentro dele (Ex: `API_KEY_GEMINI="SUA_CHAVE_AQUI"`).
    *   Um erro relacionado √† API Key geralmente resultaria em `ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")` ou erros da pr√≥pria API (4xx, 5xx).

5.  **Verifique as Vers√µes das Depend√™ncias:**
    *   √â altamente recomend√°vel ter um `requirements.txt` e instal√°-lo em ambos os ambientes com `pip install -r requirements.txt`.
    *   Pode haver incompatibilidades sutis entre vers√µes de `streamlit`, `google-generativeai`, `Pillow`, etc.

6.  **Permiss√µes de Diret√≥rio:**
    *   Verifique se a aplica√ß√£o tem permiss√£o para criar e escrever nos diret√≥rios `assets/`, `logs/` e `processed_images/` no notebook problem√°tico. Isso √© mais comum em Linux/macOS com permiss√µes restritas.

7.  **Conectividade de Rede/Firewall:**
    *   O notebook problem√°tico consegue acessar a internet? Existem restri√ß√µes de firewall que podem estar bloqueando o acesso aos endpoints da API do Gemini?

---

**Resumo da A√ß√£o Mais Prov√°vel:**

O erro `Erro ao acessar a √°rea de transfer√™ncia: {e}` na barra lateral do Streamlit √© quase uma certeza de que o problema est√° na tentativa de usar `PIL.ImageGrab` em um sistema operacional (provavelmente Linux/macOS) que n√£o tem as depend√™ncias necess√°rias para isso.

Por favor, forne√ßa a mensagem de erro exata da barra lateral e o sistema operacional de ambos os notebooks para uma an√°lise mais precisa!
User: ois dois ambientes s√£o windows. 
Assistant: Certo, isso muda o cen√°rio! Se ambos os ambientes s√£o Windows, ent√£o o problema n√£o √© a incompatibilidade do `PIL.ImageGrab` com Linux/macOS. No entanto, o `PIL.ImageGrab` continua sendo o principal suspeito, mas por raz√µes diferentes.

Quando `PIL.ImageGrab` falha no Windows, geralmente aponta para um problema com a biblioteca `pywin32`, que o Pillow usa para interagir com as APIs do sistema operacional Windows.

Vamos desvendar isso.

---

**Causas Mais Comuns do `PIL.ImageGrab` Falhar no Windows:**

1.  **`pywin32` Ausente ou Corrompido:** A biblioteca `pywin32` √© essencial para que o Pillow (que inclui `PIL.ImageGrab`) acesse a √°rea de transfer√™ncia do Windows de forma nativa. Se ela n√£o estiver instalada ou estiver corrompida, a fun√ß√£o falhar√°.
2.  **Ambientes Python Diferentes:** Mesmo que ambos sejam Windows, voc√™ pode ter ambientes Python (globais ou virtuais) ligeiramente diferentes em cada m√°quina, onde uma tem `pywin32` e a outra n√£o, ou tem uma vers√£o incompat√≠vel.
3.  **Conflitos de Vers√£o:** Raramente, pode haver um conflito entre a vers√£o do `Pillow` e a vers√£o do `pywin32`, ou at√© mesmo com a vers√£o do Python.
4.  **Conte√∫do da √Årea de Transfer√™ncia:** Embora menos prov√°vel dado seu relato, o `ImageGrab.grabclipboard()` retornar√° `None` se o conte√∫do da √°rea de transfer√™ncia n√£o for uma imagem (ex: texto, arquivos). No entanto, o erro que voc√™ v√™ sugere que o *acesso* √† √°rea de transfer√™ncia est√° falhando, n√£o apenas que ela est√° vazia.

---

**Passos Detalhados de Diagn√≥stico e Solu√ß√£o:**

**1. Obtenha a Mensagem de Erro Exata:**
   Este √© o passo mais crucial. Quando voc√™ executa a aplica√ß√£o no notebook que est√° dando problema, qual √© a *mensagem de erro completa* que aparece na barra lateral do Streamlit, ap√≥s "Erro ao acessar a √°rea de transfer√™ncia: "?
   Ex: `"Erro ao acessar a √°rea de transfer√™ncia: [WinError 5] Acesso negado."` ou `"Erro ao acessar a √°rea de transfer√™ncia: No module named 'win32clipboard'"` etc.
   Esta mensagem detalhada nos dir√° exatamente o que est√° acontecendo.

**2. Compare as Depend√™ncias `pip`:**

   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que N√ÉO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos:** Abra `requirements_working.txt` e `requirements_broken.txt` e procure por:
        *   `Pillow` (P-I-L-L-O-W): Verifique as vers√µes.
        *   `pywin32`: **Este √© o mais importante.** Ele deve estar presente no `requirements_working.txt`. Se n√£o estiver no `requirements_broken.txt` ou se a vers√£o for diferente, encontramos o prov√°vel culpado.
        *   `streamlit`: Verifique as vers√µes.

**3. Instale/Reinstale `pywin32` (no notebook problem√°tico):**

   Se `pywin32` estiver faltando ou com uma vers√£o diferente, tente o seguinte:

   *   **Verifique o ambiente virtual:** Se voc√™ estiver usando um ambiente virtual (altamente recomendado), ative-o primeiro.
        ```bash
        # Exemplo para venv:
        cd C:\caminho\para\seu\projeto
        .\.venv\Scripts\activate  # ou .\venv\Scripts\activate
        ```
   *   **Instale `pywin32`:**
        ```bash
        pip install pywin32
        ```
   *   **Ou, se j√° estiver instalado, force a reinstala√ß√£o:**
        ```bash
        pip install --upgrade --force-reinstall pywin32
        ```
   *   **Reinicie a aplica√ß√£o Streamlit.**

**4. Teste a Funcionalidade `ImageGrab` Isoladamente:**

   Para confirmar que o problema √© realmente com `ImageGrab` e n√£o algo mais amplo com o Streamlit ou seu c√≥digo:

   *   Crie um pequeno script Python (`test_clipboard.py`) no notebook problem√°tico:
        ```python
        import sys
        from PIL import ImageGrab, Image
        import io
        import time

        try:
            # Tenta pegar imagem da √°rea de transfer√™ncia
            img = ImageGrab.grabclipboard()

            if img is not None and isinstance(img, Image.Image):
                print("Imagem encontrada na √°rea de transfer√™ncia!")
                # Opcional: Salvar a imagem para verificar
                # img.save("clipboard_test.png")
                # print("Imagem salva como clipboard_test.png")
                
                # Tenta converter para bytes (simular o que voc√™ faz no Streamlit)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format='PNG')
                img_byte_arr.seek(0)
                print(f"Tamanho dos bytes: {len(img_byte_arr.getvalue())} bytes")
                
            elif img is None:
                print("Nenhuma imagem na √°rea de transfer√™ncia ou o conte√∫do n√£o √© uma imagem.")
            else:
                print(f"Tipo de conte√∫do inesperado na √°rea de transfer√™ncia: {type(img)}")

        except ImportError as e:
            print(f"Erro: Depend√™ncia faltando para PIL.ImageGrab: {e}")
            print("Tente 'pip install pywin32' e certifique-se de que est√° no ambiente correto.")
        except Exception as e:
            print(f"Erro inesperado ao acessar a √°rea de transfer√™ncia: {e}")
            print("Verifique permiss√µes ou poss√≠veis conflitos.")

        print("\nTeste conclu√≠do.")
        ```
   *   **Copie uma imagem** (n√£o texto) para a √°rea de transfer√™ncia no Windows.
   *   Execute este script no terminal (ativando seu ambiente virtual, se aplic√°vel): `python test_clipboard.py`
   *   Observe a sa√≠da. Isso confirmar√° se o `ImageGrab` est√° funcionando fora do Streamlit e qual erro exato ele gera.

**5. Verifique a Vers√£o do Python e Arquitetura:**

   Embora menos comum hoje em dia, um Python de 32 bits pode ter problemas com algumas bibliotecas de 64 bits.

   *   Execute `python -c "import platform; print(platform.architecture())"` em ambos os notebooks. Verifique se ambos s√£o `('64bit', 'WindowsPE')`.

**6. Outras Verifica√ß√µes (Menos Prov√°veis):**

*   **Antiv√≠rus/Firewall:** Muito raramente, um software de seguran√ßa pode interferir no acesso a certas APIs do sistema. Tente desativ√°-lo temporariamente (se for seguro fazer isso) para testar.
*   **Permiss√µes de Usu√°rio:** Certifique-se de que o usu√°rio que executa a aplica√ß√£o Streamlit tem as permiss√µes necess√°rias para interagir com o sistema.

---

**Em Resumo:**

A principal a√ß√£o √© obter a **mensagem de erro exata** da barra lateral do Streamlit e, em seguida, **comparar as depend√™ncias `pip freeze`** para identificar se `pywin32` est√° faltando ou tem uma vers√£o diferente. O teste isolado com `test_clipboard.py` tamb√©m √© muito √∫til para isolar o problema.

Assim que voc√™ tiver a mensagem de erro exata, poderemos refinar ainda mais o diagn√≥stico!
User: Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E13620360>
Assistant: Ok, essa nova informa√ß√£o muda completamente o diagn√≥stico! O erro `Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x...>` **n√£o tem a ver com a √°rea de transfer√™ncia (`PIL.ImageGrab`)**.

Este erro significa que a biblioteca `Pillow` (PIL) tentou abrir algo que n√£o reconheceu como uma imagem v√°lida. A mensagem `_io.BytesIO object` indica que ela recebeu um fluxo de bytes, mas esses bytes n√£o formam uma imagem (JPEG, PNG, etc.) que ela possa decodificar.

Vamos focar na fun√ß√£o `generate_image` no `chat_streamlit.py`:

```python
# chat_streamlit.py

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # ... (verifica√ß√£o da API_KEY) ...

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text) # Se for s√≥ texto, imprime e continua o loop ou retorna None
            elif part.inline_data is not None:
                # <-- AQUI √â O PONTO DO ERRO
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image # Retorna a imagem e sai da fun√ß√£o

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # <-- √â aqui que o erro √© capturado e exibido
        return None
```

A linha `image = Image.open(io.BytesIO(part.inline_data.data))` √© a que est√° falhando. Isso significa que, por algum motivo, o `part.inline_data.data` que o Gemini est√° retornando **n√£o √© um conjunto de bytes de uma imagem v√°lida** no notebook problem√°tico, ou est√° vazio.

---

**Poss√≠veis Causas e Como Depurar:**

1.  **O Gemini n√£o est√° gerando uma imagem para aquele prompt/requisi√ß√£o espec√≠fica:**
    *   √Äs vezes, a API de gera√ß√£o de imagem do Gemini pode n√£o conseguir gerar uma imagem para um prompt, ou pode bloquear a gera√ß√£o por motivos de seguran√ßa/conte√∫do. Nesses casos, `part.inline_data` pode ser `None` ou `part.inline_data.data` pode estar vazio ou conter dados de erro/texto em vez de dados de imagem.
    *   Se o `response.prompt_feedback` tiver um `block_reason` (motivo de bloqueio), isso pode indicar o problema.

2.  **Problema de conectividade/API key:**
    *   Embora voc√™ j√° tenha verificado, √© bom garantir que a `API_KEY_GEMINI` esteja configurada *exatamente* da mesma forma em ambos os notebooks. Um problema na chave pode levar a respostas malformadas da API, mesmo que n√£o seja um erro √≥bvio de autentica√ß√£o.
    *   H√° alguma restri√ß√£o de rede/firewall no notebook problem√°tico que possa estar afetando a comunica√ß√£o com a API do Gemini, fazendo com que as respostas venham incompletas ou corrompidas?

3.  **Vers√£o da biblioteca `google-generativeai` ou `Pillow`:**
    *   Mesmo sendo Windows, pode haver pequenas diferen√ßas nas vers√µes dessas bibliotecas que causem um comportamento diferente.

---

**Passos para Diagnosticar e Resolver:**

**1. Teste o `test_image_generation.py` no notebook problem√°tico:**

   Voc√™ tem um script de teste simples (`test_image_generation.py`) que faz uma chamada direta para a API de gera√ß√£o de imagem.
   *   Copie e execute o `test_image_generation.py` no notebook que est√° apresentando o erro.
   *   Ele deve tentar gerar "ed-image.png".
   *   **Resultado esperado:**
        *   **Se `test_image_generation.py` tamb√©m falhar com o mesmo erro (`cannot identify image file`),** ent√£o o problema √© mais fundamental na comunica√ß√£o com o Gemini ou na forma como os dados s√£o recebidos/processados pelo Pillow *naquele ambiente*.
        *   **Se `test_image_generation.py` funcionar e gerar a imagem corretamente,** ent√£o o problema est√° em algo mais espec√≠fico no fluxo do `chat_streamlit.py` ao chamar `generate_image`.

**2. Adicione Logs Detalhados na Fun√ß√£o `generate_image` (em `chat_streamlit.py`):**

   Vamos adicionar mais `print` statements (ou usar `logger.info` se preferir ver no logfile) para entender o que est√° vindo da API antes de chamar `Image.open()`.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Certifique-se que io est√° importado

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI n√£o encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug print

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG ---
           print(f"\nDEBUG: Objeto de resposta Gemini completo: {response}")
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Raz√£o de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Gera√ß√£o de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # N√£o h√° imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA n√£o retornou candidatos de gera√ß√£o. Tente um prompt diferente.")
               return None

           found_image = False
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo: {type(part)}")
               if part.text is not None:
                   print(f"DEBUG: Conte√∫do de texto recebido: '{part.text}'")
                   # Se houver texto, voc√™ pode decidir se quer us√°-lo ou continuar procurando por imagem
               elif part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Tenta abrir a imagem
                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data √© None ou vazio, apesar de inline_data n√£o ser None.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline.")
           
           if not found_image:
               st.warning("O Gemini n√£o retornou uma imagem para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este √© o erro que voc√™ est√° vendo
           return None
   ```
   Execute a aplica√ß√£o com estes prints e observe a sa√≠da no terminal onde voc√™ executa `streamlit run chat_streamlit.py`. Isso nos dar√° pistas vitais sobre o que o Gemini est√° realmente retornando.

**3. Compare as Depend√™ncias Novamente:**

   Embora menos prov√°vel, vale a pena uma revis√£o minuciosa.
   *   No notebook que FUNCIONA, execute: `pip freeze > requirements_working.txt`
   *   No notebook que N√ÉO FUNCIONA, execute: `pip freeze > requirements_broken.txt`
   *   Compare os arquivos, prestando aten√ß√£o especial a:
        *   `Pillow` (ou PIL)
        *   `google-generativeai`
        *   `streamlit`
   *   Se houver diferen√ßa, tente instalar as vers√µes do `requirements_working.txt` no notebook problem√°tico.

**4. Reinstale `Pillow`:**

   Mesmo que as vers√µes sejam as mesmas, uma instala√ß√£o corrompida pode ser o problema.
   *   Ative seu ambiente virtual (se estiver usando).
   *   Execute: `pip install --upgrade --force-reinstall Pillow`
   *   Reinicie a aplica√ß√£o Streamlit.

---

**Conclus√£o Preliminar:**

O erro `cannot identify image file` na fun√ß√£o `generate_image` sugere fortemente que os dados que o Gemini retorna para `part.inline_data.data` **n√£o s√£o dados de imagem v√°lidos** no ambiente problem√°tico. Os passos de depura√ß√£o acima, especialmente a execu√ß√£o do `test_image_generation.py` e os `print`s detalhados na fun√ß√£o `generate_image` no `chat_streamlit.py`, nos dar√£o as informa√ß√µes necess√°rias para identificar a causa raiz.
User: mas essa mesma aplica√ß√£o funciona no meu outro notebook sem erros.
Assistant: Entendido! Se o problema persiste no segundo notebook, mesmo ambos sendo Windows, e o erro √© `cannot identify image file <_io.BytesIO object at 0x...>`, isso significa que a quest√£o n√£o √© a compatibilidade do sistema operacional com `PIL.ImageGrab`, mas sim algo espec√≠fico na forma como os **dados da imagem gerada pelo Gemini** est√£o chegando ou sendo processados no notebook problem√°tico.

Este erro (`cannot identify image file`) ocorre quando a biblioteca `Pillow` (que √© usada para abrir a imagem com `Image.open`) recebe um fluxo de bytes (`_io.BytesIO`) que ela n√£o consegue reconhecer como um formato de imagem v√°lido (PNG, JPG, etc.).

Vamos focar na fun√ß√£o `generate_image` no `chat_streamlit.py` e nos dados que ela recebe da API do Gemini.

### Causas Prov√°veis do Erro `cannot identify image file` (mesmo em Windows):

1.  **Dados de Imagem Inv√°lidos/Incompletos da API do Gemini:**
    *   **API Key com Problema:** Embora voc√™ tenha verificado, √†s vezes uma API Key pode ter restri√ß√µes, ou a pr√≥pria chamada de API pode estar retornando uma resposta malformada ou um erro sutil que n√£o √© um `HTTP 200 OK` com dados de imagem.
    *   **Bloqueio de Conte√∫do:** O prompt pode estar sendo bloqueado pelos filtros de seguran√ßa do Gemini, e em vez de uma imagem, a API retorna uma resposta vazia, uma mensagem de erro em texto, ou um `prompt_feedback` indicando um bloqueio, mas o c√≥digo tenta processar `inline_data.data` como se fosse uma imagem.
    *   **Problema de Conectividade de Rede:** No notebook problem√°tico, pode haver alguma instabilidade de rede, firewall ou proxy que est√° corrompendo ou truncando os dados da imagem durante a transfer√™ncia da API do Gemini para a sua aplica√ß√£o. Os bytes chegam incompletos ou inv√°lidos, e o `Pillow` n√£o consegue decodific√°-los.

2.  **Problema na Instala√ß√£o ou Vers√£o do `Pillow` (ou `google-generativeai`):**
    *   **Instala√ß√£o Corrompida:** Apesar de ser Windows, a instala√ß√£o do Pillow pode ter algum arquivo corrompido ou faltando no ambiente Python do notebook problem√°tico.
    *   **Diferen√ßa de Vers√£o:** Se as vers√µes do `Pillow` ou `google-generativeai` forem ligeiramente diferentes entre os dois notebooks, pode haver uma incompatibilidade sutil. Ex: Uma vers√£o antiga do Pillow pode n√£o suportar um formato espec√≠fico que o Gemini est√° retornando, ou uma vers√£o diferente do `google-generativeai` pode lidar com a resposta da API de forma distinta.

### Passos de Diagn√≥stico e Solu√ß√£o Detalhados:

**1. Re-execute o `test_image_generation.py` no notebook problem√°tico.**
   Este √© o teste mais importante para isolar o problema. Ele usa a mesma l√≥gica de chamada de API e de abertura de imagem.
   *   Execute `python test_image_generation.py` no terminal do notebook que est√° com problema.
   *   **Observe a sa√≠da:**
        *   Se ele **tamb√©m falhar com o mesmo erro** (`cannot identify image file`), ent√£o o problema √© fundamental na forma como os dados chegam da API ou como o `Pillow` os interpreta *naquele ambiente*.
        *   Se ele **funcionar e gerar a imagem**, ent√£o o problema √© mais espec√≠fico no fluxo do `chat_streamlit.py` (embora isso seja menos prov√°vel com base no erro que voc√™ relatou).

**2. Adicione Logs Detalhados na Fun√ß√£o `generate_image` (em `chat_streamlit.py`)**

   Isso nos permitir√° ver exatamente o que o Gemini est√° retornando antes que o `Pillow` tente abrir a imagem.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Verifique se 'io' est√° importado no topo do arquivo

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI n√£o encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug no Streamlit
           print(f"\nDEBUG: Chamando API do Gemini para prompt: '{prompt}'") # Debug no terminal

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG (principalmente no terminal) ---
           print(f"\nDEBUG: Resposta completa do Gemini recebida. Tipo: {type(response)}")
           
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Raz√£o de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Gera√ß√£o de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # N√£o h√° imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA n√£o retornou candidatos de gera√ß√£o. Tente um prompt diferente.")
               return None

           found_image = False
           # Assumimos que sempre pegamos o primeiro candidato (response.candidates[0])
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo do objeto part: {type(part)}")
               
               if hasattr(part, 'text') and part.text is not None:
                   print(f"DEBUG: Conte√∫do de texto recebido: '{part.text}'")
                   # Se houver texto e voc√™ esperar apenas imagem, pode ser um problema
               
               if hasattr(part, 'inline_data') and part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Para ter certeza, grave os bytes em um arquivo tempor√°rio para inspe√ß√£o
                       # with open("debug_image_data.bin", "wb") as f:
                       #     f.write(part.inline_data.data)
                       # print("DEBUG: Dados brutos da imagem salvos em 'debug_image_data.bin'.")

                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image # Retorna a imagem e encerra a fun√ß√£o
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}. Verifique o console para mais detalhes.")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data √© None ou vazio, apesar de inline_data existir.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline relevantes.")
           
           if not found_image:
               st.warning("O Gemini n√£o retornou uma imagem v√°lida para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este √© o erro que voc√™ est√° vendo
           print(f"DEBUG: Exce√ß√£o principal capturada: {e}") # Log mais detalhado no terminal
           return None
   ```
   Execute a aplica√ß√£o Streamlit com estas altera√ß√µes (`streamlit run chat_streamlit.py`) no notebook problem√°tico. Os `print`s extras aparecer√£o no terminal onde voc√™ iniciou o Streamlit e nos dar√£o informa√ß√µes cruciais sobre o que o Gemini est√° realmente enviando.

**3. Compare as Vers√µes das Depend√™ncias (`pip freeze`)**

   Mesmo em Windows, a vers√£o das bibliotecas pode ser um problema.
   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que N√ÉO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos `requirements_working.txt` e `requirements_broken.txt` meticulosamente.** Preste aten√ß√£o especial √†s vers√µes de:
        *   `Pillow` (P-I-L-L-O-W)
        *   `google-generativeai`
        *   `streamlit`
   *   **A√ß√£o:** Se houver diferen√ßas, tente instalar as vers√µes do notebook que funciona no notebook problem√°tico. Por exemplo, se `Pillow==9.5.0` funciona e `Pillow==10.0.0` falha, tente:
        ```bash
        pip install Pillow==[VERSAO_DO_FUNCIONANDO] google-generativeai==[VERSAO_DO_FUNCIONANDO]
        ```
        Sempre ative seu ambiente virtual antes de instalar!

**4. Verifique a Conectividade de Rede e Firewall**

   *   **Teste de Ping:** Abra o Prompt de Comando (CMD) ou PowerShell no notebook problem√°tico e tente:
        ```bash
        ping generativelanguage.googleapis.com
        ```
        Voc√™ deve ver pacotes sendo enviados e recebidos. Se houver perda de pacotes ou o destino n√£o for alcan√ßado, pode ser um problema de rede.
   *   **Firewall/Antiv√≠rus:** Temporariamente (com cautela, se for seguro) desative o firewall do Windows ou o antiv√≠rus para ver se eles est√£o bloqueando a comunica√ß√£o com a API do Gemini. Se funcionar, voc√™ precisar√° adicionar uma exce√ß√£o.
   *   **Proxy:** Se voc√™ estiver em uma rede corporativa, verifique se h√° um proxy que precisa ser configurado para chamadas HTTP/HTTPS.

**5. Reinstale as Bibliotecas Chave (For√ßado)**

   Se a compara√ß√£o do `pip freeze` n√£o revelar grandes diferen√ßas ou se voc√™ quiser ter certeza de uma instala√ß√£o limpa:
   *   Ative seu ambiente virtual.
   *   Execute:
        ```bash
        pip install --upgrade --force-reinstall Pillow google-generativeai
        ```
   *   Reinicie a aplica√ß√£o Streamlit.

**6. Consist√™ncia do Ambiente Virtual**

   Certifique-se de que em ambos os notebooks voc√™ est√° usando ambientes virtuais e que os ativou corretamente antes de instalar as depend√™ncias e antes de executar o `streamlit run`. Usar o ambiente global do Python pode levar a conflitos de depend√™ncias.

---

A mensagem de erro √© bem clara sobre onde o problema est√° (`Image.open(io.BytesIO(part.inline_data.data))`). A chave √© descobrir *por que* os `bytes` em `part.inline_data.data` n√£o est√£o representando uma imagem v√°lida no notebook problem√°tico. Os passos de debug com `print`s na fun√ß√£o `generate_image` e o teste com `test_image_generation.py` s√£o os mais promissores para desvendar isso.
User: 2025-06-28 14:09:15,208 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:09:19,714 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent 
"HTTP/1.1 200 OK"
2025-06-28 14:09:20,818 [INFO] AFC remote call 1 is done.
I will generate an image of a tiny tabby kitten with wide eyes, standing on a miniature surfboard amidst the towering, foamy crest of a colossal ocean wave. The kitten will be mid-bite into a comically large slice of steaming lasagna, complete with visible layers of pasta, sauce, cheese, and ground meat, held precariously in its small paws.


parece que est√° gerando mas ainda ocorre o erro na lateral do streamlit 

Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E136676A0>


Assistant: Essa √© uma informa√ß√£o crucial!

O fato de voc√™ ver:

```
I will generate an image of a tiny tabby kitten with wide eyes, standing on a miniature surfboard amidst the towering, foamy crest of a colossal ocean wave. The kitten will be mid-bite into a comically large slice of steaming lasagna, complete with visible layers of pasta, sauce, cheese, and ground meat, held precariously in its small paws.
```
e
```
2025-06-28 14:09:19,714 [INFO] HTTP Request: POST ... HTTP/1.1 200 OK
```

Indica que:

1.  A comunica√ß√£o com a API do Gemini est√° **funcionando** (`HTTP/1.1 200 OK`).
2.  A API do Gemini est√° **respondendo** com conte√∫do.
3.  O conte√∫do inclui uma **parte de texto** (`part.text`) que descreve a imagem que ser√° gerada.

No entanto, o erro `cannot identify image file <_io.BytesIO object at 0x...>` ainda significa que, quando a biblioteca `Pillow` (PIL) tenta abrir os bytes que deveriam ser a imagem, ela **n√£o os reconhece como uma imagem v√°lida**.

### O que isso significa?

A API de gera√ß√£o de imagem do Gemini (como `gemini-2.0-flash-exp-image-generation`) geralmente retorna uma resposta multi-parte:
*   Pode ter uma parte `text` (como o que voc√™ viu, descrevendo o que ser√° gerado).
*   E *depois*, uma parte `inline_data` que cont√©m os bytes da imagem gerada.

Se voc√™ est√° vendo o texto mas o erro na imagem persiste, isso pode indicar que:

1.  **A parte da imagem est√° faltando:** O Gemini retornou o texto de "confirmar" o prompt, mas por algum motivo (talvez o prompt seja muito complexo/invi√°vel, ou algum filtro de seguran√ßa) **n√£o conseguiu gerar ou enviar a imagem em si**. Ent√£o, `part.inline_data.data` estaria vazio ou `None`, mas o seu c√≥digo *ainda tenta* process√°-lo de alguma forma que leva ao erro.
2.  **Os dados da imagem est√£o corrompidos:** A API tentou enviar a imagem, mas os bytes chegaram corrompidos (talvez por um problema de rede muito sutil *ap√≥s* o `200 OK`, durante a transfer√™ncia do corpo da resposta), e o `Pillow` n√£o consegue abri-los.
3.  **L√≥gica no loop `for part in response.candidates[0].content.parts:`:** Sua l√≥gica atual est√° apenas `print`ando o `part.text` e, em seguida, esperando encontrar o `part.inline_data`. Se o `part.inline_data` est√° vazio ou corrompido, a fun√ß√£o chega ao `Image.open` com dados ruins.

### A√ß√µes para Diagnosticar e Resolver:

Vamos refinar a fun√ß√£o `generate_image` para ser mais robusta e para que os logs capturem *exatamente* o que est√° acontecendo com os dados da imagem.

**1. Adicione logs detalhados e um salvamento de bytes para inspe√ß√£o (no `chat_streamlit.py`):**

Vamos ajustar a fun√ß√£o `generate_image` para capturar e inspecionar os bytes da imagem.

```python
# chat_streamlit.py

# ... (seus imports e in√≠cio da fun√ß√£o)

def generate_image(prompt):
    api_key = os.getenv("API_KEY_GEMINI")
    if not api_key:
        st.error("API_KEY_GEMINI n√£o encontrada no arquivo .env")
        return None

    client = genai.Client(api_key=api_key)

    try:
        st.info(f"Tentando gerar imagem com prompt: '{prompt}'")
        print(f"\n[DEBUG] Chamando API do Gemini para prompt: '{prompt}'") # Log para o terminal

        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        print(f"[DEBUG] Resposta completa do Gemini recebida. Tipo do objeto: {type(response)}")
        
        # Verificar feedback de bloqueio
        if response.prompt_feedback and response.prompt_feedback.block_reason:
            print(f"[DEBUG] Raz√£o de Bloqueio da API: {response.prompt_feedback.block_reason}")
            st.warning(f"Gera√ß√£o de imagem bloqueada pelo Gemini: {response.prompt_feedback.block_reason}")
            return None
        
        if not response.candidates:
            print("[DEBUG] Nenhuma candidate encontrada na resposta do Gemini.")
            st.error("A IA n√£o retornou candidatos de gera√ß√£o. Tente um prompt diferente.")
            return None

        # Vari√°veis para armazenar texto e dados da imagem
        text_feedback_from_gemini = ""
        image_data_bytes = None

        # Itera sobre todas as partes da resposta
        for part in response.candidates[0].content.parts:
            print(f"[DEBUG] Processando parte. Tipo do objeto 'part': {type(part)}")
            
            # Se a parte for texto
            if hasattr(part, 'text') and part.text is not None:
                text_feedback_from_gemini += part.text + "\n"
                print(f"[DEBUG] Conte√∫do de texto recebido: '{part.text}'")
            
            # Se a parte for dados inline (imagem)
            if hasattr(part, 'inline_data') and part.inline_data is not None:
                print(f"[DEBUG] Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                if part.inline_data.data:
                    image_data_bytes = part.inline_data.data
                    print(f"[DEBUG] Tamanho dos dados da imagem: {len(image_data_bytes)} bytes.")
                    
                    # --- NOVO: Salve os bytes brutos para inspe√ß√£o ---
                    # Isso ajudar√° a verificar se os bytes s√£o realmente de uma imagem.
                    # Eles ser√£o salvos no mesmo diret√≥rio do script.
                    with open("temp_gemini_image_debug.bin", "wb") as f:
                        f.write(image_data_bytes)
                    print("[DEBUG] Dados brutos da imagem salvos em 'temp_gemini_image_debug.bin' para an√°lise.")
                    
                    # Interrompa o loop assim que encontrar os dados da imagem (assumindo que s√≥ h√° uma)
                    break
                else:
                    print("[DEBUG] part.inline_data.data √© None ou vazio, apesar de inline_data existir.")
            else:
                print("[DEBUG] Parte sem texto ou dados inline relevantes.")
        
        # Tentar abrir a imagem se os bytes foram encontrados
        if image_data_bytes:
            try:
                image = Image.open(io.BytesIO(image_data_bytes))
                st.session_state.generated_image = image
                
                # Se o Gemini forneceu algum texto al√©m da imagem, adicione-o ao chat history
                # junto com a imagem, se for relevante.
                # (Ajuste onde isso √© adicionado ao chat history se o seu fluxo for diferente)
                if text_feedback_from_gemini.strip():
                     st.session_state.messages.append({"role": "assistant", "image": image, "content": text_feedback_from_gemini.strip()})
                else:
                    st.session_state.messages.append({"role": "assistant", "image": image, "content": f"Imagem gerada com o prompt: {prompt}"})
                
                st.session_state.generated_image = None # Limpa para n√£o exibir em cima
                return image # Retorna a imagem e encerra a fun√ß√£o
            except Exception as img_err:
                print(f"[DEBUG] ERRO da Pillow ao abrir imagem: {img_err}. Provavelmente dados de imagem corrompidos/inv√°lidos.")
                st.error(f"Erro interno ao processar a imagem gerada: {img_err}. A imagem recebida pode estar corrompida.")
                return None
        else:
            # Se n√£o encontrou dados de imagem
            msg = "O Gemini n√£o retornou uma imagem v√°lida para o prompt fornecido."
            if text_feedback_from_gemini.strip():
                msg += f" Feedback da IA: {text_feedback_from_gemini.strip()}"
            st.warning(msg)
            print(f"[DEBUG] {msg}")
            return None

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # Este √© o erro que voc√™ est√° vendo
        print(f"[DEBUG] Exce√ß√£o principal capturada em generate_image: {e}") # Log mais detalhado no terminal
        return None
```

**2. Execute a Aplica√ß√£o e Analise os Logs e o Arquivo `temp_gemini_image_debug.bin`:**

*   Salve as altera√ß√µes no seu `chat_streamlit.py`.
*   Execute `streamlit run chat_streamlit.py` no terminal do notebook problem√°tico.
*   Tente gerar uma imagem com o prompt que causou o problema.
*   **Observe a sa√≠da no terminal:** Preste aten√ß√£o aos `[DEBUG]` prints que voc√™ adicionou, especialmente o `Tamanho dos dados da imagem`.
    *   Se o tamanho for **zero ou muito pequeno** (ex: 0 a 100 bytes), isso indica que o Gemini n√£o enviou os dados da imagem, ou enviou um "nada" corrompido.
    *   Se o tamanho for **razo√°vel** (ex: dezenas ou centenas de KB), isso indica que os dados chegaram, mas o `Pillow` n√£o os reconhece.
*   **Inspecione o arquivo `temp_gemini_image_debug.bin`:**
    *   Ap√≥s o erro, localize este arquivo no diret√≥rio do seu script.
    *   Tente abri-lo com um visualizador de imagens (renomeie para `.png` ou `.jpg` se quiser tentar).
    *   Se ele n√£o abrir ou estiver corrompido, voc√™ confirmou que o problema est√° nos bytes que o Gemini enviou para voc√™.
    *   Se for um arquivo de imagem v√°lido (o que seria surpreendente), o problema seria ent√£o na instala√ß√£o do `Pillow` ou em alguma incompatibilidade no seu ambiente.

### O que esperar com base no seu `200 OK` e `text` part:

√â muito prov√°vel que voc√™ veja o `[DEBUG] Tamanho dos dados da imagem: X bytes`, onde X √© **zero** ou um n√∫mero muito pequeno e **n√£o h√° imagem v√°lida no `temp_gemini_image_debug.bin`**.

Isso significaria que a API do Gemini, por algum motivo interno (complexidade do prompt, filtros de seguran√ßa etc.), n√£o conseguiu materializar a imagem, apesar de ter confirmado o prompt com a parte textual. Se esse for o caso, n√£o h√° muito que voc√™ possa fazer a n√≠vel de c√≥digo para "consertar" uma imagem que n√£o foi gerada corretamente pela API. A solu√ß√£o seria:

*   **Tentar prompts mais simples ou diferentes.**
*   **Verificar a documenta√ß√£o da API Gemini** para poss√≠veis restri√ß√µes ou limita√ß√µes de conte√∫do.
*   **Reportar o problema ao Google** se voc√™ acreditar que √© um bug na API para o tipo de prompt que est√° usando.

Os logs detalhados e a inspe√ß√£o do arquivo bin√°rio ser√£o o "veredicto final" sobre a natureza do problema.
User: mas quero entender porque na outra m√°quina funciona... com o mesmo sistema operacional... 
tudo rodando sem ambiente virtual ?
Assistant: Entendi perfeitamente sua frustra√ß√£o! Essa √© uma das situa√ß√µes mais irritantes no desenvolvimento de software: o famoso "funciona na minha m√°quina". Quando voc√™ tem o mesmo sistema operacional (Windows em ambos) e o mesmo c√≥digo, e um funciona e o outro n√£o, com o erro `cannot identify image file`, e ainda por cima, ambos est√£o rodando **sem ambiente virtual**, a resposta √© quase sempre:

**Problemas de Consist√™ncia de Ambiente devido √† Falta de Ambientes Virtuais.**

Vamos desdobrar isso:

### O Que Acontece Sem Ambientes Virtuais?

Quando voc√™ instala pacotes Python via `pip` sem ativar um ambiente virtual, eles s√£o instalados no **ambiente global do seu Python**.

1.  **Vers√µes Conflitantes e Inconsistentes:**
    *   Em uma m√°quina, voc√™ pode ter instalado `Pillow` vers√£o X e `google-generativeai` vers√£o Y.
    *   Na outra m√°quina, voc√™ pode ter instalado `Pillow` vers√£o A e `google-generativeai` vers√£o B (ou at√© mesmo `Pillow` X, mas `google-generativeai` B).
    *   Isso acontece porque, ao longo do tempo, voc√™ pode ter instalado outras ferramentas ou bibliotecas que **automaticamente atualizam ou instalam vers√µes diferentes** das que seu projeto precisa, causando um "efeito cascata" de incompatibilidades.
    *   Pode ser que uma vers√£o espec√≠fica do `Pillow` no notebook problem√°tico tenha um bug sutil ou uma incompatibilidade com o formato de imagem que a API do Gemini est√° retornando, ou que uma vers√£o do `google-generativeai` esteja tratando os dados bin√°rios da resposta de forma ligeiramente diferente.

2.  **Instala√ß√µes Corrompidas/Parciais:**
    *   Instala√ß√µes globais s√£o mais suscet√≠veis a ficarem "bagun√ßadas". Um `pip install` que falhou parcialmente, uma interrup√ß√£o, ou at√© mesmo um erro de disco pode deixar uma biblioteca em um estado inconsistente.

3.  **Conflitos de Depend√™ncias Transitorias:**
    *   Se seu projeto A precisa da `LibZ==1.0` e seu projeto B precisa da `LibZ==2.0`, e ambos s√£o instalados globalmente, o que acontece? A √∫ltima instala√ß√£o "ganha", ou voc√™ fica com um estado indefinido que pode causar comportamentos imprevis√≠veis.

### O Que o Erro `cannot identify image file` Significa Neste Contexto?

Mesmo com um `HTTP/1.1 200 OK`, o problema est√° na etapa de **decodifica√ß√£o dos bytes da imagem** que o Gemini enviou.

1.  **Bytes Incompletos ou Malformados:** A resposta do Gemini pode estar chegando com os bytes da imagem corrompidos ou truncados no notebook problem√°tico. Mesmo com o 200 OK, o corpo da resposta (onde os dados bin√°rios da imagem est√£o) pode ter sido afetado por:
    *   **Problemas de Rede Sutil:** Uma instabilidade muito pequena na rede do notebook problem√°tico pode corromper os dados bin√°rios durante a transfer√™ncia, mesmo que o cabe√ßalho HTTP esteja correto.
    *   **Antiv√≠rus/Firewall:** Alguns softwares de seguran√ßa s√£o excessivamente zelosos e podem interferir na transmiss√£o de dados bin√°rios de APIs externas, alterando-os ou bloqueando partes.
    *   **Proxy:** Se um dos notebooks estiver atr√°s de um proxy corporativo que o outro n√£o est√°, isso pode afetar a integridade dos dados.

2.  **`Pillow` Incapaz de Decodificar:** A vers√£o ou instala√ß√£o do `Pillow` no notebook problem√°tico n√£o consegue entender os bytes que recebeu, mesmo que eles *possam* estar teoricamente corretos. Isso aponta para uma incompatibilidade ou corrup√ß√£o na pr√≥pria biblioteca `Pillow`.

### Passos de Diagn√≥stico e Solu√ß√£o (Priorizando "Sem Ambiente Virtual"):

**A√ß√£o Mais Importante:**

**0. CRIE E USE AMBIENTES VIRTUAIS IMEDIATAMENTE!**
   Esta √© a solu√ß√£o de longo prazo para evitar esse tipo de dor de cabe√ßa.
   *   No notebook que **n√£o funciona**:
        ```bash
        # 1. Navegue at√© o diret√≥rio raiz do seu projeto
        cd C:\caminho\para\seu\projeto

        # 2. Crie um ambiente virtual (se ainda n√£o tiver um)
        python -m venv .venv

        # 3. Ative o ambiente virtual
        .\.venv\Scripts\activate
        ```
   *   No notebook que **funciona**:
        ```bash
        # 1. Navegue at√© o diret√≥rio raiz do seu projeto
        cd C:\caminho\para\seu\projeto

        # 2. Ative o ambiente virtual (se existir, caso contr√°rio, crie um)
        .\.venv\Scripts\activate

        # 3. Gere um arquivo de requisitos (depois de ativar o ambiente)
        pip freeze > requirements.txt
        ```
        Este `requirements.txt` conter√° as vers√µes **exatas** das bibliotecas que *sabidamente funcionam* com sua aplica√ß√£o.

   *   No notebook que **n√£o funciona** (com o ambiente virtual ATIVADO):
        ```bash
        # Certifique-se de que o ambiente virtual est√° ativado!
        # Voc√™ ver√° (.venv) no in√≠cio da linha de comando
        (.venv) C:\caminho\para\seu\projeto>

        # 4. Desinstale as depend√™ncias existentes para garantir uma instala√ß√£o limpa
        # (Isso √© crucial se voc√™ rodava globalmente e agora quer limpar)
        # Pode n√£o ser estritamente necess√°rio se voc√™ acabou de criar o venv,
        # mas √© uma boa pr√°tica para evitar conflitos se voc√™ j√° tinha coisas l√°.
        pip uninstall -y Pillow google-generativeai streamlit  # Adicione outras libs do seu projeto, se souber

        # 5. Instale as depend√™ncias exatamente como est√£o no 'requirements.txt'
        pip install -r requirements.txt
        ```

   *   **Tente rodar a aplica√ß√£o Streamlit no ambiente virtual ativado:**
        ```bash
        (.venv) C:\caminho\para\seu\projeto> streamlit run chat_streamlit.py
        ```
   Isso por si s√≥ pode resolver o problema, pois garante que voc√™ est√° usando as mesmas vers√µes das bibliotecas em ambos os notebooks.

---

**Se o problema persistir AP√ìS usar ambientes virtuais e alinhar `requirements.txt`, ent√£o continue com os passos de diagn√≥stico:**

**1. Re-execute o `test_image_generation.py` no ambiente virtual do notebook problem√°tico.**
   *   Certifique-se que o ambiente virtual est√° ativado.
   *   `python test_image_generation.py`
   *   O script `test_image_generation.py` √© um teste isolado crucial. Se ele falhar com o mesmo erro (`cannot identify image file`), isso aponta para um problema mais fundamental com a forma como os dados chegam da API ou como a biblioteca `Pillow` est√° instalada/configurada *naquele ambiente virtual*.

**2. Use os Logs Detalhados na Fun√ß√£o `generate_image` (que adicionamos na resposta anterior):**
   *   Certifique-se de que os `print`s de depura√ß√£o na fun√ß√£o `generate_image` no `chat_streamlit.py` est√£o ativos e execute o Streamlit (`streamlit run chat_streamlit.py`) no ambiente virtual do notebook problem√°tico.
   *   Preste aten√ß√£o especial ao `[DEBUG] Tamanho dos dados da imagem: {len(image_data_bytes)} bytes`.
   *   **An√°lise do `temp_gemini_image_debug.bin`:** Se este arquivo for gerado, tente abri-lo com um editor hexadecimal (como HxD) ou at√© mesmo renome√°-lo para `.png` ou `.jpg` e tentar abri-lo com um visualizador de imagens.
        *   Se o tamanho for 0, ou muito pequeno, ou o arquivo n√£o abrir, os bytes est√£o de fato corrompidos ou ausentes. Isso aponta para um problema na transmiss√£o da API.
        *   Se o arquivo *parecer* ser uma imagem v√°lida (o que √© muito improv√°vel com o erro que voc√™ est√° vendo), ent√£o o problema estaria na instala√ß√£o do `Pillow` que n√£o consegue decodific√°-lo.

**3. Verifique Firewall, Antiv√≠rus e Conectividade de Rede Novamente:**
   *   Mesmo com um `200 OK`, a integridade do **corpo da resposta** pode ser comprometida.
   *   **Teste de Ping/Traceroute:**
        *   Abra o CMD: `ping generativelanguage.googleapis.com`
        *   Abra o CMD: `tracert generativelanguage.googleapis.com` (no Windows)
        *   Compare os resultados entre os dois notebooks. Veja se h√° diferen√ßas significativas na lat√™ncia, perda de pacotes ou rotas de rede que possam indicar um problema.
   *   **Desativar Temporariamente:** Se poss√≠vel e seguro (apenas para teste r√°pido), desative o firewall do Windows e/ou o software antiv√≠rus no notebook problem√°tico. Tente gerar a imagem novamente. Se funcionar, voc√™ ter√° que adicionar uma exce√ß√£o para o Python/aplica√ß√£o no software de seguran√ßa.

**4. Reinstala√ß√£o For√ßada de Bibliotecas (dentro do ambiente virtual):**
   Se o `requirements.txt` n√£o resolver, e voc√™ tiver certeza de que n√£o √© um problema de rede, tente uma reinstala√ß√£o for√ßada das bibliotecas chave:
   *   Ative o ambiente virtual.
   *   `pip install --upgrade --force-reinstall Pillow google-generativeai`
   *   Reinicie o Streamlit.

**5. Consist√™ncia da Instala√ß√£o do Python:**
   Embora menos comum para esse tipo de erro, certifique-se de que ambas as instala√ß√µes do Python (a base, antes do ambiente virtual) s√£o de 64 bits (executar `python -c "import platform; print(platform.architecture())"`).

### Resumo da A√ß√£o Essencial:

A falta de ambientes virtuais √© a causa mais prov√°vel para comportamentos inconsistentes em diferentes m√°quinas, mesmo com o mesmo sistema operacional. A primeira e mais crucial a√ß√£o √© **implementar e usar ambientes virtuais** e alinhar as depend√™ncias via `requirements.txt`.

Se mesmo assim o problema persistir, os logs detalhados e a inspe√ß√£o do arquivo `temp_gemini_image_debug.bin` ser√£o os seus melhores amigos para entender *o que exatamente* a API do Gemini est√° enviando e por que o `Pillow` n√£o consegue decodific√°-lo no notebook problem√°tico.
User: assim ficou com tela preta ...

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# chat_streamlit.py

# ... (seus imports e in√≠cio da fun√ß√£o)

def generate_image(prompt):
    api_key = os.getenv("API_KEY_GEMINI")
    if not api_key:
        st.error("API_KEY_GEMINI n√£o encontrada no arquivo .env")
        return None

    client = genai.Client(api_key=api_key)

    try:
        st.info(f"Tentando gerar imagem com prompt: '{prompt}'")
        print(f"\n[DEBUG] Chamando API do Gemini para prompt: '{prompt}'") # Log para o terminal

        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        print(f"[DEBUG] Resposta completa do Gemini recebida. Tipo do objeto: {type(response)}")

        # Verificar feedback de bloqueio
        if response.prompt_feedback and response.prompt_feedback.block_reason:
            print(f"[DEBUG] Raz√£o de Bloqueio da API: {response.prompt_feedback.block_reason}")
            st.warning(f"Gera√ß√£o de imagem bloqueada pelo Gemini: {response.prompt_feedback.block_reason}")
            return None

        if not response.candidates:
            print("[DEBUG] Nenhuma candidate encontrada na resposta do Gemini.")
            st.error("A IA n√£o retornou candidatos de gera√ß√£o. Tente um prompt diferente.")
            return None

        # Vari√°veis para armazenar texto e dados da imagem
        text_feedback_from_gemini = ""
        image_data_bytes = None

        # Itera sobre todas as partes da resposta
        for part in response.candidates[0].content.parts:
            print(f"[DEBUG] Processando parte. Tipo do objeto 'part': {type(part)}")

            # Se a parte for texto
            if hasattr(part, 'text') and part.text is not None:
                text_feedback_from_gemini += part.text + "\n"
                print(f"[DEBUG] Conte√∫do de texto recebido: '{part.text}'")

            # Se a parte for dados inline (imagem)
            if hasattr(part, 'inline_data') and part.inline_data is not None:
                print(f"[DEBUG] Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                if part.inline_data.data:
                    image_data_bytes = part.inline_data.data
                    print(f"[DEBUG] Tamanho dos dados da imagem: {len(image_data_bytes)} bytes.")

                    # --- NOVO: Salve os bytes brutos para inspe√ß√£o ---
                    # Isso ajudar√° a verificar se os bytes s√£o realmente de uma imagem.
                    # Eles ser√£o salvos no mesmo diret√≥rio do script.
                    with open("temp_gemini_image_debug.bin", "wb") as f:
                        f.write(image_data_bytes)
                    print("[DEBUG] Dados brutos da imagem salvos em 'temp_gemini_image_debug.bin' para an√°lise.")

                    # Interrompa o loop assim que encontrar os dados da imagem (assumindo que s√≥ h√° uma)
                    break
                else:
                    print("[DEBUG] part.inline_data.data √© None ou vazio, apesar de inline_data existir.")
            else:
                print("[DEBUG] Parte sem texto ou dados inline relevantes.")

        # Tentar abrir a imagem se os bytes foram encontrados
        if image_data_bytes:
            try:
                image = Image.open(io.BytesIO(image_data_bytes))
                st.session_state.generated_image = image

                # Se o Gemini forneceu algum texto al√©m da imagem, adicione-o ao chat history
                # junto com a imagem, se for relevante.
                # (Ajuste onde isso √© adicionado ao chat history se o seu fluxo for diferente)
                if text_feedback_from_gemini.strip():
                     st.session_state.messages.append({"role": "assistant", "image": image, "content": text_feedback_from_gemini.strip()})
                else:
                    st.session_state.messages.append({"role": "assistant", "image": image, "content": f"Imagem gerada com o prompt: {prompt}"})

                st.session_state.generated_image = None # Limpa para n√£o exibir em cima
                return image # Retorna a imagem e encerra a fun√ß√£o
            except Exception as img_err:
                print(f"[DEBUG] ERRO da Pillow ao abrir imagem: {img_err}. Provavelmente dados de imagem corrompidos/inv√°lidos.")
                st.error(f"Erro interno ao processar a imagem gerada: {img_err}. A imagem recebida pode estar corrompida.")
                return None
        else:
            # Se n√£o encontrou dados de imagem
            msg = "O Gemini n√£o retornou uma imagem v√°lida para o prompt fornecido."
            if text_feedback_from_gemini.strip():
                msg += f" Feedback da IA: {text_feedback_from_gemini.strip()}"
            st.warning(msg)
            print(f"[DEBUG] {msg}")
            return None

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # Este √© o erro que voc√™ est√° vendo
        print(f"[DEBUG] Exce√ß√£o principal capturada em generate_image: {e}") # Log mais detalhado no terminal
        return None
2025-06-28 14:17:20,356 [INFO] Resposta da IA (texto): Ah, a temida "tela preta" no Streamlit! Isso √© um sintoma **completamente diferente** do erro de imagem e geralmente indica um problema na forma como os componentes do Streamlit est√£o sendo chamados, especificamente em rela√ß√£o √† ordem de execu√ß√£o ou a blocos de conte√∫do.

O erro `cannot identify image file` ainda pode estar acontecendo em segundo plano, mas a tela preta significa que a aplica√ß√£o Streamlit est√° falhando *antes* de renderizar a interface corretamente, geralmente porque voc√™ est√° tentando escrever algo em uma √°rea "n√£o permitida" depois que outros componentes j√° foram renderizados.

### A Causa Prov√°vel da Tela Preta:

No Streamlit, componentes como `st.chat_input()` e `st.chat_message()` t√™m um comportamento especial de posicionamento na interface. Uma vez que voc√™ usa `st.chat_input()`, o Streamlit "assume" que tudo que vem depois na execu√ß√£o do script ser√° exibido *abaixo* do campo de entrada de chat, ou dentro de `st.chat_message` blocks.

Quando voc√™ adicionou os `st.info`, `st.warning`, e `st.error` dentro da fun√ß√£o `generate_image`, voc√™ est√° chamando comandos `st.` **fora de um bloco `st.chat_message` ou `st.sidebar`**, e possivelmente *depois* que o `st.chat_input` j√° foi processado na execu√ß√£o anterior. Isso confunde o Streamlit e causa um erro de layout que ele resolve com uma tela em branco ou uma mensagem de erro no console sobre "calling a Streamlit command in a non-allowed content block".

**O que fazer:**

Os `st.info`, `st.warning` e `st.error` que voc√™ adicionou dentro de `generate_image` devem ser apenas `print()` statements para depura√ß√£o no terminal, ou devem ser adicionados ao `st.session_state.messages` para serem exibidos no chat.

### Solu√ß√£o: Remova `st.info`, `st.warning`, `st.error` de dentro de `generate_image` (ou coloque-os no hist√≥rico de mensagens)

Vamos ajustar a fun√ß√£o `generate_image` novamente, removendo os `st.` calls que causam o problema e colocando as mensagens de aviso e erro **dentro do hist√≥rico de mensagens do chat**, que √© o lugar correto para mensagens ao usu√°rio nesse contexto.

```python
# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    api_key = os.getenv("API_KEY_GEMINI")
    if not api_key:
        print("[DEBUG] API_KEY_GEMINI n√£o encontrada no arquivo .env")
        st.session_state.messages.append({"role": "assistant", "content": "‚ùå Erro: API_KEY_GEMINI n√£o encontrada para gerar imagem."})
        return None

    client = genai.Client(api_key=api_key)

    try:
        print(f"\n[DEBUG] Chamando API do Gemini para prompt: '{prompt}'") # Log para o terminal

        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        print(f"[DEBUG] Resposta completa do Gemini recebida. Tipo do objeto: {type(response)}")

        # Verificar feedback de bloqueio
        if response.prompt_feedback and response.prompt_feedback.block_reason:
            print(f"[DEBUG] Raz√£o de Bloqueio da API: {response.prompt_feedback.block_reason}")
            st.session_state.messages.append({"role": "assistant", "content": f"‚ö†Ô∏è Gera√ß√£o de imagem bloqueada pelo Gemini: {response.prompt_feedback.block_reason}"})
            return None

        if not response.candidates:
            print("[DEBUG] Nenhuma candidate encontrada na resposta do Gemini.")
            st.session_state.messages.append({"role": "assistant", "content": "‚ùå Erro: A IA n√£o retornou candidatos de gera√ß√£o. Tente um prompt diferente."})
            return None

        # Vari√°veis para armazenar texto e dados da imagem
        text_feedback_from_gemini = ""
        image_data_bytes = None

        # Itera sobre todas as partes da resposta
        for part in response.candidates[0].content.parts:
            print(f"[DEBUG] Processando parte. Tipo do objeto 'part': {type(part)}")

            # Se a parte for texto
            if hasattr(part, 'text') and part.text is not None:
                text_feedback_from_gemini += part.text + "\n"
                print(f"[DEBUG] Conte√∫do de texto recebido: '{part.text}'")

            # Se a parte for dados inline (imagem)
            if hasattr(part, 'inline_data') and part.inline_data is not None:
                print(f"[DEBUG] Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                if part.inline_data.data:
                    image_data_bytes = part.inline_data.data
                    print(f"[DEBUG] Tamanho dos dados da imagem: {len(image_data_bytes)} bytes.")

                    # --- NOVO: Salve os bytes brutos para inspe√ß√£o ---
                    # Isso ajudar√° a verificar se os bytes s√£o realmente de uma imagem.
                    # Eles ser√£o salvos no mesmo diret√≥rio do script.
                    with open("temp_gemini_image_debug.bin", "wb") as f:
                        f.write(image_data_bytes)
                    print("[DEBUG] Dados brutos da imagem salvos em 'temp_gemini_image_debug.bin' para an√°lise.")

                    # Interrompa o loop assim que encontrar os dados da imagem (assumindo que s√≥ h√° uma)
                    break
                else:
                    print("[DEBUG] part.inline_data.data √© None ou vazio, apesar de inline_data existir.")
            else:
                print("[DEBUG] Parte sem texto ou dados inline relevantes.")

        # Tentar abrir a imagem se os bytes foram encontrados
        if image_data_bytes:
            try:
                image = Image.open(io.BytesIO(image_data_bytes))
                st.session_state.generated_image = image

                # Adiciona a imagem e o texto (se houver) ao hist√≥rico de mensagens
                content_for_chat = f"Imagem gerada com o prompt: {prompt}"
                if text_feedback_from_gemini.strip():
                    content_for_chat = f"{text_feedback_from_gemini.strip()}\n\n" + content_for_chat

                st.session_state.messages.append({"role": "assistant", "image": image, "content": content_for_chat})

                st.session_state.generated_image = None # Limpa para n√£o exibir em cima
                return image # Retorna a imagem e encerra a fun√ß√£o
            except Exception as img_err:
                print(f"[DEBUG] ERRO da Pillow ao abrir imagem: {img_err}. Provavelmente dados de imagem corrompidos/inv√°lidos.")
                st.session_state.messages.append({"role": "assistant", "content": f"‚ùå Erro interno ao processar a imagem gerada: {img_err}. A imagem recebida pode estar corrompida."})
                return None
        else:
            # Se n√£o encontrou dados de imagem
            msg = "O Gemini n√£o retornou uma imagem v√°lida para o prompt fornecido."
            if text_feedback_from_gemini.strip():
                msg += f" Feedback da IA: {text_feedback_from_gemini.strip()}"
            st.session_state.messages.append({"role": "assistant", "content": f"‚ö†Ô∏è {msg}"})
            print(f"[DEBUG] {msg}")
            return None

    except Exception as e:
        print(f"[DEBUG] Exce√ß√£o principal capturada em generate_image: {e}") # Log mais detalhado no terminal
        st.session_state.messages.append({"role": "assistant", "content": f"‚ùå Erro ao gerar imagem: {e}"})
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            # N√£o use st.rerun() aqui diretamente ap√≥s a chamada da fun√ß√£o
            # A fun√ß√£o generate_image agora adiciona mensagens ao hist√≥rico
            generate_image(image_prompt)
            st.rerun() # Force um rerun para atualizar o chat com a nova mensagem/imagem
        else:
            # Mensagem de aviso para o usu√°rio no chat, n√£o na sidebar
            st.session_state.messages.append({"role": "assistant", "content": "‚ö†Ô∏è Por favor, digite um prompt para gerar a imagem."})
            st.rerun()

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.sidebar.success("Imagem colada!") # Este pode ficar na sidebar
                st.rerun()
            else:
                st.sidebar.warning("Nada encontrado na √°rea de transfer√™ncia.") # Este pode ficar na sidebar

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.sidebar.success("Imagem carregada!") # Este pode ficar na sidebar

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)
```

**Principais altera√ß√µes na fun√ß√£o `generate_image`:**

1.  Todas as chamadas `st.info()`, `st.warning()`, `st.error()` que estavam dentro da fun√ß√£o foram substitu√≠das por `print()` statements (que aparecer√£o no seu terminal) ou foram modificadas para **adicionar mensagens ao `st.session_state.messages`**. Isso garante que a UI do Streamlit n√£o quebre.
2.  A l√≥gica de adicionar a imagem e o texto de feedback (se houver) ao `st.session_state.messages` foi consolidada e ocorre apenas **uma vez**, se a imagem for gerada com sucesso ou se houver um feedback relevante do Gemini.
3.  Quando `generate_image` √© chamado pelo bot√£o "Gerar Imagem" na sidebar, um `st.rerun()` √© chamado *ap√≥s* a fun√ß√£o, para garantir que as novas mensagens (com ou sem imagem) adicionadas ao `st.session_state.messages` sejam exibidas. O `st.session_state.generated_image = None` ap√≥s adicionar ao `messages` tamb√©m ajuda a evitar a exibi√ß√£o duplicada se houvesse um `st.image` direto na p√°gina principal usando essa vari√°vel.

Com essas altera√ß√µes, a "tela preta" deve desaparecer, e os erros ou feedbacks da gera√ß√£o de imagem aparecer√£o no chat ou no seu terminal, permitindo continuar a depura√ß√£o do `cannot identify image file`.

Lembre-se de salvar o arquivo e rodar `streamlit run chat_streamlit.py` novamente. Verifique o terminal para os `[DEBUG]` logs e, se o arquivo `temp_gemini_image_debug.bin` for criado, tente inspecion√°-lo para ver o conte√∫do.
2025-06-28 14:17:20,630 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:17:20,631 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:17:20,632 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:17:20,634 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:17:20,635 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:17:20,636 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:17:20,639 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:17:20,641 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:17:20,642 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:17:20,643 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:17:20,645 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:17:20,646 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:17:20,647 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:17:20,648 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:17:20,649 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:28:41,854 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:28:41,856 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:28:41,857 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:28:41,858 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:28:41,859 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:28:41,860 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:28:41,861 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:28:41,862 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:28:41,864 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:28:41,865 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:28:41,867 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:28:41,869 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:28:41,870 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:28:41,871 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:28:41,872 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:28:43,705 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 14:29:12,675 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:29:12,680 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:29:12,681 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:29:12,682 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:29:12,684 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:29:12,693 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:29:12,697 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:29:12,795 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:29:12,841 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:29:12,885 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:29:12,903 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:29:12,929 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:29:12,962 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:29:12,962 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:29:12,963 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:29:12,964 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:29:12,965 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:29:12,965 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:29:12,966 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:29:12,968 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:29:12,968 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:29:13,109 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:29:13,110 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:29:13,111 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:29:13,112 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:29:13,113 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:29:13,113 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:29:13,114 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:29:13,115 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:29:13,116 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:29:14,300 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:29:17,964 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 14:29:19,043 [INFO] AFC remote call 1 is done.
2025-06-28 14:39:50,249 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:39:50,261 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:39:50,266 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:39:50,269 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:39:50,273 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:39:50,274 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:39:50,275 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:39:50,285 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:39:50,326 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:39:50,332 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:39:50,376 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:39:50,404 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:39:50,407 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:39:50,410 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:39:50,418 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:39:51,690 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 14:40:07,572 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:40:07,579 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:40:07,624 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:40:07,656 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:40:07,720 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:40:07,727 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:40:07,742 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:40:07,786 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:40:07,789 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:40:07,790 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:40:07,791 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:40:07,792 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:40:07,793 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:40:07,797 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:40:07,872 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:40:07,916 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:40:07,923 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:40:07,933 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:40:07,946 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:40:07,951 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:40:07,952 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:40:07,973 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:40:08,071 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:40:08,122 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:40:08,138 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:40:08,577 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:40:08,685 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:40:08,726 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:40:08,774 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:40:08,835 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:40:12,021 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:40:15,750 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 14:40:16,789 [INFO] AFC remote call 1 is done.
2025-06-28 14:40:17,147 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:40:17,168 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:40:17,187 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:40:17,238 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:40:17,265 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:40:17,318 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:40:17,382 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:40:17,459 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:40:17,521 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:40:17,533 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:40:17,616 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:40:17,678 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:40:17,699 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:40:17,761 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:40:17,783 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:44:12,870 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:44:12,895 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:44:12,929 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:44:12,967 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:44:12,976 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:44:12,984 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:44:12,992 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:44:12,995 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:44:12,997 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:44:13,000 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:44:13,002 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:44:13,009 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:44:13,084 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:44:13,132 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:44:13,134 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:44:13,144 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:44:13,146 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:44:13,146 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:44:13,147 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:44:13,148 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:44:13,149 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:44:13,149 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:44:13,150 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:44:13,151 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:44:13,152 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:44:13,274 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:44:13,276 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:44:13,277 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:44:13,277 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:44:13,278 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:44:15,204 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:44:17,924 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 14:44:18,917 [INFO] AFC remote call 1 is done.
2025-06-28 14:44:19,039 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:44:19,040 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:44:19,041 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:44:19,043 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:44:19,044 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:44:19,046 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:44:19,048 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:44:19,051 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:44:19,054 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:44:19,056 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:44:19,057 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:44:19,067 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:44:19,071 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:44:19,097 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:44:19,106 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:44:53,192 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:44:53,194 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:44:53,195 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:44:53,197 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:44:53,199 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:44:53,202 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:44:53,204 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:44:53,207 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:44:53,209 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:44:53,210 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:44:53,212 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:44:53,222 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:44:53,224 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:44:53,225 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:44:53,228 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:44:53,451 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:44:53,456 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:44:53,457 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:44:53,458 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:44:53,459 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:44:53,460 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:44:53,461 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:44:53,463 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:44:53,465 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:44:53,467 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:44:53,469 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:44:53,471 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:44:53,472 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:44:53,474 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:44:53,480 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:44:54,969 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:44:57,697 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 14:44:58,718 [INFO] AFC remote call 1 is done.
2025-06-28 14:44:58,825 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:44:58,827 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:44:58,827 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:44:58,828 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:44:58,829 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:44:58,830 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:44:58,832 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:44:58,833 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:44:58,834 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:44:58,836 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:44:58,838 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:44:58,840 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:44:58,841 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:44:58,842 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:44:58,843 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:45:19,220 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:45:19,224 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:45:19,225 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:45:19,226 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:45:19,227 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:45:19,230 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:45:19,233 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:45:19,236 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:45:19,238 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:45:19,241 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:45:19,247 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:45:19,250 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:45:19,253 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:45:19,298 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:45:19,338 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:45:19,714 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:45:19,716 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:45:19,717 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:45:19,718 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:45:19,719 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:45:19,720 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:45:19,724 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:45:19,725 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:45:19,727 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:45:19,728 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:45:19,729 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:45:19,732 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:45:19,735 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:45:19,738 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:45:19,739 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:45:21,333 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:45:26,440 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 14:45:27,483 [INFO] AFC remote call 1 is done.
2025-06-28 14:45:27,581 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:45:27,585 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:45:27,586 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:45:27,587 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:45:27,589 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:45:27,591 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:45:27,594 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:45:27,595 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:45:27,596 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:45:27,598 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:45:27,600 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:45:27,602 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:45:27,607 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:45:27,610 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:45:27,612 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:45:43,144 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:45:43,241 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:45:43,308 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:45:43,327 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:45:43,333 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:45:43,378 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:45:43,396 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:45:43,398 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:45:43,415 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:45:43,425 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:45:43,431 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:45:43,439 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:45:43,442 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:45:43,444 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:45:43,448 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:46:36,054 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:46:36,057 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:46:36,059 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:46:36,060 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:46:36,063 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:46:36,070 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:46:36,075 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:46:36,078 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:46:36,080 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:46:36,088 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:46:36,095 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:46:36,120 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:46:36,129 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:46:36,137 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:46:36,145 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:46:36,515 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:46:36,520 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:46:36,525 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:46:36,539 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:46:36,556 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:46:36,569 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:46:36,577 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:46:36,595 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:46:36,606 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:46:36,645 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:46:36,658 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:46:36,677 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:46:36,692 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:46:36,696 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:46:36,725 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:46:57,239 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:46:57,241 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:46:57,242 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:46:57,243 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:46:57,244 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:46:57,245 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:46:57,250 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:46:57,252 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:46:57,254 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:46:57,267 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:46:57,274 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:46:57,275 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:46:57,277 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:46:57,283 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:46:57,285 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:46:57,621 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:46:57,622 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:46:57,624 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:46:57,625 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:46:57,627 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:46:57,628 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:46:57,630 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:46:57,631 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:46:57,632 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:46:57,634 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:46:57,636 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:46:57,637 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:46:57,639 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:46:57,646 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:46:57,651 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:46:57,720 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628144657_clipboard_20250628144636.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

Assistant: Imagem gerada com o prompt: gerar imagem de um gatinho surfista comendo lasanha
Assistant: Imagem gerada com o prompt: tubaragerar imagem de tubar√£o com Nike no p√© 
Assistant: Imagem gerada com o prompt: gerar imagem de um tubar√£o com um tenis nike na barbatana
Assistant: Imagem gerada com o prompt: gerar imagem de um tubar√£o com um tenis nike nas nadadeiras
User: me ajude a melhorar essa hist√≥ria do print.
2025-06-28 14:47:25,738 [INFO] Resposta da IA (imagem): Ol√°! A sua hist√≥ria j√° tem um √≥timo come√ßo, com um mist√©rio intrigante, tens√£o crescente e personagens com rea√ß√µes bem definidas. O final √© um excelente gancho.

Vamos a algumas sugest√µes para aprimorar ainda mais, focando em completar as frases cortadas e adicionar um pouco mais de detalhe e impacto:

---

**Vers√£o Original (com as suas frases cortadas marcadas):**

Eu leio. Se voc√™s quiserem.
Enzo se aproximou, ficando ao lado dela. Stella e √çsis sentaram no sof√°, Isaac no ch√£o, e Evelyn... ficou encostada no batente da porta, **o...**
A primeira p√°gina era de uma menina chamada Clara, datada de 1997.
‚ÄúHoje ele disse que me amava. Mas era segredo. Tudo era segredo. Eu, ele, a caixa, a sala proibida...‚Äù
Os olhares se cruzaram.
‚ÄúA gente achava que era s√≥ um jogo. Mas depois que achamos a caixa, as brigas come√ßaram. Gritos. Acusa√ß√µes. Ci√∫mes. O grupo se partiu... **e...**‚Äù
‚Äî Isso... √© real? ‚Äî sussurrou √çsis.
‚ÄúA casa n√£o guarda segredos. Ela mostra. Amplifica. Tudo que a gente esconde, ela transforma em ru√≠do, em sombra, em perda.‚Äù
Stella se arrepiou. Evelyn pareceu empalidecer. Giovanna fechou o di√°rio, a voz tr√™mula.
‚Äî Tem mais de cem p√°ginas...
Isaac se levantou devagar.
‚Äî A gente n√£o pode deixar isso acontecer com a gente.
‚Äî Mas j√° t√° acontecendo ‚Äî respondeu Evelyn, num fio de voz.
Um sil√™ncio tomou conta da sala. E o medo n√£o era mais do que tinha acontecido com aquelas crian√ßas. Era o medo de que aquilo j√° estivesse **...**
Cap√≠tulo 14 ‚Äì No Escuro, a Verdade Aparece
Naquela madrugada, uma queda de energia deixou a casa √†s escuras.

---

**Sugest√µes de Melhoria (com explica√ß√µes):**

1.  **Completar as frases cortadas:**

    *   "...Evelyn... ficou encostada no batente da porta, **o olhar fixo no vazio.**" (Adiciona um detalhe visual √† postura dela, que se encaixa com o empalidecimento posterior).
    *   "...O grupo se partiu... **e nunca mais foi o mesmo.**" (Um final mais impactante para a cita√ß√£o, sugerindo uma ruptura permanente). Ou "e **o pesadelo come√ßou.**"
    *   "...Era o medo de que aquilo j√° estivesse **acontecendo com eles, se infiltrando nas suas pr√≥prias vidas.**" (Torna o medo mais pessoal e imediato para os personagens presentes).

2.  **Adicionar pequenos detalhes e intensificar as rea√ß√µes:**

    *   **In√≠cio:** Refor√ßar quem est√° lendo e a atmosfera.
    *   **Rea√ß√µes:** Aprofundar as rea√ß√µes dos personagens ao que √© lido.

---

**Vers√£o Aprimorada (com as sugest√µes incorporadas):**

"Eu leio. Se voc√™s quiserem." Giovanna ofereceu, o velho di√°rio de capa gasta em suas m√£os.
Enzo se aproximou, ficando ao lado dela. Stella e √çsis se acomodaram no sof√°, Isaac no ch√£o, e Evelyn, com um pressentimento que a gelava, ficou encostada no batente da porta, **o olhar fixo no vazio.**
A primeira p√°gina, amarelada pelo tempo, era de uma menina chamada Clara, datada de 1997. Giovanna respirou fundo e come√ßou a ler:
‚ÄúHoje ele disse que me amava. Mas era segredo. Tudo era segredo. Eu, ele, a caixa, a sala proibida...‚Äù
Os olhares do grupo se cruzaram, uma mistura de fasc√≠nio e crescente apreens√£o.
Giovanna continuou, a voz um pouco mais baixa: ‚ÄúA gente achava que era s√≥ um jogo. Mas depois que achamos a caixa, as brigas come√ßaram. Gritos. Acusa√ß√µes. Ci√∫mes. O grupo se partiu... **e nunca mais foi o mesmo.**‚Äù
‚Äî Isso... √© real? ‚Äî sussurrou √çsis, os olhos arregalados para o di√°rio.
Giovanna engoliu em seco antes de ler a pr√≥xima linha, quase como um eco sombrio: ‚ÄúA casa n√£o guarda segredos. Ela mostra. Amplifica. Tudo que a gente esconde, ela transforma em ru√≠do, em sombra, em perda.‚Äù
Stella se arrepiou visivelmente. Evelyn pareceu empalidecer ainda mais, e a m√£o que segurava a porta apertou a madeira. Giovanna fechou o di√°rio com um baque surdo, a voz tr√™mula mal saindo.
‚Äî Tem mais de cem p√°ginas... ‚Äî ela murmurou, como se o peso daquelas palavras a estivesse esmagando.
Isaac se levantou devagar, seus punhos cerrados, o rosto s√©rio.
‚Äî A gente n√£o pode deixar isso acontecer com a gente ‚Äî ele declarou, a voz mais firme do que parecia.
‚Äî Mas j√° t√° acontecendo ‚Äî respondeu Evelyn, num fio de voz que mal era aud√≠vel.
Um sil√™ncio pesado tomou conta da sala. E o medo n√£o era mais sobre o que tinha acontecido com aquelas crian√ßas no passado. Era o medo paralisante de que aquilo j√° estivesse **acontecendo com eles, se infiltrando nas suas pr√≥prias vidas.**

Cap√≠tulo 14 ‚Äì No Escuro, a Verdade Aparece
Naquela madrugada, uma queda de energia deixou a casa √†s escuras.

---

**Pontos fortes que voc√™ j√° tem e pode explorar:**

*   **O Di√°rio como Revelador:** √â um √≥timo dispositivo para desvendar o mist√©rio.
*   **A "Casa":** A casa √© quase um personagem por si s√≥, com a frase "A casa n√£o guarda segredos..." Isso √© excelente e pode ser ainda mais explorado.
*   **Tens√£o Crecente:** Voc√™ constr√≥i bem a tens√£o atrav√©s das rea√ß√µes dos personagens e das revela√ß√µes do di√°rio.
*   **Mist√©rio da "Caixa" e da "Sala Proibida":** Esses elementos s√£o instigantes e d√£o margem para futuras explora√ß√µes.

Espero que essas sugest√µes ajudem a tornar sua hist√≥ria ainda mais envolvente!
2025-06-28 14:47:25,889 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:47:25,890 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:47:25,892 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:47:25,893 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:47:25,894 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:47:25,895 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:47:25,897 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:47:25,899 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:47:25,904 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:47:25,908 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:47:25,909 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:47:25,910 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:47:25,912 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:47:25,913 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:47:25,916 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 15:59:44,779 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 15:59:44,781 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 15:59:44,785 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 15:59:44,789 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 15:59:44,790 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 15:59:44,791 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 15:59:44,792 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 15:59:44,795 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 15:59:44,796 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 15:59:44,803 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 15:59:44,809 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 15:59:44,813 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 15:59:44,818 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 15:59:44,827 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 15:59:44,837 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 15:59:45,362 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 15:59:45,363 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 15:59:45,364 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 15:59:45,366 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 15:59:45,367 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 15:59:45,369 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 15:59:45,372 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 15:59:45,374 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 15:59:45,376 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 15:59:45,381 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 15:59:45,391 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 15:59:45,394 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 15:59:45,396 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 15:59:45,397 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 15:59:45,398 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 15:59:45,472 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628155945_clipboard_20250628144636.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

Assistant: Imagem gerada com o prompt: gerar imagem de um gatinho surfista comendo lasanha
Assistant: Imagem gerada com o prompt: tubaragerar imagem de tubar√£o com Nike no p√© 
Assistant: Imagem gerada com o prompt: gerar imagem de um tubar√£o com um tenis nike na barbatana
Assistant: Imagem gerada com o prompt: gerar imagem de um tubar√£o com um tenis nike nas nadadeiras
User: me ajude a melhorar essa hist√≥ria do print.
Assistant: Ol√°! A sua hist√≥ria j√° tem um √≥timo come√ßo, com um mist√©rio intrigante, tens√£o crescente e personagens com rea√ß√µes bem definidas. O final √© um excelente gancho.

Vamos a algumas sugest√µes para aprimorar ainda mais, focando em completar as frases cortadas e adicionar um pouco mais de detalhe e impacto:

---

**Vers√£o Original (com as suas frases cortadas marcadas):**

Eu leio. Se voc√™s quiserem.
Enzo se aproximou, ficando ao lado dela. Stella e √çsis sentaram no sof√°, Isaac no ch√£o, e Evelyn... ficou encostada no batente da porta, **o...**
A primeira p√°gina era de uma menina chamada Clara, datada de 1997.
‚ÄúHoje ele disse que me amava. Mas era segredo. Tudo era segredo. Eu, ele, a caixa, a sala proibida...‚Äù
Os olhares se cruzaram.
‚ÄúA gente achava que era s√≥ um jogo. Mas depois que achamos a caixa, as brigas come√ßaram. Gritos. Acusa√ß√µes. Ci√∫mes. O grupo se partiu... **e...**‚Äù
‚Äî Isso... √© real? ‚Äî sussurrou √çsis.
‚ÄúA casa n√£o guarda segredos. Ela mostra. Amplifica. Tudo que a gente esconde, ela transforma em ru√≠do, em sombra, em perda.‚Äù
Stella se arrepiou. Evelyn pareceu empalidecer. Giovanna fechou o di√°rio, a voz tr√™mula.
‚Äî Tem mais de cem p√°ginas...
Isaac se levantou devagar.
‚Äî A gente n√£o pode deixar isso acontecer com a gente.
‚Äî Mas j√° t√° acontecendo ‚Äî respondeu Evelyn, num fio de voz.
Um sil√™ncio tomou conta da sala. E o medo n√£o era mais do que tinha acontecido com aquelas crian√ßas. Era o medo de que aquilo j√° estivesse **...**
Cap√≠tulo 14 ‚Äì No Escuro, a Verdade Aparece
Naquela madrugada, uma queda de energia deixou a casa √†s escuras.

---

**Sugest√µes de Melhoria (com explica√ß√µes):**

1.  **Completar as frases cortadas:**

    *   "...Evelyn... ficou encostada no batente da porta, **o olhar fixo no vazio.**" (Adiciona um detalhe visual √† postura dela, que se encaixa com o empalidecimento posterior).
    *   "...O grupo se partiu... **e nunca mais foi o mesmo.**" (Um final mais impactante para a cita√ß√£o, sugerindo uma ruptura permanente). Ou "e **o pesadelo come√ßou.**"
    *   "...Era o medo de que aquilo j√° estivesse **acontecendo com eles, se infiltrando nas suas pr√≥prias vidas.**" (Torna o medo mais pessoal e imediato para os personagens presentes).

2.  **Adicionar pequenos detalhes e intensificar as rea√ß√µes:**

    *   **In√≠cio:** Refor√ßar quem est√° lendo e a atmosfera.
    *   **Rea√ß√µes:** Aprofundar as rea√ß√µes dos personagens ao que √© lido.

---

**Vers√£o Aprimorada (com as sugest√µes incorporadas):**

"Eu leio. Se voc√™s quiserem." Giovanna ofereceu, o velho di√°rio de capa gasta em suas m√£os.
Enzo se aproximou, ficando ao lado dela. Stella e √çsis se acomodaram no sof√°, Isaac no ch√£o, e Evelyn, com um pressentimento que a gelava, ficou encostada no batente da porta, **o olhar fixo no vazio.**
A primeira p√°gina, amarelada pelo tempo, era de uma menina chamada Clara, datada de 1997. Giovanna respirou fundo e come√ßou a ler:
‚ÄúHoje ele disse que me amava. Mas era segredo. Tudo era segredo. Eu, ele, a caixa, a sala proibida...‚Äù
Os olhares do grupo se cruzaram, uma mistura de fasc√≠nio e crescente apreens√£o.
Giovanna continuou, a voz um pouco mais baixa: ‚ÄúA gente achava que era s√≥ um jogo. Mas depois que achamos a caixa, as brigas come√ßaram. Gritos. Acusa√ß√µes. Ci√∫mes. O grupo se partiu... **e nunca mais foi o mesmo.**‚Äù
‚Äî Isso... √© real? ‚Äî sussurrou √çsis, os olhos arregalados para o di√°rio.
Giovanna engoliu em seco antes de ler a pr√≥xima linha, quase como um eco sombrio: ‚ÄúA casa n√£o guarda segredos. Ela mostra. Amplifica. Tudo que a gente esconde, ela transforma em ru√≠do, em sombra, em perda.‚Äù
Stella se arrepiou visivelmente. Evelyn pareceu empalidecer ainda mais, e a m√£o que segurava a porta apertou a madeira. Giovanna fechou o di√°rio com um baque surdo, a voz tr√™mula mal saindo.
‚Äî Tem mais de cem p√°ginas... ‚Äî ela murmurou, como se o peso daquelas palavras a estivesse esmagando.
Isaac se levantou devagar, seus punhos cerrados, o rosto s√©rio.
‚Äî A gente n√£o pode deixar isso acontecer com a gente ‚Äî ele declarou, a voz mais firme do que parecia.
‚Äî Mas j√° t√° acontecendo ‚Äî respondeu Evelyn, num fio de voz que mal era aud√≠vel.
Um sil√™ncio pesado tomou conta da sala. E o medo n√£o era mais sobre o que tinha acontecido com aquelas crian√ßas no passado. Era o medo paralisante de que aquilo j√° estivesse **acontecendo com eles, se infiltrando nas suas pr√≥prias vidas.**

Cap√≠tulo 14 ‚Äì No Escuro, a Verdade Aparece
Naquela madrugada, uma queda de energia deixou a casa √†s escuras.

---

**Pontos fortes que voc√™ j√° tem e pode explorar:**

*   **O Di√°rio como Revelador:** √â um √≥timo dispositivo para desvendar o mist√©rio.
*   **A "Casa":** A casa √© quase um personagem por si s√≥, com a frase "A casa n√£o guarda segredos..." Isso √© excelente e pode ser ainda mais explorado.
*   **Tens√£o Crecente:** Voc√™ constr√≥i bem a tens√£o atrav√©s das rea√ß√µes dos personagens e das revela√ß√µes do di√°rio.
*   **Mist√©rio da "Caixa" e da "Sala Proibida":** Esses elementos s√£o instigantes e d√£o margem para futuras explora√ß√µes.

Espero que essas sugest√µes ajudem a tornar sua hist√≥ria ainda mais envolvente!
User: Em que lugar dessa aplica√ß√£o eu defino os modelos de IA? 

gemini-2.0-flash-exp

esse por exemplo
2025-06-28 16:00:03,846 [INFO] Resposta da IA (imagem): Voc√™ define os modelos de IA em **v√°rios lugares**, dependendo da funcionalidade espec√≠fica que voc√™ quer que o modelo realize.

A classe `GenerativeModelHandler` (definida em `services/gpt_services.py`) √© a respons√°vel por *inicializar* o modelo `genai.GenerativeModel` com o nome do modelo que lhe √© passado. No entanto, o **nome exato do modelo (a string como "gemini-2.5-flash" ou "gemini-2.0-flash-exp-image-generation") √© definido onde essa classe √© instanciada ou onde a fun√ß√£o de gera√ß√£o de conte√∫do √© chamada diretamente.**

Vamos detalhar:

1.  **Para o chat principal (intera√ß√£o de texto e an√°lise de imagem carregada):**
    *   No arquivo `chat_streamlit.py`, dentro da fun√ß√£o `@st.cache_resource def get_gemini_handler():`, voc√™ define o modelo para a intera√ß√£o geral do chat:
        ```python
        # chat_streamlit.py
        @st.cache_resource
        def get_gemini_handler():
            return GeminiHandler("gemini-2.5-flash") # <--- AQUI
        ```
        Este `GeminiHandler` por sua vez usa `GenerativeModelHandler("gemini-2.5-flash")`.

2.  **Para a gera√ß√£o de imagens (via prompt de texto no chat UI):**
    *   No arquivo `chat_streamlit.py`, dentro da fun√ß√£o `generate_image`:
        ```python
        # chat_streamlit.py
        def generate_image(prompt):
            # ...
            client = genai.Client(api_key=api_key)
            try:
                response = client.models.generate_content(
                    model='gemini-2.0-flash-exp-image-generation', # <--- AQUI
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_modalities=['Text', 'Image']
                    )
                )
        ```

3.  **Para o processamento de imagens em lote (no `main.py`):**
    *   No arquivo `services/image_processor.py`, no `__init__` da classe `ImageProcessor`:
        ```python
        # services/image_processor.py
        class ImageProcessor:
            def __init__(self, rate_limiter: RateLimiter):
                self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash") # <--- AQUI
                # ...
        ```
        Este √© o modelo usado para gerar os resumos das imagens na execu√ß√£o via `main.py`.

4.  **No servi√ßo de imagem aut√¥nomo (n√£o diretamente usado no chat, mas existe):**
    *   No arquivo `services/image_services.py`, na fun√ß√£o `generate_image`:
        ```python
        # services/image_services.py
        def generate_image(prompt: str) -> Image.Image | None:
            try:
                model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation') # <--- AQUI
                # ...
        ```
        Note que a fun√ß√£o `generate_image` em `chat_streamlit.py` chama diretamente a API do `genai`, e n√£o esta fun√ß√£o `services/image_services.py`. No entanto, esta √© outra inst√¢ncia onde um modelo de gera√ß√£o de imagem √© especificado.

**Em resumo:**

*   **`gemini-2.5-flash`** √© usado para o chat interativo no Streamlit (`chat_streamlit.py`) e para o processamento em lote de imagens (`services/image_processor.py`).
*   **`gemini-2.0-flash-exp-image-generation`** √© usado especificamente para a funcionalidade de "Gerar Imagem" dentro do Streamlit (`chat_streamlit.py`) e tamb√©m existe como um servi√ßo aut√¥nomo (`services/image_services.py`).
2025-06-28 16:00:03,988 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:00:03,989 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:00:03,990 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:00:03,991 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:00:03,992 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:00:03,993 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:00:03,994 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:00:03,996 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:00:03,997 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:00:04,003 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:00:04,005 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:00:04,007 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:00:04,010 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:00:04,013 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:00:04,015 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:00:13,328 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:00:13,332 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:00:13,333 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:00:13,335 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:00:13,340 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:00:13,353 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:00:13,357 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:00:13,360 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:00:13,362 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:00:13,364 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:00:13,365 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:00:13,367 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:00:13,384 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:00:13,387 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:00:13,390 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:00:13,620 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:00:13,626 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:00:13,628 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:00:13,633 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:00:13,648 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:00:13,650 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:00:13,651 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:00:13,652 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:00:13,653 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:00:13,662 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:00:13,668 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:00:13,684 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:00:13,686 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:00:13,687 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:00:13,700 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:02:10,145 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:02:10,147 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:02:10,148 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:02:10,149 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:02:10,150 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:02:10,151 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:02:10,153 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:02:10,166 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:02:10,169 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:02:10,171 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:02:10,182 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:02:10,185 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:02:10,186 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:02:10,187 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:02:10,188 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:02:10,669 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:02:10,670 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:02:10,671 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:02:10,673 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:02:10,675 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:02:10,676 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:02:10,678 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:02:10,680 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:02:10,686 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:02:10,687 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:02:10,690 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:02:10,691 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:02:10,695 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:02:10,697 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:02:10,699 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:02:10,702 [INFO] Enviando para IA - Prompt (sem imagem): Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Fun√ß√£o para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta cont√©m dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

Assistant: Imagem gerada com o prompt: gerar imagem de um gatinho surfista comendo lasanha
Assistant: Imagem gerada com o prompt: tubaragerar imagem de tubar√£o com Nike no p√© 
Assistant: Imagem gerada com o prompt: gerar imagem de um tubar√£o com um tenis nike na barbatana
Assistant: Imagem gerada com o prompt: gerar imagem de um tubar√£o com um tenis nike nas nadadeiras
User: me ajude a melhorar essa hist√≥ria do print.
Assistant: Ol√°! A sua hist√≥ria j√° tem um √≥timo come√ßo, com um mist√©rio intrigante, tens√£o crescente e personagens com rea√ß√µes bem definidas. O final √© um excelente gancho.

Vamos a algumas sugest√µes para aprimorar ainda mais, focando em completar as frases cortadas e adicionar um pouco mais de detalhe e impacto:

---

**Vers√£o Original (com as suas frases cortadas marcadas):**

Eu leio. Se voc√™s quiserem.
Enzo se aproximou, ficando ao lado dela. Stella e √çsis sentaram no sof√°, Isaac no ch√£o, e Evelyn... ficou encostada no batente da porta, **o...**
A primeira p√°gina era de uma menina chamada Clara, datada de 1997.
‚ÄúHoje ele disse que me amava. Mas era segredo. Tudo era segredo. Eu, ele, a caixa, a sala proibida...‚Äù
Os olhares se cruzaram.
‚ÄúA gente achava que era s√≥ um jogo. Mas depois que achamos a caixa, as brigas come√ßaram. Gritos. Acusa√ß√µes. Ci√∫mes. O grupo se partiu... **e...**‚Äù
‚Äî Isso... √© real? ‚Äî sussurrou √çsis.
‚ÄúA casa n√£o guarda segredos. Ela mostra. Amplifica. Tudo que a gente esconde, ela transforma em ru√≠do, em sombra, em perda.‚Äù
Stella se arrepiou. Evelyn pareceu empalidecer. Giovanna fechou o di√°rio, a voz tr√™mula.
‚Äî Tem mais de cem p√°ginas...
Isaac se levantou devagar.
‚Äî A gente n√£o pode deixar isso acontecer com a gente.
‚Äî Mas j√° t√° acontecendo ‚Äî respondeu Evelyn, num fio de voz.
Um sil√™ncio tomou conta da sala. E o medo n√£o era mais do que tinha acontecido com aquelas crian√ßas. Era o medo de que aquilo j√° estivesse **...**
Cap√≠tulo 14 ‚Äì No Escuro, a Verdade Aparece
Naquela madrugada, uma queda de energia deixou a casa √†s escuras.

---

**Sugest√µes de Melhoria (com explica√ß√µes):**

1.  **Completar as frases cortadas:**

    *   "...Evelyn... ficou encostada no batente da porta, **o olhar fixo no vazio.**" (Adiciona um detalhe visual √† postura dela, que se encaixa com o empalidecimento posterior).
    *   "...O grupo se partiu... **e nunca mais foi o mesmo.**" (Um final mais impactante para a cita√ß√£o, sugerindo uma ruptura permanente). Ou "e **o pesadelo come√ßou.**"
    *   "...Era o medo de que aquilo j√° estivesse **acontecendo com eles, se infiltrando nas suas pr√≥prias vidas.**" (Torna o medo mais pessoal e imediato para os personagens presentes).

2.  **Adicionar pequenos detalhes e intensificar as rea√ß√µes:**

    *   **In√≠cio:** Refor√ßar quem est√° lendo e a atmosfera.
    *   **Rea√ß√µes:** Aprofundar as rea√ß√µes dos personagens ao que √© lido.

---

**Vers√£o Aprimorada (com as sugest√µes incorporadas):**

"Eu leio. Se voc√™s quiserem." Giovanna ofereceu, o velho di√°rio de capa gasta em suas m√£os.
Enzo se aproximou, ficando ao lado dela. Stella e √çsis se acomodaram no sof√°, Isaac no ch√£o, e Evelyn, com um pressentimento que a gelava, ficou encostada no batente da porta, **o olhar fixo no vazio.**
A primeira p√°gina, amarelada pelo tempo, era de uma menina chamada Clara, datada de 1997. Giovanna respirou fundo e come√ßou a ler:
‚ÄúHoje ele disse que me amava. Mas era segredo. Tudo era segredo. Eu, ele, a caixa, a sala proibida...‚Äù
Os olhares do grupo se cruzaram, uma mistura de fasc√≠nio e crescente apreens√£o.
Giovanna continuou, a voz um pouco mais baixa: ‚ÄúA gente achava que era s√≥ um jogo. Mas depois que achamos a caixa, as brigas come√ßaram. Gritos. Acusa√ß√µes. Ci√∫mes. O grupo se partiu... **e nunca mais foi o mesmo.**‚Äù
‚Äî Isso... √© real? ‚Äî sussurrou √çsis, os olhos arregalados para o di√°rio.
Giovanna engoliu em seco antes de ler a pr√≥xima linha, quase como um eco sombrio: ‚ÄúA casa n√£o guarda segredos. Ela mostra. Amplifica. Tudo que a gente esconde, ela transforma em ru√≠do, em sombra, em perda.‚Äù
Stella se arrepiou visivelmente. Evelyn pareceu empalidecer ainda mais, e a m√£o que segurava a porta apertou a madeira. Giovanna fechou o di√°rio com um baque surdo, a voz tr√™mula mal saindo.
‚Äî Tem mais de cem p√°ginas... ‚Äî ela murmurou, como se o peso daquelas palavras a estivesse esmagando.
Isaac se levantou devagar, seus punhos cerrados, o rosto s√©rio.
‚Äî A gente n√£o pode deixar isso acontecer com a gente ‚Äî ele declarou, a voz mais firme do que parecia.
‚Äî Mas j√° t√° acontecendo ‚Äî respondeu Evelyn, num fio de voz que mal era aud√≠vel.
Um sil√™ncio pesado tomou conta da sala. E o medo n√£o era mais sobre o que tinha acontecido com aquelas crian√ßas no passado. Era o medo paralisante de que aquilo j√° estivesse **acontecendo com eles, se infiltrando nas suas pr√≥prias vidas.**

Cap√≠tulo 14 ‚Äì No Escuro, a Verdade Aparece
Naquela madrugada, uma queda de energia deixou a casa √†s escuras.

---

**Pontos fortes que voc√™ j√° tem e pode explorar:**

*   **O Di√°rio como Revelador:** √â um √≥timo dispositivo para desvendar o mist√©rio.
*   **A "Casa":** A casa √© quase um personagem por si s√≥, com a frase "A casa n√£o guarda segredos..." Isso √© excelente e pode ser ainda mais explorado.
*   **Tens√£o Crecente:** Voc√™ constr√≥i bem a tens√£o atrav√©s das rea√ß√µes dos personagens e das revela√ß√µes do di√°rio.
*   **Mist√©rio da "Caixa" e da "Sala Proibida":** Esses elementos s√£o instigantes e d√£o margem para futuras explora√ß√µes.

Espero que essas sugest√µes ajudem a tornar sua hist√≥ria ainda mais envolvente!
User: Em que lugar dessa aplica√ß√£o eu defino os modelos de IA? 

gemini-2.0-flash-exp

esse por exemplo
Assistant: Voc√™ define os modelos de IA em **v√°rios lugares**, dependendo da funcionalidade espec√≠fica que voc√™ quer que o modelo realize.

A classe `GenerativeModelHandler` (definida em `services/gpt_services.py`) √© a respons√°vel por *inicializar* o modelo `genai.GenerativeModel` com o nome do modelo que lhe √© passado. No entanto, o **nome exato do modelo (a string como "gemini-2.5-flash" ou "gemini-2.0-flash-exp-image-generation") √© definido onde essa classe √© instanciada ou onde a fun√ß√£o de gera√ß√£o de conte√∫do √© chamada diretamente.**

Vamos detalhar:

1.  **Para o chat principal (intera√ß√£o de texto e an√°lise de imagem carregada):**
    *   No arquivo `chat_streamlit.py`, dentro da fun√ß√£o `@st.cache_resource def get_gemini_handler():`, voc√™ define o modelo para a intera√ß√£o geral do chat:
        ```python
        # chat_streamlit.py
        @st.cache_resource
        def get_gemini_handler():
            return GeminiHandler("gemini-2.5-flash") # <--- AQUI
        ```
        Este `GeminiHandler` por sua vez usa `GenerativeModelHandler("gemini-2.5-flash")`.

2.  **Para a gera√ß√£o de imagens (via prompt de texto no chat UI):**
    *   No arquivo `chat_streamlit.py`, dentro da fun√ß√£o `generate_image`:
        ```python
        # chat_streamlit.py
        def generate_image(prompt):
            # ...
            client = genai.Client(api_key=api_key)
            try:
                response = client.models.generate_content(
                    model='gemini-2.0-flash-exp-image-generation', # <--- AQUI
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_modalities=['Text', 'Image']
                    )
                )
        ```

3.  **Para o processamento de imagens em lote (no `main.py`):**
    *   No arquivo `services/image_processor.py`, no `__init__` da classe `ImageProcessor`:
        ```python
        # services/image_processor.py
        class ImageProcessor:
            def __init__(self, rate_limiter: RateLimiter):
                self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash") # <--- AQUI
                # ...
        ```
        Este √© o modelo usado para gerar os resumos das imagens na execu√ß√£o via `main.py`.

4.  **No servi√ßo de imagem aut√¥nomo (n√£o diretamente usado no chat, mas existe):**
    *   No arquivo `services/image_services.py`, na fun√ß√£o `generate_image`:
        ```python
        # services/image_services.py
        def generate_image(prompt: str) -> Image.Image | None:
            try:
                model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation') # <--- AQUI
                # ...
        ```
        Note que a fun√ß√£o `generate_image` em `chat_streamlit.py` chama diretamente a API do `genai`, e n√£o esta fun√ß√£o `services/image_services.py`. No entanto, esta √© outra inst√¢ncia onde um modelo de gera√ß√£o de imagem √© especificado.

**Em resumo:**

*   **`gemini-2.5-flash`** √© usado para o chat interativo no Streamlit (`chat_streamlit.py`) e para o processamento em lote de imagens (`services/image_processor.py`).
*   **`gemini-2.0-flash-exp-image-generation`** √© usado especificamente para a funcionalidade de "Gerar Imagem" dentro do Streamlit (`chat_streamlit.py`) e tamb√©m existe como um servi√ßo aut√¥nomo (`services/image_services.py`).
User: eu tenho fun√ß√µes repetidas nessa aplica√ß√£o que pode ser melhorada para POO ?   por exemplo 

generate_image

tenho em dois lugares essa fun√ß√£o.
2025-06-28 16:02:46,309 [INFO] Resposta da IA (texto): Voc√™ est√° absolutamente certo! A duplica√ß√£o da fun√ß√£o `generate_image` √© uma excelente oportunidade para aplicar os princ√≠pios de POO, especificamente o de **Reutiliza√ß√£o de C√≥digo** e **Single Responsibility Principle (SRP)**, encapsulando a l√≥gica de gera√ß√£o de imagem em uma classe de servi√ßo dedicada.

Atualmente, voc√™ tem a l√≥gica de gera√ß√£o de imagem em:
1.  `services/image_services.py`
2.  `chat_streamlit.py` (e `test_image_generation.py` tamb√©m usa uma l√≥gica similar, que pode ser consolidada).

Vamos refatorar isso criando uma nova classe de servi√ßo que ser√° a √∫nica respons√°vel por interagir com a API de gera√ß√£o de imagens do Gemini.

### 1. Criar uma Nova Classe de Servi√ßo para Gera√ß√£o de Imagens

Vamos criar um novo arquivo, por exemplo, `services/gemini_image_generator.py`, para hospedar essa l√≥gica.

**`services/gemini_image_generator.py`**
```python
import os
import google.generativeai as genai
from google.genai import types
from PIL import Image
from io import BytesIO
from dotenv import load_dotenv
from core.logger_config import logger # Importe seu logger existente

class GeminiImageGenerator:
    _instance = None # Para implementar um singleton simples, garantindo uma √∫nica inst√¢ncia
    _is_initialized = False

    def __new__(cls, *args, **kwargs):
        # Implementa o padr√£o Singleton: garante que apenas uma inst√¢ncia da classe seja criada.
        if cls._instance is None:
            cls._instance = super(GeminiImageGenerator, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not self._is_initialized:
            load_dotenv()
            self.api_key = os.getenv("API_KEY_GEMINI")

            if not self.api_key:
                logger.error("API_KEY_GEMINI n√£o encontrada nas vari√°veis de ambiente para gera√ß√£o de imagens.")
                raise ValueError("API_KEY_GEMINI n√£o encontrada nas vari√°veis de ambiente.")

            # Configura a API do Gemini uma √∫nica vez
            genai.configure(api_key=self.api_key)
            self.client = genai.Client(api_key=self.api_key) # Usar genai.Client para models.generate_content
            self.model_name = 'gemini-2.0-flash-exp-image-generation'
            self._is_initialized = True
            logger.info(f"GeminiImageGenerator inicializado para o modelo {self.model_name}.")

    def generate_image(self, prompt: str) -> Image.Image | None:
        """
        Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

        Args:
            prompt (str): O prompt de texto para gerar a imagem.

        Returns:
            Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
        """
        try:
            logger.info(f"Iniciando gera√ß√£o de imagem para o prompt: '{prompt[:100]}...'")
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_modalities=['Text', 'Image']
                )
            )
            
            if response.prompt_feedback:
                feedback = response.prompt_feedback.block_reason
                logger.warning(f"Feedback do prompt na gera√ß√£o de imagem: {feedback}")
                # Dependendo do feedback, voc√™ pode querer lan√ßar uma exce√ß√£o ou retornar None
                return None

            # Percorre as partes da resposta para encontrar a imagem
            if response.candidates and response.candidates[0].content.parts:
                for part in response.candidates[0].content.parts:
                    if part.inline_data is not None and part.inline_data.mime_type.startswith('image/'):
                        image = Image.open(BytesIO(part.inline_data.data))
                        logger.info(f"Imagem gerada com sucesso para o prompt: '{prompt[:100]}...'")
                        return image
            
            logger.warning(f"Nenhuma imagem encontrada na resposta para o prompt: '{prompt[:100]}...'. Resposta completa: {response.text}")
            return None

        except Exception as e:
            logger.error(f"Erro ao gerar imagem para prompt '{prompt[:100]}...': {e}", exc_info=True)
            return None

```

### 2. Modificar `chat_streamlit.py` para Usar a Nova Classe

Agora, `chat_streamlit.py` pode importar e usar esta nova classe, removendo a l√≥gica duplicada.

**`chat_streamlit.py`** (apenas as partes que mudam)

```python
# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python
from services.gemini_image_generator import GeminiImageGenerator # <--- NOVA IMPORTA√á√ÉO

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter (pode ser compartilhado ou ter seu pr√≥prio para a UI)
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# ... (restante das inicializa√ß√µes de session_state) ...

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# <--- NOVA FUN√á√ÉO PARA INICIALIZAR GeminiImageGenerator
@st.cache_resource
def get_image_generator():
    return GeminiImageGenerator()

image_generator = get_image_generator()
# FIM DA NOVA INICIALIZA√á√ÉO

# ... (fun√ß√µes check_clipboard, reset_uploader, process_message, execute_processing) ...

# Fun√ß√£o para gerar imagem com Gemini (AGORA USA A CLASSE DE SERVI√áO)
def generate_image(prompt):
    try:
        # Chama o m√©todo da nova classe de servi√ßo
        generated_image = image_generator.generate_image(prompt)

        if generated_image:
            st.session_state.generated_image = generated_image
            return generated_image
        else:
            # A classe de servi√ßo j√° loga o erro, aqui √© para a UI
            st.error("N√£o foi poss√≠vel gerar a imagem. Verifique o prompt ou a resposta da API.")
            return None

    except Exception as e:
        st.error(f"Erro inesperado ao gerar imagem: {e}")
        return None

# ... (restante do c√≥digo do Streamlit) ...
```

### 3. Excluir ou Refatorar `services/image_services.py`

A partir de agora, o arquivo `services/image_services.py` se torna redundante, pois sua funcionalidade foi migrada para `services/gemini_image_generator.py`. Voc√™ pode:

*   **Exclu√≠-lo completamente** se ele n√£o for usado em mais nenhum lugar do projeto (que √© o caso aparente).
*   Se houvesse outras fun√ß√µes utilit√°rias que n√£o fossem a `generate_image`, voc√™ poderia refator√°-lo para usar a nova classe `GeminiImageGenerator` ou mover essas fun√ß√µes para um local mais apropriado.

### 4. Modificar `test_image_generation.py` (Opcional, mas recomendado)

Para manter a consist√™ncia, seu script de teste tamb√©m deve usar a nova classe:

**`test_image_generation.py`** (apenas as partes que mudam)

```python
import os
from dotenv import load_dotenv
# from google import genai # N√£o precisa mais disso diretamente aqui
# from google.genai import types # N√£o precisa mais disso diretamente aqui
from PIL import Image
from io import BytesIO

from services.gemini_image_generator import GeminiImageGenerator # <--- NOVA IMPORTA√á√ÉO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Instancia o gerador de imagens
image_generator = GeminiImageGenerator() # A classe j√° cuida da API_KEY e configure

prompt = "gerar a imagem do simbolo do corinthians."

# Chama o m√©todo da classe de servi√ßo
image = image_generator.generate_image(prompt)

if image:
    image.save("ed-image.png")
    image.show()
else:
    print("Falha ao gerar imagem.") # A classe j√° loga detalhes do erro
```

---

### Benef√≠cios da Refatora√ß√£o:

1.  **Reutiliza√ß√£o de C√≥digo:** A l√≥gica de como interagir com a API de gera√ß√£o de imagens do Gemini est√° agora em um √∫nico lugar (`GeminiImageGenerator`). Qualquer parte da sua aplica√ß√£o que precise gerar uma imagem usar√° essa classe, evitando duplica√ß√£o.
2.  **Single Responsibility Principle (SRP):**
    *   `GeminiImageGenerator` tem uma √∫nica responsabilidade: gerar imagens atrav√©s da API do Gemini.
    *   `chat_streamlit.py` √© respons√°vel pela interface do usu√°rio e por orquestrar as intera√ß√µes, n√£o pela l√≥gica interna de como a imagem √© gerada.
3.  **Manutenibilidade:** Se a API de gera√ß√£o de imagens do Gemini mudar (e.g., um novo modelo, novos par√¢metros, autentica√ß√£o), voc√™ s√≥ precisar√° atualizar o c√≥digo em `services/gemini_image_generator.py`.
4.  **Testabilidade:** A classe `GeminiImageGenerator` pode ser testada isoladamente, sem a necessidade de simular o ambiente do Streamlit ou outros componentes da aplica√ß√£o.
5.  **Coes√£o:** O c√≥digo relacionado √† gera√ß√£o de imagens est√° agrupado de forma l√≥gica.
6.  **Legibilidade:** O c√≥digo em `chat_streamlit.py` fica mais limpo e f√°cil de entender, pois a complexidade da API foi abstra√≠da.

Essa √© uma excelente melhoria de arquitetura para a sua aplica√ß√£o!
2025-06-28 16:02:46,541 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:02:46,543 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:02:46,544 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:02:46,545 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:02:46,547 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:02:46,549 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:02:46,562 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:02:46,564 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:02:46,566 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:02:46,568 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:02:46,570 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:02:46,572 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:02:46,579 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:02:46,581 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:02:46,583 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:19:32,810 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:19:32,812 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:19:32,813 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:19:32,819 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:19:32,821 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:19:32,828 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:19:32,834 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:19:32,853 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:19:32,859 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:19:32,862 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:19:32,870 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:19:32,873 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:19:32,875 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:19:32,876 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:19:32,877 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:19:32,878 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:19:33,944 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 16:24:15,805 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:24:15,805 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:24:15,806 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:24:15,807 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:24:15,808 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:24:15,808 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:24:15,809 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:24:15,810 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:24:15,810 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:24:15,811 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:24:15,813 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:24:15,814 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:24:15,820 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:24:15,823 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:24:15,824 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:24:15,825 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:25:22,562 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:25:22,568 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:25:22,581 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:25:22,598 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:25:22,599 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:25:22,600 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:25:22,601 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:25:22,609 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:25:22,611 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:25:22,613 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:25:22,614 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:25:22,618 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:25:22,634 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:25:22,642 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:25:22,644 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:25:22,646 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:26:08,791 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:26:08,800 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:26:08,801 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:26:08,802 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:26:08,803 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:26:08,804 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:26:08,807 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:26:08,813 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:26:08,840 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:26:08,852 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:26:08,854 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:26:08,855 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:26:08,856 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:26:08,856 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:26:08,857 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:26:08,868 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:28:50,988 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:28:50,994 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:28:50,995 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:28:50,996 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:28:50,996 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:28:50,997 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:28:50,998 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:28:51,003 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:28:51,006 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:28:51,007 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:28:51,011 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:28:51,014 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:28:51,020 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:28:51,023 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:28:51,024 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:28:51,026 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:29:12,375 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:29:12,385 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:29:12,386 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:29:12,388 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:29:12,390 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:29:12,392 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:29:12,393 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:29:12,408 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:29:12,426 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:29:12,434 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:29:12,440 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:29:12,441 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:29:12,442 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:29:12,442 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:29:12,443 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:29:12,453 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:29:30,116 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:29:30,117 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:29:30,118 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:29:30,119 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:29:30,121 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:29:30,124 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:29:30,126 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:29:30,128 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:29:30,133 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:29:30,155 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:29:30,158 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:29:30,168 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:29:30,169 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:29:30,176 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:29:30,187 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:29:30,188 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:29:30,221 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 16:30:16,012 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:30:16,013 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:30:16,014 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:30:16,018 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:30:16,019 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:30:16,020 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:30:16,030 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:30:16,039 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:30:16,041 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:30:16,042 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:30:16,043 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:30:16,044 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:30:16,044 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:30:16,047 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:30:16,061 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:30:16,063 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:30:16,672 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 16:36:44,761 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:36:44,762 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:36:44,763 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:36:44,765 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:36:44,769 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:36:44,771 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:36:44,772 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:36:44,776 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:36:44,782 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:36:44,787 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:36:44,791 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:36:44,793 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:36:44,810 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:36:44,811 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:36:44,813 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:36:44,816 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:41:27,764 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:41:27,764 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:41:27,765 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:41:27,766 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:41:27,766 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:41:27,767 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:41:27,767 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:41:27,768 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:41:27,768 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:41:27,769 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:41:27,770 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:41:27,770 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:41:27,771 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:41:27,773 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:41:27,775 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:41:27,776 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:41:27,778 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:41:27,778 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:41:27,779 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:41:27,780 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:41:27,780 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:41:27,781 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:41:27,781 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:41:27,782 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:41:27,783 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:41:27,783 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:41:27,784 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:41:27,784 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:41:27,785 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:41:27,785 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:41:27,786 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:41:27,786 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:41:27,799 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 16:41:50,871 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:41:50,880 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:41:50,881 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:41:50,889 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:41:50,892 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:41:50,896 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:41:50,911 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:41:50,914 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:41:50,915 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:41:50,929 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:41:50,930 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:41:50,931 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:41:50,939 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:41:50,948 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:41:50,955 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:41:50,964 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:41:51,189 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:41:51,192 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:41:51,193 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:41:51,196 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:41:51,197 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:41:51,209 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:41:51,211 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:41:51,212 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:41:51,219 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:41:51,225 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:41:51,228 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:41:51,241 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:41:51,245 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:41:51,246 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:41:51,250 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:41:51,260 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:41:51,295 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 16:41:56,124 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 16:41:57,271 [INFO] AFC remote call 1 is done.
2025-06-28 16:41:57,445 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:41:57,447 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:41:57,448 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:41:57,450 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:41:57,452 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:41:57,456 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:41:57,464 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:41:57,466 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:41:57,467 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:41:57,468 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:41:57,470 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:41:57,472 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:41:57,472 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:41:57,477 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:41:57,483 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:41:57,484 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:44:10,785 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:44:10,786 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:44:10,787 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:44:10,788 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:44:10,788 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:44:10,789 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:44:10,790 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:44:10,791 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:44:10,791 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:44:10,792 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:44:10,793 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:44:10,794 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:44:10,795 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:44:10,795 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:44:10,797 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:44:10,799 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:44:11,235 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 16:44:25,479 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:44:25,484 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:44:25,485 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:44:25,486 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:44:25,488 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:44:25,489 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:44:25,491 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:44:25,493 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:44:25,499 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:44:25,504 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:44:25,506 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:44:25,507 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:44:25,509 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:44:25,510 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:44:25,511 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:44:25,512 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:44:25,690 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:44:25,692 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:44:25,693 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:44:25,694 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:44:25,695 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:44:25,696 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:44:25,698 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:44:25,701 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:44:25,703 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:44:25,705 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:44:25,706 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:44:25,708 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:44:25,709 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:44:25,710 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:44:25,714 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:44:25,718 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:44:25,739 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 16:44:30,135 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 16:44:31,370 [INFO] AFC remote call 1 is done.
2025-06-28 16:44:31,553 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:44:31,554 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:44:31,555 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:44:31,562 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:44:31,567 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:44:31,570 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:44:31,578 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:44:31,587 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:44:31,588 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:44:31,601 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:44:31,605 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:44:31,609 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:44:31,743 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:44:31,749 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:44:31,752 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:44:31,756 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:45:14,005 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:45:14,006 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:45:14,006 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:45:14,007 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:45:14,008 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:45:14,009 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:45:14,009 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:45:14,010 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:45:14,010 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:45:14,011 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:45:14,012 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:45:14,012 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:45:14,013 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:45:14,014 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:45:14,014 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:45:14,016 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:45:14,396 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:45:14,397 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:45:14,399 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:45:14,401 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:45:14,402 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:45:14,403 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:45:14,404 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:45:14,406 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:45:14,407 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:45:14,408 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:45:14,409 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:45:14,412 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:45:14,413 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:45:14,415 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:45:14,419 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:45:14,424 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:45:24,010 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:45:24,011 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:45:24,012 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:45:24,013 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:45:24,014 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:45:24,015 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:45:24,017 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:45:24,019 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:45:24,020 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:45:24,021 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:45:24,023 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:45:24,025 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:45:24,032 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:45:24,033 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:45:24,034 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:45:24,035 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:45:24,254 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:45:24,255 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:45:24,256 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:45:24,257 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:45:24,258 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:45:24,259 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:45:24,261 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:45:24,263 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:45:24,266 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:45:24,268 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:45:24,269 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:45:24,274 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:45:24,276 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:45:24,278 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:45:24,279 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:45:24,280 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:45:24,415 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628164524_clipboard_20250628164514.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python
from services.gemini_image_generator import GeminiImageGenerator

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

@st.cache_resource
def get_image_generator():
    return GeminiImageGenerator()

image_generator = get_image_generator()
# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = image_generator.generate_image(image_prompt)


            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gemini_image_generator.py

from google import genai
from google.genai import types
from PIL import Image
import io
import os

class GeminiImageGenerator:
    def __init__(self):
        api_key = os.getenv("API_KEY_GEMINI")
        if not api_key:
            raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")
        self.client = genai.Client(api_key=api_key)

    def generate_image(self, prompt: str) -> Image.Image | None:
        try:
            response = self.client.models.generate_content(
                model='gemini-2.0-flash-exp-image-generation',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_modalities=['Text', 'Image']
                )
            )
            for part in response.candidates[0].content.parts:
                if part.inline_data is not None:
                    return Image.open(io.BytesIO(part.inline_data.data))
            return None
        except Exception as e:
            print(f"Erro ao gerar imagem: {e}")
            return None


# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

# import os
# from dotenv import load_dotenv
# from google import genai
# from PIL import Image
# from io import BytesIO

# # Carrega as vari√°veis de ambiente do arquivo .env
# load_dotenv()

# # Obt√©m a chave da API Gemini do arquivo .env
# api_key = os.getenv("API_KEY_GEMINI")

# # Verifica se a chave da API foi carregada corretamente
# if not api_key:
#     raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# # Inicializa o Gemini
# genai.configure(api_key=api_key)

# def generate_image(prompt: str) -> Image.Image | None:
#     """
#     Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

#     Args:
#         prompt (str): O prompt de texto para gerar a imagem.

#     Returns:
#         Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
#     """
#     try:
#         model = genai.GenerativeModel('gemini-2.0-flash-preview-image-generation')
#         response = model.generate_content(prompt)
#         if response.prompt_feedback:
#             print('Reason: {}'.format(response.prompt_feedback.block_reason))
#         # Verifique se a resposta cont√©m dados de imagem
#         if response.parts:
#             for part in response.parts:
#                 if part.mime_type == 'image/png':
#                     return Image.open(BytesIO(part.data))
#         print(response.text)
#         return None
#     except Exception as e:
#         print(f"Erro ao gerar imagem: {e}")
#         return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

Assistant: Imagem gerada com o prompt: gere uma imagem aleatoria
User: Que imagem [e esas ?
2025-06-28 16:45:34,099 [INFO] Resposta da IA (imagem): Esta imagem mostra uma criatura fant√°stica e fofa de cor roxa vibrante, com olhos grandes e amarelos que parecem brilhar. Ela tem pelos bem felpudos e orelhas pontudas, grandes e peludas, al√©m de um pequeno focinho rosa. A criatura est√° sentada em um terreno coberto por folhagem densa e azulada, que se assemelha a musgo ou pequenos arbustos.

O ambiente ao redor √© uma floresta m√°gica e escura, com √°rvores de troncos retorcidos e escuros que emolduram a cena. H√° pontos de luz brilhantes e esferas azul-turquesa luminosas espalhadas pelo ar e na folhagem, criando uma atmosfera encantada. Ao fundo, parece haver uma cachoeira ou uma forma√ß√£o rochosa vertical envolta em n√©voa, sob um c√©u que varia de tons de azul a √¢mbar, com mais pontos luminosos.

√â uma imagem que evoca um sentimento de fantasia e mist√©rio, com a criatura parecendo amig√°vel e curiosa. Esta √© a imagem gerada aleatoriamente que voc√™ viu anteriormente.
2025-06-28 16:45:34,193 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:45:34,195 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:45:34,197 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:45:34,198 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:45:34,200 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:45:34,203 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:45:34,206 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:45:34,209 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:45:34,211 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:45:34,212 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:45:34,216 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:45:34,220 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:45:34,222 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:45:34,223 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:45:34,226 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:45:34,227 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:46:46,859 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:46:46,860 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:46:46,861 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:46:46,862 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:46:46,863 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:46:46,863 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:46:46,864 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:46:46,865 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:46:46,867 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:46:46,869 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:46:46,870 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:46:46,871 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:46:46,873 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:46:46,874 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:46:46,875 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:46:46,876 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:46:47,157 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:46:47,158 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:46:47,159 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:46:47,161 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:46:47,162 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:46:47,162 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:46:47,163 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:46:47,168 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:46:47,173 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:46:47,180 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:46:47,181 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:46:47,183 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:46:47,185 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:46:47,189 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:46:47,190 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:46:47,197 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:46:47,334 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628164647_clipboard_20250628164514.png, Prompt: Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python
from services.gemini_image_generator import GeminiImageGenerator

# Carrega as vari√°veis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite m√°ximo de mensagens no hist√≥rico
MAX_MESSAGES = 15

# Fun√ß√£o para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Voc√™ √© um assistente de IA vers√°til e √∫til. Voc√™ pode conversar sobre diversos assuntos e tamb√©m analisar imagens quando elas forem fornecidas."

# Adicione o conte√∫do dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

@st.cache_resource
def get_image_generator():
    return GeminiImageGenerator()

image_generator = get_image_generator()
# Fun√ß√£o para verificar e processar a √°rea de transfer√™ncia
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da √°rea de transfer√™ncia
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a √°rea de transfer√™ncia: {e}")
        return None, None

# Fun√ß√£o para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o n√∫mero da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o n√∫mero
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Fun√ß√£o que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # For√ßa a reexecu√ß√£o para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que n√£o exceda o limite de requisi√ß√µes
    rate_limiter.wait_for_slot()  # Espera at√© que um slot esteja dispon√≠vel

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usu√°rio ao hist√≥rico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constr√≥i o prompt completo incluindo o hist√≥rico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt espec√≠fico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se n√£o tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"‚ùå Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o hist√≥rico n√£o exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem tempor√°ria do disco ap√≥s uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como conclu√≠do, mas N√ÉO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o bot√£o de colar da √°rea de transfer√™ncia √© clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da √°rea de transfer√™ncia
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo √© carregado
def on_file_upload():
    # Limpa qualquer imagem da √°rea de transfer√™ncia
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configura√ß√£o da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Se√ß√£o de gera√ß√£o de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = image_generator.generate_image(image_prompt)


            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para n√£o exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Se√ß√£o de imagens (sempre vis√≠vel)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os bot√µes de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Bot√£o para verificar a √°rea de transfer√™ncia
        if st.button("üìã Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Bot√£o para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("üóëÔ∏è Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave din√¢mica
    uploaded_file = st.file_uploader(
        "üì∑ Ou fa√ßa upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo √© carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da √°rea de transfer√™ncia")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Bot√£o para limpar o hist√≥rico de conversa
    if st.button("üßπ Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibi√ß√£o da imagem gerada aqui (ela ser√° exibida no hist√≥rico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibi√ß√£o do hist√≥rico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conte√∫do da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digita√ß√£o quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como √∫ltimo elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se h√° uma imagem dispon√≠vel
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presen√ßa de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts din√¢micos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configura√ß√£o de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configura√ß√£o de hist√≥rico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configura√ß√£o de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diret√≥rios necess√°rios existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que voc√™ salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda at√© que haja um slot dispon√≠vel

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrup√ß√£o controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrup√ß√£o controlada.
        
        Args:
            signum: N√∫mero do sinal
            frame: Frame atual
        """
        logger.warning("üö® Processamento interrompido pelo usu√°rio.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplica√ß√£o."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Inst√¢ncia global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o n√∫mero de requisi√ß√µes
    em um determinado per√≠odo de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: N√∫mero m√°ximo de requisi√ß√µes permitidas no per√≠odo
            period_seconds: Per√≠odo de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisi√ß√£o pode ser feita.
        
        Returns:
            bool: True se a requisi√ß√£o for permitida, False caso contr√°rio
        """
        with self.lock:
            current_time = time.time()

            # Remove requisi√ß√µes antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda at√© que um slot esteja dispon√≠vel para uma nova requisi√ß√£o.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisi√ß√£o mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padr√£o se n√£o houver requisi√ß√µes
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o n√∫mero m√°ximo de requisi√ß√µes e o per√≠odo
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisi√ß√µes por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o hist√≥rico ap√≥s o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o hist√≥rico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importa√ß√£o correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configura√ß√£o inicial do documento
        title = doc.add_heading('An√°lise de Imagens com Intelig√™ncia Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subt√≠tulo
        subtitle = doc.add_paragraph('Relat√≥rio Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de p√°gina ap√≥s o t√≠tulo
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para t√≠tulo de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o t√≠tulo
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espa√ßamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o t√≠tulo da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o t√≠tulo antes da imagem


        # Adiciona a imagem ao documento com tamanho de p√°gina inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obt√©m a largura da p√°gina
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura dispon√≠vel (largura da p√°gina menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura dispon√≠vel
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o par√°grafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('‚îÄ' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marca√ß√µes markdown do texto"""
        # Remove cabe√ßalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marca√ß√µes de negrito e it√°lico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '‚Ä¢ ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informa√ß√µes de rodap√©
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gemini_image_generator.py

from google import genai
from google.genai import types
from PIL import Image
import io
import os

class GeminiImageGenerator:
    def __init__(self):
        api_key = os.getenv("API_KEY_GEMINI")
        if not api_key:
            raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")
        self.client = genai.Client(api_key=api_key)

    def generate_image(self, prompt: str) -> Image.Image | None:
        try:
            response = self.client.models.generate_content(
                model='gemini-2.0-flash-exp-image-generation',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_modalities=['Text', 'Image']
                )
            )
            for part in response.candidates[0].content.parts:
                if part.inline_data is not None:
                    return Image.open(io.BytesIO(part.inline_data.data))
            return None
        except Exception as e:
            print(f"Erro ao gerar imagem: {e}")
            return None


# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key n√£o encontrada nas vari√°veis de ambiente")
            raise ValueError("API Key n√£o encontrada nas vari√°veis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conte√∫do: {e}")
            raise RuntimeError(f"Erro ao gerar conte√∫do: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o hist√≥rico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt n√£o encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # N√£o adicionar a mesma informa√ß√£o repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a pr√≥xima an√°lise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # N√£o precisa carregar o hist√≥rico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHist√≥rico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Hist√≥rico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

# import os
# from dotenv import load_dotenv
# from google import genai
# from PIL import Image
# from io import BytesIO

# # Carrega as vari√°veis de ambiente do arquivo .env
# load_dotenv()

# # Obt√©m a chave da API Gemini do arquivo .env
# api_key = os.getenv("API_KEY_GEMINI")

# # Verifica se a chave da API foi carregada corretamente
# if not api_key:
#     raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

# # Inicializa o Gemini
# genai.configure(api_key=api_key)

# def generate_image(prompt: str) -> Image.Image | None:
#     """
#     Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

#     Args:
#         prompt (str): O prompt de texto para gerar a imagem.

#     Returns:
#         Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
#     """
#     try:
#         model = genai.GenerativeModel('gemini-2.0-flash-preview-image-generation')
#         response = model.generate_content(prompt)
#         if response.prompt_feedback:
#             print('Reason: {}'.format(response.prompt_feedback.block_reason))
#         # Verifique se a resposta cont√©m dados de imagem
#         if response.parts:
#             for part in response.parts:
#                 if part.mime_type == 'image/png':
#                     return Image.open(BytesIO(part.data))
#         print(response.text)
#         return None
#     except Exception as e:
#         print(f"Erro ao gerar imagem: {e}")
#         return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conte√∫do do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conte√∫do seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo n√£o existir, cria o cabe√ßalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das An√°lises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista ap√≥s salvar para evitar duplica√ß√£o


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o m√≥dulo de logging

# Configure o logging (voc√™ pode ajustar o n√≠vel conforme necess√°rio)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """L√™ todo o conte√∫do de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diret√≥rio 'src' n√£o encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Obt√©m a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI n√£o encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

Assistant: Imagem gerada com o prompt: gere uma imagem aleatoria
User: Que imagem [e esas ?
Assistant: Esta imagem mostra uma criatura fant√°stica e fofa de cor roxa vibrante, com olhos grandes e amarelos que parecem brilhar. Ela tem pelos bem felpudos e orelhas pontudas, grandes e peludas, al√©m de um pequeno focinho rosa. A criatura est√° sentada em um terreno coberto por folhagem densa e azulada, que se assemelha a musgo ou pequenos arbustos.

O ambiente ao redor √© uma floresta m√°gica e escura, com √°rvores de troncos retorcidos e escuros que emolduram a cena. H√° pontos de luz brilhantes e esferas azul-turquesa luminosas espalhadas pelo ar e na folhagem, criando uma atmosfera encantada. Ao fundo, parece haver uma cachoeira ou uma forma√ß√£o rochosa vertical envolta em n√©voa, sob um c√©u que varia de tons de azul a √¢mbar, com mais pontos luminosos.

√â uma imagem que evoca um sentimento de fantasia e mist√©rio, com a criatura parecendo amig√°vel e curiosa. Esta √© a imagem gerada aleatoriamente que voc√™ viu anteriormente.
User: faca um prompt para gerar uma imagem do mundial do Corinthians de 2012
2025-06-28 16:47:01,704 [INFO] Resposta da IA (imagem): Para gerar uma imagem do Mundial do Corinthians de 2012, voc√™ precisa ser bem espec√≠fico com os detalhes para que a IA capte a ess√™ncia do evento. Aqui est√£o algumas op√ß√µes de prompts, focando em diferentes aspectos:

---

**Prompt Principal (Cena de Vit√≥ria e Trof√©u):**

"Uma cena dram√°tica e √©pica do time de futebol Sport Club Corinthians Paulista celebrando sua vit√≥ria na final da Copa do Mundo de Clubes da FIFA de 2012. Os jogadores, usando seus uniformes brancos com listras pretas, est√£o levantando o trof√©u dourado bem alto, cercados por confetes caindo. Foco em jogadores ic√¥nicos como o goleiro C√°ssio, o artilheiro Paolo Guerrero, e Emerson Sheik, mostrando pura alegria e triunfo. O fundo √© o Est√°dio Internacional de Yokohama lotado √† noite, com um mar de torcedores apaixonados do Corinthians agitando bandeiras e comemorando. As luzes do est√°dio iluminam a cena, criando uma atmosfera vibrante e emocionante. Fotografia realista, alto detalhe, clima vitorioso, cinem√°tico."

---

**Varia√ß√£o 1 (Foco no Jogador e Momento Crucial):**

"Close-up din√¢mico de Paolo Guerrero, o jogador do Corinthians, no momento em que ele marca o gol decisivo na final da Copa do Mundo de Clubes da FIFA de 2012 contra o Chelsea. Ele est√° comemorando intensamente, com a camisa branca e preta do Corinthians, e a bola no fundo da rede. O est√°dio de Yokohama ao fundo, com a torcida em √™xtase e luzes noturnas. Captura de movimento, emo√ß√£o crua, fotografia de a√ß√£o."

---

**Varia√ß√£o 2 (Foco na Torcida e Atmosfera):**

"Uma vista a√©rea espetacular do Est√°dio Internacional de Yokohama completamente lotado durante a final da Copa do Mundo de Clubes da FIFA de 2012, focando na torcida do Corinthians. Um mar de preto e branco, com bandeiras balan√ßando, sinalizadores (opcional, se permitido pelo modelo) e a energia eletrizante da vit√≥ria. Luzes do est√°dio, confetes no ar. Perspectiva ampla, cores vibrantes, captura da paix√£o dos f√£s."

---

**Dicas para Melhores Resultados:**

*   **Especificidade:** Quanto mais detalhes voc√™ der (cores, jogadores, local, emo√ß√£o), melhor.
*   **Qualidade Visual:** Termos como "fotografia realista", "alto detalhe", "cinem√°tico", "cores vibrantes" ajudam a IA a entender o estilo desejado.
*   **Emo√ß√£o:** Palavras como "alegria", "triunfo", "paix√£o", "√™xtase" guiam a IA para o tom certo.
*   **Evite Negativas:** Tente descrever o que voc√™ quer ver, em vez do que n√£o quer (ex: em vez de "sem chuva", diga "c√©u claro").

Experimente esses prompts e ajuste conforme o modelo de IA responde!
2025-06-28 16:47:02,065 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:47:02,066 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:47:02,068 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:47:02,069 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:47:02,070 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:47:02,079 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:47:02,092 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:47:02,098 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:47:02,104 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:47:02,107 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:47:02,123 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:47:02,136 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:47:02,187 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:47:02,208 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:47:02,236 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:47:02,274 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:47:26,712 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:47:26,715 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:47:26,715 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:47:26,716 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:47:26,717 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:47:26,721 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:47:26,732 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:47:26,735 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:47:26,756 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:47:26,762 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:47:26,772 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:47:26,792 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:47:26,817 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:47:26,849 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:47:26,857 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:47:26,968 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:47:26,977 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:47:26,982 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:47:26,995 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:47:27,000 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:47:27,140 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:47:27,141 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:47:27,142 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:47:27,143 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:47:27,144 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:47:27,144 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:47:27,145 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:47:27,146 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:47:27,147 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:47:27,148 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:47:27,149 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:47:27,156 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:47:27,166 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 16:47:31,843 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 16:47:33,152 [INFO] AFC remote call 1 is done.
2025-06-28 16:47:33,260 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:47:33,266 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:47:33,268 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:47:33,270 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:47:33,272 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:47:33,275 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:47:33,287 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:47:33,289 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:47:33,290 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:47:33,292 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:47:33,293 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:47:33,297 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:47:33,308 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:47:33,309 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:47:33,317 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:47:33,388 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:48:26,792 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:48:26,795 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:48:26,797 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:48:26,798 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:48:26,799 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:48:26,801 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:48:26,802 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:48:26,805 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:48:26,819 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:48:26,821 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:48:26,822 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:48:26,823 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:48:26,825 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:48:26,832 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:48:26,834 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:48:26,838 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:48:27,168 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:48:27,171 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:48:27,172 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:48:27,173 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:48:27,174 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:48:27,176 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:48:27,188 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:48:27,191 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:48:27,202 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:48:27,205 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:48:27,206 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:48:27,208 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:48:27,216 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:48:27,217 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:48:27,224 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:48:27,249 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:48:27,261 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 16:48:28,380 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:48:28,380 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:48:28,382 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:48:28,383 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:48:28,384 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:48:28,386 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:48:28,388 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:48:28,389 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:48:28,401 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:48:28,402 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:48:28,403 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:48:28,404 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:48:28,406 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:48:28,407 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:48:28,415 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:48:28,418 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:48:28,597 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:48:28,598 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:48:28,600 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:48:28,602 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:48:28,603 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:48:28,604 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:48:28,605 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:48:28,607 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:48:28,614 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:48:28,617 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:48:28,618 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:48:28,619 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:48:28,621 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:48:28,622 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:48:28,623 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:48:28,631 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:48:31,022 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:48:31,024 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:48:31,025 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:48:31,025 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:48:31,026 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:48:31,027 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:48:31,029 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:48:31,032 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:48:31,034 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:48:31,043 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:48:31,045 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:48:31,046 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:48:31,047 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:48:31,048 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:48:31,049 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:48:31,050 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:48:31,065 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 16:48:31,469 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 16:48:32,577 [INFO] AFC remote call 1 is done.
2025-06-28 16:48:35,284 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 16:48:36,430 [INFO] AFC remote call 1 is done.
2025-06-28 16:48:36,751 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:48:36,754 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:48:36,755 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:48:36,757 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:48:36,758 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:48:36,760 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:48:36,762 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:48:36,775 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:48:36,776 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:48:36,778 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:48:36,779 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:48:36,780 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:48:36,782 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:48:36,794 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:48:36,796 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:48:36,798 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:50:24,684 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:50:24,685 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:50:24,686 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:50:24,687 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:50:24,688 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:50:24,690 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:50:24,692 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:50:24,695 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:50:24,698 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:50:24,705 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:50:24,930 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:50:25,080 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:50:25,096 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:50:25,098 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:50:25,101 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:50:25,102 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:50:25,111 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 16:50:29,609 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 16:50:30,715 [INFO] AFC remote call 1 is done.
2025-06-28 16:50:30,805 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:50:30,806 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:50:30,807 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:50:30,808 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:50:30,809 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:50:30,810 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:50:30,810 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:50:30,813 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:50:30,818 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:50:30,819 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:50:30,821 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:50:30,824 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:50:30,828 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:50:30,829 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:50:30,830 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:50:30,831 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:53:03,429 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:53:03,432 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:53:03,434 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:53:03,439 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:53:03,446 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:53:03,447 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:53:03,450 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:53:03,452 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:53:03,467 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:53:03,503 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:53:03,515 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:53:03,517 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:53:03,536 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:53:03,548 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:53:03,551 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:53:03,553 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:53:04,464 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 16:53:11,714 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:53:11,715 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:53:11,717 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:53:11,719 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:53:11,720 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:53:11,720 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:53:11,722 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:53:11,724 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:53:11,726 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:53:11,738 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:53:11,740 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:53:11,748 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:53:11,755 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:53:11,757 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:53:11,758 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:53:11,759 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:53:16,192 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:53:16,195 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:53:16,196 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:53:16,198 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:53:16,200 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:53:16,202 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:53:16,204 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:53:16,210 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:53:16,211 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:53:16,217 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:53:16,218 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:53:16,219 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:53:16,220 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:53:16,221 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:53:16,222 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:53:16,227 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:53:16,386 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:53:16,387 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:53:16,388 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:53:16,389 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:53:16,390 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:53:16,393 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:53:16,394 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:53:16,396 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:53:16,398 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:53:16,402 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:53:16,404 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:53:16,405 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:53:16,406 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:53:16,410 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:53:16,418 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:53:16,419 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:53:16,435 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 16:53:20,227 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-preview-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 16:53:21,385 [INFO] AFC remote call 1 is done.
2025-06-28 16:53:21,518 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:53:21,519 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:53:21,525 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:53:21,527 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:53:21,531 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:53:21,533 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:53:21,534 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:53:21,535 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:53:21,547 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:53:21,549 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:53:21,551 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:53:21,552 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:53:21,562 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:53:21,564 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:53:21,566 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:53:21,567 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 17:02:18,434 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 17:02:18,439 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 17:02:18,443 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 17:02:18,447 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 17:02:18,465 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 17:02:18,594 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 17:02:18,596 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 17:02:18,601 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 17:02:18,606 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 17:02:18,614 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 17:02:18,618 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 17:02:18,622 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 17:02:18,626 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 17:02:18,628 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 17:02:18,636 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 17:02:18,639 [INFO] Arquivo lido com sucesso: utils\file_utils.py
