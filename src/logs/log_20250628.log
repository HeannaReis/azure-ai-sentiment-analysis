2025-06-28 11:33:14,871 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:33:14,872 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:33:14,873 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:33:14,874 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:33:14,875 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:33:14,876 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:33:14,876 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:33:14,877 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:33:14,882 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:33:14,883 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:33:14,884 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:33:14,886 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:33:14,887 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:33:14,888 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:33:14,889 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:37:53,170 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:37:53,173 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:37:53,174 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:37:53,175 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:37:53,177 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:37:53,182 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:37:53,183 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:37:53,184 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:37:53,185 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:37:53,185 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:37:53,186 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:37:53,189 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:37:53,197 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:37:53,198 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:37:53,199 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:37:53,214 [INFO] Modelo Gemini 'gemini-2.0-flash-exp' inicializado com sucesso.
2025-06-28 11:39:13,368 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:39:13,369 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:39:13,371 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:39:13,372 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:39:13,373 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:39:13,376 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:39:13,379 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:39:13,380 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:39:13,383 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:39:13,386 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:39:13,398 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:39:13,399 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:39:13,403 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:39:13,422 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:39:13,425 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:39:13,814 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:39:13,816 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:39:13,817 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:39:13,818 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:39:13,820 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:39:13,826 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:39:13,830 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:39:13,837 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:39:13,840 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:39:13,850 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:39:13,851 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:39:13,856 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:39:13,864 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:39:13,867 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:39:13,869 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:39:15,171 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 11:39:23,837 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 11:39:25,091 [INFO] AFC remote call 1 is done.
2025-06-28 11:42:04,218 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:42:04,219 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:42:04,220 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:42:04,221 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:42:04,222 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:42:04,223 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:42:04,225 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:42:04,228 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:42:04,229 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:42:04,231 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:42:04,232 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:42:04,234 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:42:04,236 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:42:04,237 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:42:04,239 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:42:04,536 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:42:04,537 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:42:04,539 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:42:04,545 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:42:04,549 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:42:04,554 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:42:04,561 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:42:04,567 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:42:04,588 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:42:04,593 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:42:04,615 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:42:04,618 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:42:04,621 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:42:04,632 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:42:04,635 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:42:04,649 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")



=============

Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000018E29EB7B50>

erro na minha aplicação conforme códigos anexos
2025-06-28 11:42:16,509 [INFO] Resposta da IA (texto): O erro "cannot identify image file <_io.BytesIO object at...>" indica que a função `Image.open()` do PIL (Pillow) não conseguiu identificar o formato da imagem dentro do objeto `BytesIO`. Isso geralmente acontece quando os dados dentro do `BytesIO` não são dados de imagem válidos ou o PIL não consegue determinar o formato.

Analisando o código, o problema mais provável reside na função `generate_image` em `services\image_services.py` e, consequentemente, no uso dessa função no script `chat_streamlit.py`.

Aqui estão os pontos a serem revisados e as possíveis soluções:

**1. Confirmação do Tipo MIME (services\image_services.py):**

Certifique-se de que o `part.mime_type` seja realmente 'image/png' (ou outro tipo de imagem válido como 'image/jpeg') antes de tentar abrir com `Image.open()`.  Se o tipo MIME for diferente, trate-o adequadamente ou ignore a parte.

```python
def generate_image(prompt: str) -> Image.Image | None:
    # ... (código anterior) ...
    if response.parts:
        for part in response.parts:
            if part.mime_type == 'image/png' or part.mime_type == 'image/jpeg':  # Adicionado suporte a JPEG
                try:
                    return Image.open(BytesIO(part.data))
                except Exception as e:
                    print(f"Erro ao abrir imagem do BytesIO: {e}")
                    return None
            else:
                print(f"Parte ignorada, tipo MIME: {part.mime_type}") # Debugging
    # ... (código posterior) ...
```

**2. Tratamento de Erros ao Abrir a Imagem (services\image_services.py):**

Adicione um bloco `try...except` dentro do loop `for part in response.parts:` para capturar erros que podem ocorrer ao tentar abrir a imagem com `Image.open()`. Isso pode fornecer informações mais específicas sobre o motivo da falha.  A impressão do `part.mime_type` ajudará a entender se o problema é um tipo mime inesperado.

**3. Validação dos Dados da Imagem (services\image_services.py):**

Antes de usar `Image.open()`, verifique se `part.data` contém dados.  Se estiver vazio, não tente abrir.

```python
if part.inline_data is not None:
    if part.inline_data.data:  # Verifica se há dados
        image = Image.open(BytesIO(part.inline_data.data))
        st.session_state.generated_image = image
        return image
    else:
        print("Dados da imagem vazios!") # Debugging
```

**4. Chame a função generate_image corretamente no Streamlit (chat_streamlit.py):**

Certifique-se que `st.session_state.generated_image` seja None no início da função `generate_image`, para evitar estados incorretos.

**5. API Key e Modelo:**

Embora o código verifique se a API Key está definida, verifique se ela é válida e tem permissão para usar o modelo `gemini-2.0-flash-exp-image-generation`.  A mensagem de erro pode ser genérica e esconder um problema de autenticação ou permissão.

**6.  Informações Adicionais no Log:**

Adicione mais informações de log nos pontos críticos do código para facilitar a depuração.  Por exemplo, registre o tamanho de `part.data` antes de tentar abrir a imagem.

```python
logger.info(f"Tamanho dos dados da imagem: {len(part.inline_data.data)}") # Log do tamanho dos dados
```

**7. Contexto dos Arquivos Python (prompt):**

O prompt que você está enviando para o modelo Gemini inclui o código fonte de vários arquivos Python. Isso é útil para o Gemini responder perguntas sobre seu código, mas pode estar consumindo muitos tokens e potencialmente afetando o comportamento do modelo (e talvez até contribuindo para o erro de geração de imagem). Considere remover o código fonte do prompt, a menos que seja absolutamente necessário.

**8. Dependências:**

Certifique-se de que todas as dependências necessárias (google-generativeai, Pillow, etc.) estejam instaladas corretamente.

**9.  Rate Limiting:**

O código possui um mecanismo de rate limiting, mas certifique-se de que ele esteja configurado corretamente e não esteja bloqueando as requisições.  Aumente o tempo de espera para teste, só para ter certeza que o problema não é esse.

**10. Teste Simplificado:**

Crie um script de teste muito simples (fora do Streamlit) que apenas chame a função `generate_image` com um prompt básico e imprima os resultados. Isso ajudará a isolar o problema e determinar se ele está relacionado ao código de geração de imagem em si ou à integração com o Streamlit. O exemplo no final do arquivo `services\image_services.py` é um bom começo, mas adicione tratamento de erros mais detalhado e logging.

**Exemplo de código de depuração:**

```python
from services.image_services import generate_image

try:
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        print("Imagem gerada com sucesso!")
        image.show()  # Exibe a imagem (opcional)
        image.save("gato_astronauta.png")  # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")
except Exception as e:
    print(f"Erro geral: {e}")

```

Lembre-se de verificar os logs (tanto os logs do sistema quanto os logs adicionados no seu código) para obter mais informações sobre a causa do erro.  Analise a saída da impressão dos tipos MIME para entender melhor o que está acontecendo.

2025-06-28 11:42:16,738 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:42:16,739 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:42:16,740 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:42:16,741 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:42:16,742 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:42:16,743 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:42:16,746 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:42:16,748 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:42:16,753 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:42:16,754 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:42:16,756 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:42:16,756 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:42:16,758 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:42:16,759 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:42:16,762 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:44:32,605 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:44:32,606 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:44:32,607 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:44:32,608 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:44:32,609 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:44:32,611 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:44:32,612 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:44:32,615 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:44:32,618 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:44:32,619 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:44:32,620 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:44:32,621 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:44:32,624 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:44:32,625 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:44:32,627 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:44:33,146 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:44:33,147 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:44:33,148 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:44:33,150 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:44:33,151 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:44:33,152 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:44:33,153 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:44:33,155 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:44:33,157 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:44:33,160 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:44:33,166 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:44:33,167 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:44:33,169 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:44:33,170 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:44:33,171 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:44:33,180 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")



=============

Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000018E29EB7B50>

erro na minha aplicação conforme códigos anexos
Assistant: O erro "cannot identify image file <_io.BytesIO object at...>" indica que a função `Image.open()` do PIL (Pillow) não conseguiu identificar o formato da imagem dentro do objeto `BytesIO`. Isso geralmente acontece quando os dados dentro do `BytesIO` não são dados de imagem válidos ou o PIL não consegue determinar o formato.

Analisando o código, o problema mais provável reside na função `generate_image` em `services\image_services.py` e, consequentemente, no uso dessa função no script `chat_streamlit.py`.

Aqui estão os pontos a serem revisados e as possíveis soluções:

**1. Confirmação do Tipo MIME (services\image_services.py):**

Certifique-se de que o `part.mime_type` seja realmente 'image/png' (ou outro tipo de imagem válido como 'image/jpeg') antes de tentar abrir com `Image.open()`.  Se o tipo MIME for diferente, trate-o adequadamente ou ignore a parte.

```python
def generate_image(prompt: str) -> Image.Image | None:
    # ... (código anterior) ...
    if response.parts:
        for part in response.parts:
            if part.mime_type == 'image/png' or part.mime_type == 'image/jpeg':  # Adicionado suporte a JPEG
                try:
                    return Image.open(BytesIO(part.data))
                except Exception as e:
                    print(f"Erro ao abrir imagem do BytesIO: {e}")
                    return None
            else:
                print(f"Parte ignorada, tipo MIME: {part.mime_type}") # Debugging
    # ... (código posterior) ...
```

**2. Tratamento de Erros ao Abrir a Imagem (services\image_services.py):**

Adicione um bloco `try...except` dentro do loop `for part in response.parts:` para capturar erros que podem ocorrer ao tentar abrir a imagem com `Image.open()`. Isso pode fornecer informações mais específicas sobre o motivo da falha.  A impressão do `part.mime_type` ajudará a entender se o problema é um tipo mime inesperado.

**3. Validação dos Dados da Imagem (services\image_services.py):**

Antes de usar `Image.open()`, verifique se `part.data` contém dados.  Se estiver vazio, não tente abrir.

```python
if part.inline_data is not None:
    if part.inline_data.data:  # Verifica se há dados
        image = Image.open(BytesIO(part.inline_data.data))
        st.session_state.generated_image = image
        return image
    else:
        print("Dados da imagem vazios!") # Debugging
```

**4. Chame a função generate_image corretamente no Streamlit (chat_streamlit.py):**

Certifique-se que `st.session_state.generated_image` seja None no início da função `generate_image`, para evitar estados incorretos.

**5. API Key e Modelo:**

Embora o código verifique se a API Key está definida, verifique se ela é válida e tem permissão para usar o modelo `gemini-2.0-flash-exp-image-generation`.  A mensagem de erro pode ser genérica e esconder um problema de autenticação ou permissão.

**6.  Informações Adicionais no Log:**

Adicione mais informações de log nos pontos críticos do código para facilitar a depuração.  Por exemplo, registre o tamanho de `part.data` antes de tentar abrir a imagem.

```python
logger.info(f"Tamanho dos dados da imagem: {len(part.inline_data.data)}") # Log do tamanho dos dados
```

**7. Contexto dos Arquivos Python (prompt):**

O prompt que você está enviando para o modelo Gemini inclui o código fonte de vários arquivos Python. Isso é útil para o Gemini responder perguntas sobre seu código, mas pode estar consumindo muitos tokens e potencialmente afetando o comportamento do modelo (e talvez até contribuindo para o erro de geração de imagem). Considere remover o código fonte do prompt, a menos que seja absolutamente necessário.

**8. Dependências:**

Certifique-se de que todas as dependências necessárias (google-generativeai, Pillow, etc.) estejam instaladas corretamente.

**9.  Rate Limiting:**

O código possui um mecanismo de rate limiting, mas certifique-se de que ele esteja configurado corretamente e não esteja bloqueando as requisições.  Aumente o tempo de espera para teste, só para ter certeza que o problema não é esse.

**10. Teste Simplificado:**

Crie um script de teste muito simples (fora do Streamlit) que apenas chame a função `generate_image` com um prompt básico e imprima os resultados. Isso ajudará a isolar o problema e determinar se ele está relacionado ao código de geração de imagem em si ou à integração com o Streamlit. O exemplo no final do arquivo `services\image_services.py` é um bom começo, mas adicione tratamento de erros mais detalhado e logging.

**Exemplo de código de depuração:**

```python
from services.image_services import generate_image

try:
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        print("Imagem gerada com sucesso!")
        image.show()  # Exibe a imagem (opcional)
        image.save("gato_astronauta.png")  # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")
except Exception as e:
    print(f"Erro geral: {e}")

```

Lembre-se de verificar os logs (tanto os logs do sistema quanto os logs adicionados no seu código) para obter mais informações sobre a causa do erro.  Analise a saída da impressão dos tipos MIME para entender melhor o que está acontecendo.

User: preciso instalar pilow com pip ?
2025-06-28 11:44:36,189 [INFO] Resposta da IA (texto): Assistant: Sim, você precisa instalar a biblioteca Pillow (PIL) usando o pip, caso ainda não a tenha instalada. Pillow é a versão moderna e ativa do PIL (Python Imaging Library), que é necessária para manipular imagens em Python.

Abra seu terminal ou prompt de comando e execute o seguinte comando:

```bash
pip install Pillow
```

Este comando irá baixar e instalar a versão mais recente da Pillow no seu ambiente Python. Após a instalação, você poderá importar e usar a biblioteca Pillow no seu código sem problemas.

Certifique-se de que você está executando o `pip` dentro do mesmo ambiente Python que você está usando para executar seu script Streamlit.  Se você estiver usando um ambiente virtual (venv), ative-o antes de executar o `pip install Pillow`.

2025-06-28 11:44:36,394 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:44:36,396 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:44:36,398 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:44:36,399 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:44:36,401 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:44:36,403 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:44:36,404 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:44:36,405 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:44:36,416 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:44:36,417 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:44:36,418 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:44:36,418 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:44:36,419 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:44:36,420 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:44:36,420 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:48:37,002 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:48:37,003 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:48:37,005 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:48:37,006 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:48:37,007 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:48:37,008 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:48:37,009 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:48:37,011 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:48:37,013 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:48:37,014 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:48:37,015 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:48:37,016 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:48:37,017 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:48:37,018 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:48:37,020 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:48:37,184 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:48:37,185 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:48:37,187 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:48:37,189 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:48:37,190 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:48:37,192 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:48:37,193 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:48:37,194 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:48:37,195 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:48:37,197 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:48:37,200 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:48:37,201 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:48:37,203 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:48:37,204 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:48:37,205 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:48:37,210 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")



=============

Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000018E29EB7B50>

erro na minha aplicação conforme códigos anexos
Assistant: O erro "cannot identify image file <_io.BytesIO object at...>" indica que a função `Image.open()` do PIL (Pillow) não conseguiu identificar o formato da imagem dentro do objeto `BytesIO`. Isso geralmente acontece quando os dados dentro do `BytesIO` não são dados de imagem válidos ou o PIL não consegue determinar o formato.

Analisando o código, o problema mais provável reside na função `generate_image` em `services\image_services.py` e, consequentemente, no uso dessa função no script `chat_streamlit.py`.

Aqui estão os pontos a serem revisados e as possíveis soluções:

**1. Confirmação do Tipo MIME (services\image_services.py):**

Certifique-se de que o `part.mime_type` seja realmente 'image/png' (ou outro tipo de imagem válido como 'image/jpeg') antes de tentar abrir com `Image.open()`.  Se o tipo MIME for diferente, trate-o adequadamente ou ignore a parte.

```python
def generate_image(prompt: str) -> Image.Image | None:
    # ... (código anterior) ...
    if response.parts:
        for part in response.parts:
            if part.mime_type == 'image/png' or part.mime_type == 'image/jpeg':  # Adicionado suporte a JPEG
                try:
                    return Image.open(BytesIO(part.data))
                except Exception as e:
                    print(f"Erro ao abrir imagem do BytesIO: {e}")
                    return None
            else:
                print(f"Parte ignorada, tipo MIME: {part.mime_type}") # Debugging
    # ... (código posterior) ...
```

**2. Tratamento de Erros ao Abrir a Imagem (services\image_services.py):**

Adicione um bloco `try...except` dentro do loop `for part in response.parts:` para capturar erros que podem ocorrer ao tentar abrir a imagem com `Image.open()`. Isso pode fornecer informações mais específicas sobre o motivo da falha.  A impressão do `part.mime_type` ajudará a entender se o problema é um tipo mime inesperado.

**3. Validação dos Dados da Imagem (services\image_services.py):**

Antes de usar `Image.open()`, verifique se `part.data` contém dados.  Se estiver vazio, não tente abrir.

```python
if part.inline_data is not None:
    if part.inline_data.data:  # Verifica se há dados
        image = Image.open(BytesIO(part.inline_data.data))
        st.session_state.generated_image = image
        return image
    else:
        print("Dados da imagem vazios!") # Debugging
```

**4. Chame a função generate_image corretamente no Streamlit (chat_streamlit.py):**

Certifique-se que `st.session_state.generated_image` seja None no início da função `generate_image`, para evitar estados incorretos.

**5. API Key e Modelo:**

Embora o código verifique se a API Key está definida, verifique se ela é válida e tem permissão para usar o modelo `gemini-2.0-flash-exp-image-generation`.  A mensagem de erro pode ser genérica e esconder um problema de autenticação ou permissão.

**6.  Informações Adicionais no Log:**

Adicione mais informações de log nos pontos críticos do código para facilitar a depuração.  Por exemplo, registre o tamanho de `part.data` antes de tentar abrir a imagem.

```python
logger.info(f"Tamanho dos dados da imagem: {len(part.inline_data.data)}") # Log do tamanho dos dados
```

**7. Contexto dos Arquivos Python (prompt):**

O prompt que você está enviando para o modelo Gemini inclui o código fonte de vários arquivos Python. Isso é útil para o Gemini responder perguntas sobre seu código, mas pode estar consumindo muitos tokens e potencialmente afetando o comportamento do modelo (e talvez até contribuindo para o erro de geração de imagem). Considere remover o código fonte do prompt, a menos que seja absolutamente necessário.

**8. Dependências:**

Certifique-se de que todas as dependências necessárias (google-generativeai, Pillow, etc.) estejam instaladas corretamente.

**9.  Rate Limiting:**

O código possui um mecanismo de rate limiting, mas certifique-se de que ele esteja configurado corretamente e não esteja bloqueando as requisições.  Aumente o tempo de espera para teste, só para ter certeza que o problema não é esse.

**10. Teste Simplificado:**

Crie um script de teste muito simples (fora do Streamlit) que apenas chame a função `generate_image` com um prompt básico e imprima os resultados. Isso ajudará a isolar o problema e determinar se ele está relacionado ao código de geração de imagem em si ou à integração com o Streamlit. O exemplo no final do arquivo `services\image_services.py` é um bom começo, mas adicione tratamento de erros mais detalhado e logging.

**Exemplo de código de depuração:**

```python
from services.image_services import generate_image

try:
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        print("Imagem gerada com sucesso!")
        image.show()  # Exibe a imagem (opcional)
        image.save("gato_astronauta.png")  # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")
except Exception as e:
    print(f"Erro geral: {e}")

```

Lembre-se de verificar os logs (tanto os logs do sistema quanto os logs adicionados no seu código) para obter mais informações sobre a causa do erro.  Analise a saída da impressão dos tipos MIME para entender melhor o que está acontecendo.

User: preciso instalar pilow com pip ?
Assistant: Assistant: Sim, você precisa instalar a biblioteca Pillow (PIL) usando o pip, caso ainda não a tenha instalada. Pillow é a versão moderna e ativa do PIL (Python Imaging Library), que é necessária para manipular imagens em Python.

Abra seu terminal ou prompt de comando e execute o seguinte comando:

```bash
pip install Pillow
```

Este comando irá baixar e instalar a versão mais recente da Pillow no seu ambiente Python. Após a instalação, você poderá importar e usar a biblioteca Pillow no seu código sem problemas.

Certifique-se de que você está executando o `pip` dentro do mesmo ambiente Python que você está usando para executar seu script Streamlit.  Se você estiver usando um ambiente virtual (venv), ative-o antes de executar o `pip install Pillow`.

User: enviar os arquivos que devem ser alterados completos.
2025-06-28 11:49:05,114 [INFO] Resposta da IA (texto): Assistant: Ok, aqui estão os arquivos `services\image_services.py` e `chat_streamlit.py` completos com as alterações sugeridas.  Eu combinei as mudanças de tratamento de erros e logging para maior clareza.

**services\image_services.py**

```python
import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO
import logging
from core.logger_config import logger  # Importe o logger

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            logger.warning(f"Prompt Feedback Reason: {response.prompt_feedback.block_reason}")

        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                logger.info(f"Tipo MIME da parte: {part.mime_type}")  # Log do tipo MIME
                if part.mime_type == 'image/png' or part.mime_type == 'image/jpeg':
                    try:
                        image_data = part.inline_data.data if hasattr(part, 'inline_data') and part.inline_data else part.data

                        if not image_data:
                            logger.warning("Dados da imagem vazios!")
                            continue  # Pula para a próxima parte

                        logger.info(f"Tamanho dos dados da imagem: {len(image_data)}")  # Log do tamanho dos dados
                        image = Image.open(BytesIO(image_data))
                        logger.info("Imagem aberta com sucesso!")
                        return image
                    except Exception as e:
                        logger.error(f"Erro ao abrir imagem do BytesIO: {e}", exc_info=True)
                        continue  # Tenta a próxima parte
                else:
                    logger.warning(f"Parte ignorada, tipo MIME: {part.mime_type}")  # Debugging
        logger.info(f"Resposta da IA (texto): {response.text}")  # Log da resposta de texto
        return None
    except Exception as e:
        logger.error(f"Erro ao gerar imagem: {e}", exc_info=True)
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    try:
        image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
        if image:
            print("Imagem gerada com sucesso!")
            image.show() # Exibe a imagem (opcional)
            image.save("gato_astronauta.png") # Salva a imagem (opcional)
        else:
            print("Falha ao gerar a imagem.")
    except Exception as e:
        print(f"Erro geral no exemplo de uso: {e}")
```

**chat_streamlit.py**

```python
import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
import base64
from services.search_files import ler_todos_arquivos_python
from services.image_services import generate_image  # Importa a função generate_image
from core.logger_config import logger  # Importe o logger

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 20

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image_wrapper(prompt):
    try:
        st.session_state.generated_image = None # Limpa para evitar estados incorretos
        image = generate_image(prompt)

        if image:
            return image
        else:
            st.error("Falha ao gerar a imagem (verifique os logs).") # Mensagem mais clara
            return None
    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        logger.error(f"Erro ao gerar imagem: {e}", exc_info=True) # Log completo
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):
        if image_prompt:
            generated_image = generate_image_wrapper(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)
```

**Principais Mudanças e Explicações:**

*   **Logging:** Adicionei logging mais detalhado em `services\image_services.py` para ajudar a identificar exatamente onde a falha está ocorrendo.  Usei o `logger` do seu projeto, configurado em `core\logger_config.py`.
*   **Tratamento de `inline_data` e `part.data`:**  O código agora verifica se `part.inline_data` existe antes de tentar acessar `part.inline_data.data`.  Também, caso `part.inline_data` não exista, ele tenta usar `part.data` diretamente.
*   **Verificação do Tipo MIME:** O código agora verifica explicitamente se o `mime_type` é `'image/png'` ou `'image/jpeg'` antes de tentar abrir a imagem. Ele também imprime um aviso se o tipo MIME for diferente.
*   **Verificação de Dados Vazios:** O código agora verifica se `image_data` (anteriormente `part.inline_data.data`) não está vazio antes de tentar abrir a imagem.
*   **Tratamento de Erros:**  Adicionei blocos `try...except` para capturar erros que podem ocorrer ao abrir a imagem com `Image.open()` e registrar esses erros.
*   **Função Wrapper para `generate_image`:** Criei uma função `generate_image_wrapper` em `chat_streamlit.py` para lidar com erros que podem ocorrer ao chamar a função `generate_image` e para garantir que o `st.session_state.generated_image` seja limpo corretamente.
*   **Manipulação de Erros no Streamlit:** A mensagem de erro no Streamlit foi aprimorada para fornecer informações mais úteis ao usuário.
*   **Log Completo em Caso de Erro:** O `logger.error` agora inclui `exc_info=True` para registrar o stack trace completo em caso de erro.
*   **Limpeza do Estado do Streamlit:** O `st.session_state.generated_image` é explicitamente definido como `None` antes de chamar a função `generate_image`, para evitar estados incorretos.
*   **Importação de `generate_image`:** Certifique-se de importar a função `generate_image` de `services\image_services.py` no `chat_streamlit.py`.

**Como Usar:**

1.  Substitua o conteúdo dos arquivos `services\image_services.py` e `chat_streamlit.py` pelo código acima.
2.  Execute sua aplicação Streamlit.
3.  Se ocorrerem erros, verifique os logs do seu aplicativo (os logs serão gravados em um arquivo e também exibidos no console) para obter mais informações sobre a causa do problema.
4.  Analise a saída da impressão dos tipos MIME para entender melhor o que está acontecendo.

Lembre-se de que essas mudanças adicionam mais tratamento de erros e logging para ajudar a diagnosticar o problema. Se o problema persistir, a análise dos logs fornecerá informações cruciais para a resolução.

2025-06-28 11:49:05,341 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:49:05,346 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:49:05,348 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:49:05,350 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:49:05,351 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:49:05,353 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:49:05,358 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:49:05,360 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:49:05,361 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:49:05,362 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:49:05,364 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:49:05,366 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:49:05,367 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:49:05,368 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:49:05,368 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:51:31,967 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:51:31,968 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:51:31,969 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:51:31,969 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:51:31,970 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:51:31,971 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:51:31,972 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:51:31,973 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:51:31,975 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:51:31,975 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:51:31,976 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:51:31,977 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:51:31,978 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:51:31,979 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:51:31,979 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:51:39,659 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:51:39,662 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:51:39,663 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:51:39,665 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:51:39,666 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:51:39,668 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:51:39,669 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:51:39,671 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:51:39,672 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:51:39,674 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:51:39,677 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:51:39,678 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:51:39,680 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:51:39,682 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:51:39,684 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:51:40,093 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:51:40,094 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:51:40,095 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:51:40,096 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:51:40,097 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:51:40,098 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:51:40,099 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:51:40,102 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:51:40,103 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:51:40,104 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:51:40,107 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:51:40,108 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:51:40,111 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:51:40,112 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:51:40,113 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:51:40,946 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 11:51:45,794 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 11:51:46,983 [INFO] AFC remote call 1 is done.
2025-06-28 11:53:03,536 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:53:03,540 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:53:03,548 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:53:03,560 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:53:03,565 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:53:03,634 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:53:03,735 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:53:03,817 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:53:03,823 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:53:03,828 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:53:03,830 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:53:03,832 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:53:03,834 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:53:03,836 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:53:03,837 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:54:02,532 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:54:02,537 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:54:02,539 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:54:02,540 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:54:02,541 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:54:02,542 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:54:02,545 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:54:02,547 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:54:02,548 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:54:02,551 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:54:02,553 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:54:02,555 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:54:02,556 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:54:02,557 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:54:02,559 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:54:02,857 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:54:02,858 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:54:02,859 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:54:02,861 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:54:02,862 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:54:02,863 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:54:02,865 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:54:02,869 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:54:02,871 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:54:02,873 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:54:02,874 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:54:02,875 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:54:02,876 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:54:02,878 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:54:02,879 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:54:03,845 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 11:54:14,048 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 11:54:15,147 [INFO] AFC remote call 1 is done.
2025-06-28 11:55:53,157 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:55:53,159 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:55:53,160 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:55:53,161 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:55:53,162 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:55:53,162 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:55:53,163 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:55:53,164 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:55:53,165 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:55:53,166 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:55:53,167 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:55:53,168 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:55:53,169 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:55:53,170 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:55:53,171 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:55:53,956 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 11:56:00,448 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 11:56:01,460 [INFO] AFC remote call 1 is done.
2025-06-28 11:56:58,340 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:56:58,347 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:56:58,357 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:56:58,368 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:56:58,369 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:56:58,370 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:56:58,373 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:56:58,376 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:56:58,379 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:56:58,380 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:56:58,387 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:56:58,388 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:56:58,390 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:56:58,395 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:56:58,399 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:57:54,778 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:57:54,779 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:57:54,780 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:57:54,781 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:57:54,783 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:57:54,784 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:57:54,786 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:57:54,787 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:57:54,789 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:57:54,790 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:57:54,792 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:57:54,793 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:57:54,794 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:57:54,795 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:57:54,796 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:58:02,898 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:58:02,903 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:58:02,905 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:58:02,906 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:58:02,908 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:58:02,909 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:58:02,910 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:58:02,912 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:58:02,918 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:58:02,919 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:58:02,920 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:58:02,922 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:58:02,923 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:58:02,925 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:58:02,928 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:58:24,356 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:58:24,360 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:58:24,361 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:58:24,363 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:58:24,364 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:58:24,366 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:58:24,367 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:58:24,368 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:58:24,369 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:58:24,371 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:58:24,373 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:58:24,374 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:58:24,376 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:58:24,379 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:58:24,380 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:58:24,565 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:58:24,569 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:58:24,572 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:58:24,574 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:58:24,578 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:58:24,582 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:58:24,584 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:58:24,586 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:58:24,587 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:58:24,589 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:58:24,590 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:58:24,590 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:58:24,594 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:58:24,598 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:58:24,600 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 11:58:24,606 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.0-flash-exp")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: TypeError: ImageMixin.image() got an unexpected keyword argument 'use_container_width'
Traceback:
File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 589, in _run_script
    exec(code, module.__dict__)
File "C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\chat_streamlit.py", line 322, in <module>
    st.image(st.session_state.uploaded_image, use_container_width=True)
File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\metrics_util.py", line 408, in wrapped_func
    result = non_optional_func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

porque tive esse erro após inserir uma imagem da minha máquina nesse aplicação ?
2025-06-28 11:58:32,589 [INFO] Resposta da IA (texto): O erro `TypeError: ImageMixin.image() got an unexpected keyword argument 'use_container_width'` indica que a função `st.image()` na versão do Streamlit que você está usando não aceita o argumento `use_container_width`. Esse argumento foi introduzido em versões mais recentes do Streamlit.

Aqui estão os passos para resolver o problema:

1.  **Verifique a versão do Streamlit:**

    *   Abra um terminal ou prompt de comando.
    *   Execute o comando: `pip show streamlit` ou `conda list streamlit`.
    *   Verifique a versão instalada.

2.  **Atualize o Streamlit (se necessário):**

    *   Se a sua versão for antiga, atualize para a versão mais recente: `pip install --upgrade streamlit` ou `conda update streamlit`.
    *   Após a atualização, verifique a versão novamente para confirmar a atualização.

3.  **Se a atualização não resolver (versão já atualizada ou impossibilidade de atualizar):**

    *   Remova o argumento `use_container_width` das chamadas para `st.image()`. A imagem ainda será exibida, mas pode não ocupar toda a largura do contêiner.
    *   Use CSS personalizado para estilizar a imagem para ocupar a largura do contêiner (opção mais avançada).

**Correções no seu código:**

Remova `use_container_width=True` de todas as chamadas `st.image()`.  Especifique a largura diretamente.  Por exemplo, use `width=None` para permitir que a imagem dimensione automaticamente ou defina uma largura fixa em pixels.

```python
# src\chat_streamlit.py

# ...

# Exibe a imagem selecionada na barra lateral
if st.session_state.clipboard_image_preview:
    st.image(st.session_state.clipboard_image_preview, width=None)
    st.caption("Imagem da área de transferência")
elif st.session_state.uploaded_image:
    st.image(st.session_state.uploaded_image, width=None)
    st.caption("Imagem carregada")

# ...

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], width=None)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# ...
```

**Explicação:**

O erro ocorre porque você está passando um argumento (`use_container_width`) que não é suportado pela função `st.image()` na versão do Streamlit que você está usando.  Remover o argumento é a maneira mais direta de resolver o problema, a menos que você precise desesperadamente que a imagem ocupe toda a largura do contêiner e não possa atualizar o Streamlit.

Se remover o argumento causar problemas de layout, você pode usar CSS personalizado para ajustar o tamanho da imagem. Isso requer um pouco mais de conhecimento de HTML e CSS, mas oferece mais flexibilidade.  Exemplo:

```python
# src\chat_streamlit.py

import streamlit as st

# ...

if st.session_state.uploaded_image:
    st.image(st.session_state.uploaded_image, width=200)  # Largura em pixels

    # CSS para fazer a imagem ocupar a largura do contêiner
    st.markdown(
        """
        <style>
        [data-testid="stImage"] {
            width: 100%; /* Faz a imagem ocupar toda a largura do contêiner */
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

# ...
```

**Resumo:**

A solução mais simples é atualizar o Streamlit. Se isso não for possível, remova o argumento `use_container_width` e ajuste o layout da imagem se necessário.  Se você precisa que a imagem ocupe a largura total do contêiner e não pode atualizar o Streamlit, use CSS personalizado.

2025-06-28 11:58:32,747 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 11:58:32,748 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 11:58:32,749 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 11:58:32,750 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 11:58:32,751 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 11:58:32,752 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 11:58:32,753 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 11:58:32,754 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 11:58:32,756 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 11:58:32,757 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 11:58:32,759 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 11:58:32,760 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 11:58:32,761 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 11:58:32,762 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 11:58:32,763 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:03:07,402 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:03:07,403 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:03:07,405 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:03:07,406 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:03:07,407 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:03:07,408 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:03:07,410 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:03:07,412 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:03:07,413 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:03:07,418 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:03:07,420 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:03:07,423 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:03:07,424 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:03:07,425 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:03:07,426 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:03:13,181 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:03:13,182 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:03:13,183 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:03:13,184 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:03:13,185 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:03:13,186 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:03:13,187 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:03:13,190 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:03:13,192 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:03:13,194 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:03:13,195 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:03:13,197 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:03:13,198 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:03:13,199 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:03:13,200 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:03:16,861 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:03:16,864 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:03:16,865 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:03:16,867 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:03:16,871 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:03:16,873 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:03:16,877 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:03:16,878 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:03:16,883 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:03:16,885 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:03:16,887 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:03:16,893 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:03:16,896 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:03:16,897 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:03:16,899 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:03:45,255 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:03:45,256 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:03:45,257 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:03:45,258 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:03:45,260 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:03:45,261 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:03:45,263 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:03:45,264 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:03:45,266 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:03:45,267 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:03:45,269 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:03:45,271 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:03:45,272 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:03:45,272 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:03:45,273 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:03:45,566 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:03:45,568 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:03:45,569 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:03:45,570 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:03:45,571 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:03:45,572 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:03:45,574 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:03:45,578 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:03:45,582 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:03:45,584 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:03:45,585 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:03:45,586 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:03:45,587 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:03:45,588 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:03:45,589 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:12:42,984 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:12:42,988 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:12:42,991 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:12:42,995 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:12:43,001 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:12:43,004 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:12:43,008 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:12:43,011 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:12:43,018 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:12:43,021 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:12:43,022 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:12:43,026 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:12:43,030 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:12:43,035 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:12:43,039 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:12:48,632 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:12:48,635 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:12:48,636 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:12:48,638 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:12:48,640 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:12:48,649 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:12:48,650 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:12:48,653 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:12:48,655 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:12:48,658 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:12:48,660 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:12:48,663 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:12:48,664 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:12:48,666 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:12:48,669 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:12:52,905 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:12:52,913 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:12:52,969 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:12:52,983 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:12:53,011 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:12:53,030 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:12:53,050 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:12:53,068 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:12:53,081 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:12:53,086 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:12:53,103 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:12:53,115 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:12:53,117 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:12:53,118 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:12:53,120 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:12:58,800 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:12:58,801 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:12:58,802 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:12:58,804 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:12:58,805 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:12:58,806 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:12:58,808 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:12:58,809 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:12:58,810 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:12:58,811 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:12:58,812 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:12:58,817 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:12:58,818 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:12:58,819 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:12:58,819 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:12:59,085 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:12:59,086 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:12:59,087 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:12:59,088 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:12:59,090 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:12:59,091 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:12:59,093 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:12:59,094 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:12:59,095 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:12:59,096 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:12:59,102 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:12:59,103 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:12:59,104 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:12:59,105 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:12:59,106 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:12:59,857 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 12:13:05,615 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 12:13:06,634 [INFO] AFC remote call 1 is done.
2025-06-28 12:13:19,107 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:13:19,111 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:13:19,114 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:13:19,115 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:13:19,117 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:13:19,118 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:13:19,120 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:13:19,121 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:13:19,123 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:13:19,125 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:13:19,126 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:13:19,126 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:13:19,128 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:13:19,129 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:13:19,132 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:13:41,236 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:13:41,238 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:13:41,239 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:13:41,240 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:13:41,241 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:13:41,243 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:13:41,246 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:13:41,247 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:13:41,249 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:13:41,250 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:13:41,252 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:13:41,253 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:13:41,254 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:13:41,255 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:13:41,259 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:13:41,265 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 12:13:44,482 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:13:44,483 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:13:44,484 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:13:44,487 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:13:44,491 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:13:44,493 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:13:44,495 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:13:44,500 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:13:44,501 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:13:44,503 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:13:44,506 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:13:44,507 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:13:44,508 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:13:44,509 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:13:44,510 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:13:45,944 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:13:45,945 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:13:45,946 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:13:45,947 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:13:45,948 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:13:45,950 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:13:45,951 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:13:45,954 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:13:45,956 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:13:45,958 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:13:45,959 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:13:45,960 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:13:45,962 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:13:45,963 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:13:45,969 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:13:46,201 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:13:46,203 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:13:46,204 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:13:46,205 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:13:46,207 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:13:46,210 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:13:46,212 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:13:46,214 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:13:46,216 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:13:46,220 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:13:46,231 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:13:46,232 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:13:46,234 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:13:46,235 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:13:46,241 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:13:51,005 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:13:51,021 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:13:51,022 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:13:51,025 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:13:51,026 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:13:51,036 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:13:51,047 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:13:51,050 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:13:51,067 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:13:51,071 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:13:51,073 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:13:51,077 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:13:51,081 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:13:51,083 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:13:51,085 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:14:15,990 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:14:15,992 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:14:15,993 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:14:15,994 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:14:15,996 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:14:15,998 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:14:15,999 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:14:16,001 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:14:16,002 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:14:16,003 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:14:16,004 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:14:16,007 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:14:16,008 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:14:16,009 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:14:16,010 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:14:16,218 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:14:16,219 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:14:16,220 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:14:16,224 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:14:16,225 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:14:16,228 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:14:16,229 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:14:16,232 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:14:16,238 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:14:16,245 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:14:16,249 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:14:16,252 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:14:16,253 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:14:16,254 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:14:16,258 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:14:16,261 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
          print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# Exemplo de uso (fora do Streamlit):
if __name__ == "__main__":
    image = generate_image("Desenhe um gato astronauta no espaço sideral, estilo cartoon.")
    if image:
        image.show() # Exibe a imagem (opcional)
        image.save("gato_astronauta.png") # Salva a imagem (opcional)
    else:
        print("Falha ao gerar a imagem.")

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: TypeError: ImageMixin.image() got an unexpected keyword argument 'use_container_width'
Traceback:
File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 589, in _run_script
    exec(code, module.__dict__)
File "C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\chat_streamlit.py", line 322, in <module>
    st.image(st.session_state.uploaded_image, use_container_width=True)
File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\metrics_util.py", line 408, in wrapped_func
    result = non_optional_func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

porque tive esse erro após inserir uma imagem da minha máquina nesse aplicação ?
Assistant: O erro `TypeError: ImageMixin.image() got an unexpected keyword argument 'use_container_width'` indica que a função `st.image()` na versão do Streamlit que você está usando não aceita o argumento `use_container_width`. Esse argumento foi introduzido em versões mais recentes do Streamlit.

Aqui estão os passos para resolver o problema:

1.  **Verifique a versão do Streamlit:**

    *   Abra um terminal ou prompt de comando.
    *   Execute o comando: `pip show streamlit` ou `conda list streamlit`.
    *   Verifique a versão instalada.

2.  **Atualize o Streamlit (se necessário):**

    *   Se a sua versão for antiga, atualize para a versão mais recente: `pip install --upgrade streamlit` ou `conda update streamlit`.
    *   Após a atualização, verifique a versão novamente para confirmar a atualização.

3.  **Se a atualização não resolver (versão já atualizada ou impossibilidade de atualizar):**

    *   Remova o argumento `use_container_width` das chamadas para `st.image()`. A imagem ainda será exibida, mas pode não ocupar toda a largura do contêiner.
    *   Use CSS personalizado para estilizar a imagem para ocupar a largura do contêiner (opção mais avançada).

**Correções no seu código:**

Remova `use_container_width=True` de todas as chamadas `st.image()`.  Especifique a largura diretamente.  Por exemplo, use `width=None` para permitir que a imagem dimensione automaticamente ou defina uma largura fixa em pixels.

```python
# src\chat_streamlit.py

# ...

# Exibe a imagem selecionada na barra lateral
if st.session_state.clipboard_image_preview:
    st.image(st.session_state.clipboard_image_preview, width=None)
    st.caption("Imagem da área de transferência")
elif st.session_state.uploaded_image:
    st.image(st.session_state.uploaded_image, width=None)
    st.caption("Imagem carregada")

# ...

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], width=None)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# ...
```

**Explicação:**

O erro ocorre porque você está passando um argumento (`use_container_width`) que não é suportado pela função `st.image()` na versão do Streamlit que você está usando.  Remover o argumento é a maneira mais direta de resolver o problema, a menos que você precise desesperadamente que a imagem ocupe toda a largura do contêiner e não possa atualizar o Streamlit.

Se remover o argumento causar problemas de layout, você pode usar CSS personalizado para ajustar o tamanho da imagem. Isso requer um pouco mais de conhecimento de HTML e CSS, mas oferece mais flexibilidade.  Exemplo:

```python
# src\chat_streamlit.py

import streamlit as st

# ...

if st.session_state.uploaded_image:
    st.image(st.session_state.uploaded_image, width=200)  # Largura em pixels

    # CSS para fazer a imagem ocupar a largura do contêiner
    st.markdown(
        """
        <style>
        [data-testid="stImage"] {
            width: 100%; /* Faz a imagem ocupar toda a largura do contêiner */
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

# ...
```

**Resumo:**

A solução mais simples é atualizar o Streamlit. Se isso não for possível, remova o argumento `use_container_width` e ajuste o layout da imagem se necessário.  Se você precisa que a imagem ocupe a largura total do contêiner e não pode atualizar o Streamlit, use CSS personalizado.

User: em qual arquivo devo alterar ?

TypeError: ImageMixin.image() got an unexpected keyword argument 'use_container_width'
Traceback:
File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 589, in _run_script
    exec(code, module.__dict__)
File "C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\chat_streamlit.py", line 322, in <module>
    st.image(st.session_state.uploaded_image, use_container_width=True)
File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\metrics_util.py", line 408, in wrapped_func
    result = non_optional_func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-06-28 12:14:25,912 [INFO] Resposta da IA (texto): O erro aponta diretamente para o arquivo e a linha onde a chamada problemática está acontecendo:

Você deve alterar o arquivo:
`C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\chat_streamlit.py`

As linhas que precisam ser corrigidas são as que utilizam `use_container_width=True`. No código que você forneceu, encontrei três ocorrências:

1.  **Linha 319:**
    ```python
    st.image(st.session_state.clipboard_image_preview, use_container_width=True)
    ```
2.  **Linha 322:** (Esta é a linha exata que o traceback está apontando)
    ```python
    st.image(st.session_state.uploaded_image, use_container_width=True)
    ```
3.  **Linha 333:**
    ```python
    st.image(message["image"], use_container_width=True)
    ```

**Como corrigir:**

Você precisa remover o argumento `use_container_width=True` e, se desejar, pode substituí-lo por `width=None` (para que o Streamlit tente ajustar a largura automaticamente com base no conteúdo) ou por um valor numérico em pixels (ex: `width=400`) para definir uma largura fixa.

**Altere essas linhas no `chat_streamlit.py`:**

```python
# ... (código anterior)

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        # Altere esta linha:
        st.image(st.session_state.clipboard_image_preview, width=None) # Removido use_container_width=True
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        # Altere esta linha:
        st.image(st.session_state.uploaded_image, width=None) # Removido use_container_width=True
        st.caption("Imagem carregada")

    st.markdown("---")

# ... (código intermediário)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            # Altere esta linha:
            st.image(message["image"], width=None) # Removido use_container_width=True
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# ... (código posterior)
```

**Recomendação:**

Como mencionei antes, a melhor solução a longo prazo é **atualizar sua versão do Streamlit** para uma que suporte o argumento `use_container_width`. Isso geralmente resolve esses problemas de compatibilidade e permite que você use os recursos mais recentes da biblioteca.

Para atualizar, execute no seu terminal (com o ambiente virtual ativado, se estiver usando um):
`pip install --upgrade streamlit`
2025-06-28 12:14:26,081 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:14:26,082 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:14:26,083 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:14:26,084 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:14:26,085 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:14:26,086 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:14:26,087 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:14:26,089 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:14:26,090 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:14:26,092 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:14:26,093 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:14:26,094 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:14:26,096 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:14:26,097 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:14:26,098 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:17:02,053 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:17:02,054 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:17:02,055 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:17:02,056 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:17:02,058 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:17:02,071 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:17:02,072 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:17:02,073 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:17:02,084 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:17:02,086 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:17:02,089 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:17:02,093 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:17:02,164 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:17:02,217 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:17:02,256 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:17:11,805 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:17:11,806 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:17:11,807 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:17:11,808 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:17:11,809 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:17:11,811 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:17:11,813 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:17:11,815 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:17:11,823 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:17:11,825 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:17:11,826 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:17:11,828 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:17:11,831 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:17:11,836 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:17:11,839 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:17:39,425 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:17:39,428 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:17:39,429 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:17:39,430 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:17:39,433 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:17:39,435 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:17:39,436 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:17:39,437 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:17:39,443 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:17:39,444 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:17:39,445 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:17:39,446 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:17:39,447 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:17:39,449 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:17:39,450 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:17:45,340 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:17:45,343 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:17:45,345 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:17:45,346 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:17:45,352 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:17:45,354 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:17:45,357 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:17:45,377 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:17:45,386 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:17:45,388 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:17:45,392 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:17:45,394 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:17:45,396 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:17:45,397 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:17:45,398 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:18:54,156 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:18:54,163 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:18:54,173 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:18:54,178 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:18:54,183 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:18:54,188 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:18:54,191 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:18:54,195 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:18:54,204 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:18:54,209 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:18:54,212 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:18:54,219 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:18:54,223 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:18:54,225 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:18:54,226 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:18:59,176 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:18:59,180 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:18:59,181 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:18:59,182 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:18:59,183 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:18:59,184 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:18:59,187 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:18:59,189 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:18:59,192 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:18:59,199 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:18:59,200 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:18:59,201 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:18:59,202 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:18:59,203 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:18:59,205 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:19:07,776 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:19:07,778 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:19:07,779 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:19:07,780 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:19:07,782 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:19:07,783 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:19:07,784 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:19:07,789 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:19:07,791 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:19:07,792 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:19:07,797 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:19:07,799 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:19:07,809 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:19:07,811 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:19:07,813 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:19:24,445 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:19:24,447 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:19:24,448 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:19:24,452 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:19:24,453 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:19:24,455 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:19:24,457 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:19:24,462 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:19:24,464 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:19:24,465 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:19:24,467 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:19:24,467 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:19:24,469 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:19:24,472 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:19:24,473 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 12:19:26,650 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 12:19:26,651 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 12:19:26,652 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 12:19:26,654 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 12:19:26,655 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 12:19:26,656 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 12:19:26,657 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 12:19:26,660 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 12:19:26,662 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 12:19:26,664 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 12:19:26,666 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 12:19:26,667 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 12:19:26,668 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 12:19:26,669 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 12:19:26,672 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:08:16,207 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:08:16,208 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:08:16,209 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:08:16,210 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:08:16,210 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:08:16,211 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:08:16,212 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:08:16,213 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:08:16,214 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:08:16,216 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:08:16,218 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:08:16,222 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:08:16,225 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:08:16,228 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:08:16,229 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:08:17,161 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 13:08:39,356 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:08:39,357 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:08:39,359 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:08:39,360 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:08:39,361 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:08:39,362 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:08:39,368 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:08:39,371 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:08:39,374 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:08:39,377 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:08:39,378 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:08:39,380 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:08:39,381 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:08:39,387 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:08:39,401 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:08:39,643 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:08:39,644 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:08:39,645 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:08:39,647 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:08:39,656 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:08:39,658 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:08:39,664 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:08:39,675 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:08:39,677 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:08:39,679 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:08:39,685 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:08:39,691 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:08:39,692 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:08:39,694 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:08:39,697 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:08:41,118 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:08:44,997 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:08:46,088 [INFO] AFC remote call 1 is done.
2025-06-28 13:08:52,441 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:08:52,442 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:08:52,444 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:08:52,447 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:08:52,455 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:08:52,457 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:08:52,464 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:08:52,469 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:08:52,470 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:08:52,481 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:08:52,483 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:08:52,484 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:08:52,493 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:08:52,495 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:08:52,499 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:14:17,788 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:14:17,793 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:14:17,796 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:14:17,803 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:14:17,813 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:14:17,824 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:14:17,830 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:14:17,840 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:14:17,844 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:14:17,846 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:14:17,860 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:14:17,902 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:14:17,905 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:14:17,907 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:14:17,909 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:16:40,917 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:16:40,918 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:16:40,919 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:16:40,920 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:16:40,921 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:16:40,922 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:16:40,923 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:16:40,927 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:16:40,929 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:16:40,932 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:16:40,933 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:16:40,935 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:16:40,943 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:16:40,945 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:16:40,948 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:16:41,394 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:16:41,395 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:16:41,396 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:16:41,397 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:16:41,398 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:16:41,399 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:16:41,404 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:16:41,408 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:16:41,409 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:16:41,411 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:16:41,417 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:16:41,419 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:16:41,420 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:16:41,422 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:16:41,427 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:16:41,466 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque essa aplicação apresenta esse erro 

2025-06-28 13:08:52.540 Uncaught app exception
Traceback (most recent call last):
  File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 589, in _run_script
    exec(code, module.__dict__)
  File "C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\chat_streamlit.py", line 322, in <module>
    st.image(st.session_state.uploaded_image, use_container_width=True)
  File "C:\Users\HeannarReis\AppData\Local\Programs\Python\Python312\Lib\site-packages\streamlit\runtime\metrics_util.py", line 408, in wrapped_func
    result = non_optional_func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: ImageMixin.image() got an unexpected keyword argument 'use_container_width'


porém eu programei ela em outra máquina e só fiz o git clone do repositório atualizado.
2025-06-28 13:16:57,566 [INFO] Resposta da IA (texto): O erro que você está enfrentando, `TypeError: ImageMixin.image() got an unexpected keyword argument 'use_container_width'`, indica um problema de **compatibilidade de versão** da biblioteca `streamlit`.

**Entendendo o Erro:**

*   **`ImageMixin.image()`** refere-se à função `st.image()`.
*   **`got an unexpected keyword argument 'use_container_width'`** significa que a versão do Streamlit instalada na sua nova máquina não reconhece ou suporta o argumento `use_container_width=True` que está sendo usado na linha:
    ```python
    st.image(st.session_state.uploaded_image, use_container_width=True)
    ```
    Este argumento foi introduzido em versões mais recentes do Streamlit (a partir da versão 1.10.0, se não me engano). A máquina onde você programou inicialmente provavelmente tem uma versão mais nova, enquanto a máquina onde você clonou o repositório tem uma versão mais antiga do Streamlit.

**Causa Principal:**

*   **Versões diferentes do Streamlit:** Embora o código seja o mesmo (clonado via Git), o ambiente de execução (as bibliotecas Python instaladas) não é. A versão do Streamlit na segunda máquina é mais antiga e não inclui a funcionalidade `use_container_width`.

**Como Resolver:**

A solução mais direta e recomendada é atualizar o Streamlit na máquina onde o erro ocorre.

1.  **Atualizar o Streamlit:**
    Abra o terminal ou prompt de comando na máquina onde o erro está acontecendo e execute:
    ```bash
    pip install --upgrade streamlit
    ```
    Isso instalará a versão mais recente do Streamlit, que certamente suportará `use_container_width`.

2.  **Verificar a Versão (Opcional):**
    Para confirmar as versões, você pode executar em ambas as máquinas:
    ```bash
    pip show streamlit
    ```
    Você verá a versão na saída. A máquina que funciona deve ter uma versão 1.10.0 ou superior.

**Alternativas (Menos recomendadas, mas possíveis):**

Se por algum motivo você não puder atualizar o Streamlit (o que é raro e desaconselhado para este tipo de problema), você teria que adaptar o código:

*   **Remover o argumento:**
    ```python
    st.image(st.session_state.uploaded_image) # A imagem pode não preencher a largura total
    ```
*   **Usar um `width` fixo (em pixels):**
    ```python
    st.image(st.session_state.uploaded_image, width=500) # Exemplo: 500 pixels de largura
    ```
    No entanto, `use_container_width=True` é geralmente a opção preferida por ser responsiva e se ajustar automaticamente ao layout do Streamlit.

**Melhores Práticas para Prevenir Problemas Semelhantes no Futuro:**

Este é um problema clássico de "funciona na minha máquina". Para evitar isso, adote as seguintes práticas:

1.  **Use Ambientes Virtuais (Virtual Environments):**
    Sempre crie e ative um ambiente virtual para cada projeto. Isso isola as dependências do projeto do seu ambiente global Python.
    ```bash
    # Na pasta do seu projeto
    python -m venv venv
    # No Windows
    .\venv\Scripts\activate
    # No macOS/Linux
    source venv/bin/activate
    ```
    Depois de ativar, instale todas as suas dependências dentro deste ambiente.

2.  **Crie um `requirements.txt`:**
    Depois de instalar todas as bibliotecas necessárias e ter certeza de que o projeto funciona, gere um arquivo `requirements.txt` que lista todas as dependências e suas versões exatas:
    ```bash
    pip freeze > requirements.txt
    ```
    Este arquivo deve ser commitado no seu repositório Git.

3.  **Instale Dependências a Partir do `requirements.txt`:**
    Quando você clonar o repositório em uma nova máquina (e tiver ativado seu ambiente virtual), basta executar:
    ```bash
    pip install -r requirements.txt
    ```
    Isso garantirá que exatamente as mesmas versões de todas as bibliotecas sejam instaladas, replicando seu ambiente de desenvolvimento.

Ao seguir essas práticas, você garante que o ambiente de execução seja consistente em todas as máquinas, eliminando problemas como o que você encontrou.
2025-06-28 13:16:57,777 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:16:57,779 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:16:57,780 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:16:57,781 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:16:57,782 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:16:57,783 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:16:57,784 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:16:57,786 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:16:57,788 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:16:57,789 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:16:57,791 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:16:57,795 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:16:57,799 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:16:57,800 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:16:57,802 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:19:42,325 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:19:42,325 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:19:42,326 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:19:42,327 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:19:42,327 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:19:42,327 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:19:42,328 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:19:42,328 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:19:42,329 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:19:42,329 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:19:42,330 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:19:42,330 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:19:42,331 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:19:42,332 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:19:42,333 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:19:42,334 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:19:42,334 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:19:42,335 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:19:42,336 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:19:42,337 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:19:42,337 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:19:42,338 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:19:42,339 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:19:42,339 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:19:42,340 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:19:42,341 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:19:42,341 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:19:42,343 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:19:42,344 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:19:42,347 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:19:42,360 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 13:19:49,182 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:19:49,183 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:19:49,184 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:19:49,185 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:19:49,186 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:19:49,188 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:19:49,191 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:19:49,193 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:19:49,198 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:19:49,200 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:19:49,201 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:19:49,202 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:19:49,204 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:19:49,205 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:19:49,206 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:19:54,528 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:19:54,529 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:19:54,530 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:19:54,531 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:19:54,534 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:19:54,537 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:19:54,542 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:19:54,543 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:19:54,545 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:19:54,546 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:19:54,547 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:19:54,550 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:19:54,552 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:19:54,554 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:19:54,556 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:19:54,796 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:19:54,797 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:19:54,799 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:19:54,800 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:19:54,801 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:19:54,802 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:19:54,803 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:19:54,805 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:19:54,806 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:19:54,808 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:19:54,811 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:19:54,813 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:19:54,814 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:19:54,816 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:19:54,824 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:19:56,107 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:20:01,454 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:20:02,577 [INFO] AFC remote call 1 is done.
2025-06-28 13:20:19,554 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:20:19,561 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:20:19,563 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:20:19,564 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:20:19,569 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:20:19,582 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:20:19,584 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:20:19,586 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:20:19,644 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:20:19,705 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:20:19,712 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:20:19,714 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:20:19,718 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:20:19,719 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:20:19,723 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:20:31,985 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:20:31,986 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:20:31,987 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:20:31,988 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:20:31,990 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:20:31,991 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:20:31,992 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:20:31,994 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:20:31,995 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:20:31,999 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:20:32,002 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:20:32,004 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:20:32,005 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:20:32,006 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:20:32,007 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:20:32,391 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:20:32,393 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:20:32,394 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:20:32,396 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:20:32,404 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:20:32,407 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:20:32,408 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:20:32,410 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:20:32,411 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:20:32,413 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:20:32,414 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:20:32,420 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:20:32,422 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:20:32,423 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:20:32,424 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:20:32,572 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628132032_cybersecurity-hacker-with-a-concealed-face-wearing-a-hoodie-in-a-technology-themed-wallpaper-photo.jpg, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: Descreva essa imagem.
2025-06-28 13:20:41,975 [INFO] Resposta da IA (imagem): A imagem apresenta uma pessoa, provavelmente um homem jovem, usando um capuz escuro que cobre parcialmente seu rosto. Ele olha diretamente para a frente com uma expressão intensa e séria.

O rosto da pessoa é iluminado por uma luz amarela-alaranjada vinda de baixo, criando um contraste dramático com as sombras profundas sob o capuz. Há um elemento notável em seu rosto: linhas luminosas e pontilhadas, de cor âmbar, que descem dos cantos internos dos olhos, assemelhando-se a lágrimas de energia ou dados. Ao redor dessas linhas, há também algumas marcas escuras e borradas, como tinta ou sujeira.

O fundo é composto por uma matriz de luzes azuis-esverdeadas brilhantes e desfocadas, dispostas em padrões que lembram um ambiente digital ou cibernético, reforçando a sensação de tecnologia ou hacking. A iluminação geral confere à imagem uma atmosfera misteriosa e futurista, remetendo a temas de ficção científica ou cyberpunk.
2025-06-28 13:20:42,170 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:20:42,172 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:20:42,173 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:20:42,174 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:20:42,175 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:20:42,177 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:20:42,179 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:20:42,180 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:20:42,182 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:20:42,185 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:20:42,188 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:20:42,190 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:20:42,193 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:20:42,194 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:20:42,195 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:22:30,904 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:22:30,906 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:22:30,907 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:22:30,908 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:22:30,910 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:22:30,911 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:22:30,912 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:22:30,914 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:22:30,916 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:22:30,919 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:22:30,920 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:22:30,922 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:22:30,924 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:22:30,925 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:22:30,926 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:22:31,423 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:22:31,424 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:22:31,425 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:22:31,426 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:22:31,427 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:22:31,429 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:22:31,431 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:22:31,434 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:22:31,437 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:22:31,439 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:22:31,440 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:22:31,441 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:22:31,442 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:22:31,446 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:22:31,451 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:22:40,462 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:22:40,463 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:22:40,464 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:22:40,465 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:22:40,466 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:22:40,467 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:22:40,468 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:22:40,471 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:22:40,473 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:22:40,474 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:22:40,475 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:22:40,475 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:22:40,478 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:22:40,479 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:22:40,481 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:22:40,973 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:22:40,974 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:22:40,975 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:22:40,976 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:22:40,978 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:22:40,979 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:22:40,980 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:22:40,981 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:22:40,983 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:22:40,987 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:22:40,992 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:22:40,993 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:22:40,994 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:22:40,995 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:22:40,996 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:22:41,091 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628132240_clipboard_20250628132231.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: Descreva essa imagem.
Assistant: A imagem apresenta uma pessoa, provavelmente um homem jovem, usando um capuz escuro que cobre parcialmente seu rosto. Ele olha diretamente para a frente com uma expressão intensa e séria.

O rosto da pessoa é iluminado por uma luz amarela-alaranjada vinda de baixo, criando um contraste dramático com as sombras profundas sob o capuz. Há um elemento notável em seu rosto: linhas luminosas e pontilhadas, de cor âmbar, que descem dos cantos internos dos olhos, assemelhando-se a lágrimas de energia ou dados. Ao redor dessas linhas, há também algumas marcas escuras e borradas, como tinta ou sujeira.

O fundo é composto por uma matriz de luzes azuis-esverdeadas brilhantes e desfocadas, dispostas em padrões que lembram um ambiente digital ou cibernético, reforçando a sensação de tecnologia ou hacking. A iluminação geral confere à imagem uma atmosfera misteriosa e futurista, remetendo a temas de ficção científica ou cyberpunk.
User: descreva a imagem enviada.
2025-06-28 13:22:49,068 [INFO] Resposta da IA (imagem): Como descrito anteriormente, a imagem que você enviou apresenta uma pessoa, provavelmente um homem jovem, usando um capuz escuro que cobre parcialmente seu rosto.

Ele olha diretamente para a frente com uma expressão intensa e séria. O rosto é iluminado por uma luz amarela-alaranjada vinda de baixo, com linhas luminosas e pontilhadas, de cor âmbar, que descem dos cantos internos dos olhos, assemelhando-se a lágrimas de energia ou dados.

O fundo é composto por uma matriz de luzes azuis-esverdeadas brilhantes e desfocadas, dispostas em padrões que lembram um ambiente digital ou cibernético, reforçando a sensação de tecnologia ou hacking. A iluminação geral confere à imagem uma atmosfera misteriosa e futurista.
2025-06-28 13:22:49,368 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:22:49,370 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:22:49,371 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:22:49,372 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:22:49,372 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:22:49,373 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:22:49,374 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:22:49,376 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:22:49,377 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:22:49,380 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:22:49,386 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:22:49,387 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:22:49,388 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:22:49,389 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:22:49,390 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:23:03,265 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:23:03,266 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:23:03,267 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:23:03,268 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:23:03,269 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:23:03,270 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:23:03,273 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:23:03,274 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:23:03,282 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:23:03,284 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:23:03,287 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:23:03,288 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:23:03,289 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:23:03,291 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:23:03,293 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:23:03,583 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:23:03,587 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:23:03,589 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:23:03,590 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:23:03,592 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:23:03,594 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:23:03,600 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:23:03,603 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:23:03,605 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:23:03,606 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:23:03,607 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:23:03,608 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:23:03,610 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:23:03,613 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:23:03,619 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:23:13,112 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:23:13,114 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:23:13,115 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:23:13,115 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:23:13,116 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:23:13,118 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:23:13,118 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:23:13,121 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:23:13,123 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:23:13,124 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:23:13,126 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:23:13,127 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:23:13,128 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:23:13,129 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:23:13,129 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:23:13,432 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:23:13,433 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:23:13,434 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:23:13,435 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:23:13,435 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:23:13,436 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:23:13,437 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:23:13,439 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:23:13,440 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:23:13,441 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:23:13,444 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:23:13,448 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:23:13,449 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:23:13,450 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:23:13,451 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:23:13,546 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628132313_clipboard_20250628132231.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: o que tem na imagem ?
2025-06-28 13:23:34,789 [INFO] Resposta da IA (imagem): A imagem que você forneceu é uma captura de tela de uma aplicação de chat construída com Streamlit, que permite interagir com uma inteligência artificial e inclui funcionalidades de processamento de imagens.

### Descrição da Imagem

A imagem principal exibida na tela é um retrato de uma pessoa, provavelmente um homem jovem, usando um capuz escuro que cobre parcialmente seu rosto. Ele olha diretamente para a frente com uma expressão intensa e séria.

*   **Iluminação e Efeitos Visuais:** O rosto da pessoa é dramaticamente iluminado por uma luz amarela-alaranjada que parece vir de baixo ou de uma fonte interna. Isso cria um forte contraste com as sombras profundas sob o capuz e ao redor do rosto. O elemento mais notável são as linhas luminosas e pontilhadas, de cor âmbar (amarelo-alaranjado), que descem dos cantos internos dos olhos, assemelhando-se a lágrimas de energia, dados ou circuitos digitais. Ao redor dessas "lágrimas", há também algumas marcas escuras e borradas, como se fosse tinta escorrendo ou sujeira.
*   **Fundo:** O fundo é composto por padrões abstratos de linhas e pontos luminosos em tons de azul e verde, criando um efeito futurista e digital, semelhante a um código de computador, uma rede de dados ou o interior de um sistema cibernético. Há marcas d'água "Vecteezy" visíveis na imagem.
*   **Temática:** No geral, a imagem evoca temas de hacking, cibersegurança, tecnologia e mistério, com uma atmosfera sombria e intensa.

### Como o Sistema Lida com a Imagem (Baseado no Código Fornecido)

O código Python fornecido detalha como a aplicação Streamlit (`chat_streamlit.py`) e os serviços de IA (`services/gpt_services.py`, `core/handlers/gemini_handler.py`, `services/image_services.py`) interagem para processar e, agora, também **gerar** imagens.

Aqui está um resumo do fluxo:

1.  **Entrada da Imagem (UI - `chat_streamlit.py`):**
    *   **Upload de Arquivo:** O usuário pode carregar uma imagem (`.png`, `.jpg`, `.jpeg`) usando o `st.file_uploader`. O arquivo carregado é armazenado em `st.session_state.uploaded_image`.
    *   **Colar da Área de Transferência:** O botão "📋 Colar" utiliza a função `check_clipboard()` (que tenta acessar `PIL.ImageGrab.grabclipboard()`) para verificar se há uma imagem na área de transferência do sistema. Se encontrada, a imagem é convertida para bytes e armazenada em `st.session_state.clipboard_image_file` e `st.session_state.clipboard_image_preview`.
    *   **Geração de Imagem (NOVO):** Há uma nova seção na barra lateral ("Gerar Imagem") onde o usuário pode digitar um prompt e clicar em "Gerar Imagem". A função `generate_image` (em `chat_streamlit.py`, que chama `services.image_services.generate_image`) utiliza o modelo `gemini-2.0-flash-exp-image-generation` para criar uma imagem a partir do prompt. A imagem gerada é então exibida no histórico de mensagens e pode ser referenciada para futuras perguntas.
    *   **Limpar Imagem:** O botão "🗑️ Limpar" e a função `clear_all_images()` removem qualquer imagem carregada ou colada dos estados da sessão.

2.  **Preparação para o Processamento (`process_message`, `execute_processing` em `chat_streamlit.py`):**
    *   Quando o usuário digita uma pergunta no `st.chat_input` e há uma imagem (carregada, colada ou gerada) disponível (`current_image`), a função `process_message` é chamada.
    *   Esta função marca o estado `processing` como `True` e armazena o prompt e a imagem. Isso força uma reexecução do Streamlit.
    *   Na reexecução, se `processing` for `True`, a função `execute_processing` é ativada.
    *   `execute_processing` primeiramente salva a imagem de entrada (seja do upload, clipboard ou gerada) em um arquivo temporário dentro do diretório `assets` (`Config.ASSETS_DIR`). Isso é necessário porque o `GeminiHandler` trabalha com caminhos de arquivo.
    *   Adiciona a mensagem do usuário (e a imagem, se houver) ao `st.session_state.messages` (o histórico do chat).

3.  **Interação com a IA (`core/handlers/gemini_handler.py`, `services/gpt_services.py`):**
    *   O `GeminiHandler` é instanciado e gerencia a comunicação com a API do Google Gemini.
    *   Ele usa um `RateLimiter` (`core/rate_limiter.py`) para garantir que o número de requisições à API não exceda os limites definidos (ex: 15 requisições por minuto).
    *   A função `gemini_handler.generate_content()` é chamada:
        *   Se um `img_path` (caminho para a imagem temporária) é fornecido, ele invoca `self.handler.generate_content_from_image()`.
        *   `generate_content_from_image` (dentro de `services/gpt_services.py`) lê os bytes da imagem do arquivo temporário e os envia, juntamente com o prompt do usuário, para o modelo Gemini configurado (`gemini-2.5-flash`) usando `model.generate_content([{"mime_type": "image/png", "data": image_bytes}, prompt])`.
        *   Se não houver imagem, ele chama `generate_content_from_text()` para processar apenas o prompt de texto.

4.  **Resposta e Limpeza (`execute_processing` em `chat_streamlit.py`):**
    *   A resposta textual gerada pela IA é recebida.
    *   A resposta é adicionada ao `st.session_state.messages` (histórico do chat).
    *   O arquivo de imagem temporário salvo em `assets` é removido (`os.remove(img_path)`) para economizar espaço e manter o sistema limpo.
    *   O estado `processing` é definido como `False`, liberando o `st.chat_input` para uma nova entrada do usuário.

Em resumo, o sistema permite que o usuário forneça imagens (via upload, colar ou geração) para que o modelo Gemini as analise e responda com base no conteúdo visual e textual, mantendo um histórico da conversa e gerenciando as requisições à API de forma controlada.
2025-06-28 13:23:34,989 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:23:34,990 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:23:34,991 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:23:34,992 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:23:34,993 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:23:34,994 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:23:34,996 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:23:34,997 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:23:34,998 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:23:34,999 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:23:35,000 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:23:35,004 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:23:35,007 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:23:35,008 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:23:35,009 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:24:24,647 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:24:24,650 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:24:24,653 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:24:24,654 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:24:24,657 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:24:24,658 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:24:24,662 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:24:24,668 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:24:24,672 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:24:24,673 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:24:24,674 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:24:24,675 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:24:24,677 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:24:24,678 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:24:24,681 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:24:26,118 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:24:26,119 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:24:26,120 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:24:26,121 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:24:26,122 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:24:26,123 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:24:26,125 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:24:26,127 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:24:26,130 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:24:26,134 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:24:26,156 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:24:26,160 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:24:26,193 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:24:26,194 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:24:26,196 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:24:27,071 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:24:32,807 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:24:34,119 [INFO] AFC remote call 1 is done.
2025-06-28 13:25:04,364 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:25:04,365 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:25:04,366 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:25:04,369 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:25:04,370 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:25:04,372 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:25:04,383 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:25:04,385 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:25:04,387 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:25:04,388 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:25:04,389 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:25:04,391 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:25:04,393 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:25:04,395 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:25:04,401 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:25:04,720 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:25:04,721 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:25:04,722 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:25:04,723 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:25:04,724 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:25:04,725 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:25:04,734 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:25:04,738 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:25:04,739 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:25:04,740 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:25:04,740 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:25:04,741 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:25:04,742 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:25:04,746 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:25:04,808 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:26:13,809 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:26:13,810 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:26:13,811 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:26:13,812 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:26:13,822 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:26:13,824 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:26:13,827 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:26:13,829 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:26:13,830 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:26:13,831 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:26:13,832 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:26:13,833 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:26:13,844 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:26:13,877 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:26:13,888 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:26:14,403 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:26:14,404 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:26:14,405 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:26:14,406 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:26:14,407 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:26:14,408 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:26:14,409 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:26:14,412 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:26:14,413 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:26:14,416 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:26:14,420 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:26:14,421 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:26:14,422 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:26:14,423 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:26:14,424 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:26:14,474 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628132614_clipboard_20250628132504.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: o que tem na imagem ?
Assistant: A imagem que você forneceu é uma captura de tela de uma aplicação de chat construída com Streamlit, que permite interagir com uma inteligência artificial e inclui funcionalidades de processamento de imagens.

### Descrição da Imagem

A imagem principal exibida na tela é um retrato de uma pessoa, provavelmente um homem jovem, usando um capuz escuro que cobre parcialmente seu rosto. Ele olha diretamente para a frente com uma expressão intensa e séria.

*   **Iluminação e Efeitos Visuais:** O rosto da pessoa é dramaticamente iluminado por uma luz amarela-alaranjada que parece vir de baixo ou de uma fonte interna. Isso cria um forte contraste com as sombras profundas sob o capuz e ao redor do rosto. O elemento mais notável são as linhas luminosas e pontilhadas, de cor âmbar (amarelo-alaranjado), que descem dos cantos internos dos olhos, assemelhando-se a lágrimas de energia, dados ou circuitos digitais. Ao redor dessas "lágrimas", há também algumas marcas escuras e borradas, como se fosse tinta escorrendo ou sujeira.
*   **Fundo:** O fundo é composto por padrões abstratos de linhas e pontos luminosos em tons de azul e verde, criando um efeito futurista e digital, semelhante a um código de computador, uma rede de dados ou o interior de um sistema cibernético. Há marcas d'água "Vecteezy" visíveis na imagem.
*   **Temática:** No geral, a imagem evoca temas de hacking, cibersegurança, tecnologia e mistério, com uma atmosfera sombria e intensa.

### Como o Sistema Lida com a Imagem (Baseado no Código Fornecido)

O código Python fornecido detalha como a aplicação Streamlit (`chat_streamlit.py`) e os serviços de IA (`services/gpt_services.py`, `core/handlers/gemini_handler.py`, `services/image_services.py`) interagem para processar e, agora, também **gerar** imagens.

Aqui está um resumo do fluxo:

1.  **Entrada da Imagem (UI - `chat_streamlit.py`):**
    *   **Upload de Arquivo:** O usuário pode carregar uma imagem (`.png`, `.jpg`, `.jpeg`) usando o `st.file_uploader`. O arquivo carregado é armazenado em `st.session_state.uploaded_image`.
    *   **Colar da Área de Transferência:** O botão "📋 Colar" utiliza a função `check_clipboard()` (que tenta acessar `PIL.ImageGrab.grabclipboard()`) para verificar se há uma imagem na área de transferência do sistema. Se encontrada, a imagem é convertida para bytes e armazenada em `st.session_state.clipboard_image_file` e `st.session_state.clipboard_image_preview`.
    *   **Geração de Imagem (NOVO):** Há uma nova seção na barra lateral ("Gerar Imagem") onde o usuário pode digitar um prompt e clicar em "Gerar Imagem". A função `generate_image` (em `chat_streamlit.py`, que chama `services.image_services.generate_image`) utiliza o modelo `gemini-2.0-flash-exp-image-generation` para criar uma imagem a partir do prompt. A imagem gerada é então exibida no histórico de mensagens e pode ser referenciada para futuras perguntas.
    *   **Limpar Imagem:** O botão "🗑️ Limpar" e a função `clear_all_images()` removem qualquer imagem carregada ou colada dos estados da sessão.

2.  **Preparação para o Processamento (`process_message`, `execute_processing` em `chat_streamlit.py`):**
    *   Quando o usuário digita uma pergunta no `st.chat_input` e há uma imagem (carregada, colada ou gerada) disponível (`current_image`), a função `process_message` é chamada.
    *   Esta função marca o estado `processing` como `True` e armazena o prompt e a imagem. Isso força uma reexecução do Streamlit.
    *   Na reexecução, se `processing` for `True`, a função `execute_processing` é ativada.
    *   `execute_processing` primeiramente salva a imagem de entrada (seja do upload, clipboard ou gerada) em um arquivo temporário dentro do diretório `assets` (`Config.ASSETS_DIR`). Isso é necessário porque o `GeminiHandler` trabalha com caminhos de arquivo.
    *   Adiciona a mensagem do usuário (e a imagem, se houver) ao `st.session_state.messages` (o histórico do chat).

3.  **Interação com a IA (`core/handlers/gemini_handler.py`, `services/gpt_services.py`):**
    *   O `GeminiHandler` é instanciado e gerencia a comunicação com a API do Google Gemini.
    *   Ele usa um `RateLimiter` (`core/rate_limiter.py`) para garantir que o número de requisições à API não exceda os limites definidos (ex: 15 requisições por minuto).
    *   A função `gemini_handler.generate_content()` é chamada:
        *   Se um `img_path` (caminho para a imagem temporária) é fornecido, ele invoca `self.handler.generate_content_from_image()`.
        *   `generate_content_from_image` (dentro de `services/gpt_services.py`) lê os bytes da imagem do arquivo temporário e os envia, juntamente com o prompt do usuário, para o modelo Gemini configurado (`gemini-2.5-flash`) usando `model.generate_content([{"mime_type": "image/png", "data": image_bytes}, prompt])`.
        *   Se não houver imagem, ele chama `generate_content_from_text()` para processar apenas o prompt de texto.

4.  **Resposta e Limpeza (`execute_processing` em `chat_streamlit.py`):**
    *   A resposta textual gerada pela IA é recebida.
    *   A resposta é adicionada ao `st.session_state.messages` (histórico do chat).
    *   O arquivo de imagem temporário salvo em `assets` é removido (`os.remove(img_path)`) para economizar espaço e manter o sistema limpo.
    *   O estado `processing` é definido como `False`, liberando o `st.chat_input` para uma nova entrada do usuário.

Em resumo, o sistema permite que o usuário forneça imagens (via upload, colar ou geração) para que o modelo Gemini as analise e responda com base no conteúdo visual e textual, mantendo um histórico da conversa e gerenciando as requisições à API de forma controlada.
User: a imagem representa o erro que está ocorrendo na aplicação que estou enviando o código.

qual Lib pode ser necessário installar ?

pois eu apenas fiz o git clone do mesmo respositório que estou usando em outro notebook na minha casa e lá funciona com o mesmo código. e não gera esse erro.
2025-06-28 13:26:29,705 [INFO] Resposta da IA (imagem): O erro `Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x00000273AAB653A0>` indica que a biblioteca responsável por processar (neste caso, abrir) os dados de imagem recebidos da API do Gemini não conseguiu reconhecer o formato do arquivo.

Analisando o seu código, a linha crítica está na função `generate_image` dentro de `chat_streamlit.py`:

```python
                elif part.inline_data is not None:
                    image = Image.open(io.BytesIO(part.inline_data.data)) # <-- Acontece aqui
                    st.session_state.generated_image = image
                    return image
```

Essa linha utiliza a biblioteca `Pillow` (ou PIL) para converter os dados brutos da imagem (que vêm em um objeto `io.BytesIO`) em um objeto de imagem que pode ser exibido pelo Streamlit (`st.image`).

Se funciona em um notebook e não em outro com o mesmo código, a causa mais provável é uma **discrepância no ambiente de Python**, especificamente na instalação da biblioteca `Pillow` ou em suas dependências subjacentes.

### Lib Pode ser Necessário Instalar (ou Reinstalar)

A biblioteca principal envolvida neste erro é a **Pillow**.

**Passos para Resolver:**

1.  **Reinstalar Pillow:**
    É possível que a instalação do Pillow esteja corrompida ou incompleta no notebook onde o erro ocorre. Tente desinstalá-la e reinstalá-la:

    ```bash
    pip uninstall Pillow
    pip install Pillow
    ```

    Ou, para garantir que você está usando a versão mais recente e que todas as dependências são resolvidas:

    ```bash
    pip install --upgrade Pillow
    ```

2.  **Garantir Consistência de Ambiente (Melhor Prática):**
    A forma mais robusta de garantir que seu ambiente de desenvolvimento seja idêntico em todas as máquinas é usando um arquivo `requirements.txt`.

    *   **No notebook onde funciona (o "bom" ambiente):**
        Abra o terminal na pasta raiz do seu projeto (ou no ambiente virtual que você usa para o projeto) e execute:
        ```bash
        pip freeze > requirements.txt
        ```
        Isso criará um arquivo `requirements.txt` com todas as bibliotecas e suas versões exatas que estão instaladas e funcionando.

    *   **No notebook onde não funciona:**
        Primeiro, garanta que você está usando um ambiente virtual (é sempre uma boa prática). Se não estiver usando um, crie um e ative-o.
        ```bash
        python -m venv venv_project  # Cria um ambiente virtual (nome 'venv_project')
        source venv_project/bin/activate # No Linux/macOS
        # Ou venv_project\Scripts\activate # No Windows
        ```
        Em seguida, navegue até a pasta onde está o `requirements.txt` (que você gerou no outro notebook) e instale tudo:
        ```bash
        pip install -r requirements.txt
        ```
        Isso garantirá que você tenha *exatamente* as mesmas versões de todas as bibliotecas, incluindo `Pillow` e `google-generativeai`, que funcionam na sua outra máquina.

3.  **Verificar a Instalação do `google-generativeai`:**
    Embora o erro aponte mais para `Pillow`, o `google-generativeai` também está envolvido. Certifique-se de que ele esteja instalado corretamente.
    ```bash
    pip install google-generativeai
    ```
    Ou via `requirements.txt` como mencionado acima.

4.  **Verificar Versão do Python:**
    Confirme se a versão do Python é a mesma ou compatível nos dois notebooks. Diferenças significativas podem levar a problemas de compatibilidade com algumas bibliotecas.

5.  **Dependências do Sistema Operacional (Menos Comum para Pillow em geral, mas possível):**
    Em alguns sistemas Linux, bibliotecas de sistema para imagem (como `libjpeg-dev`, `zlib1g-dev`, `libpng-dev`) podem precisar ser instaladas antes que `Pillow` possa ser totalmente compilada com suporte a todos os formatos. Se você estiver usando Linux, tente:
    ```bash
    sudo apt-get update
    sudo apt-get install libjpeg-dev zlib1g-dev libpng-dev
    ```
    E então reinstale o `Pillow`.

**Resumo da Causa Provável:**

O erro `cannot identify image file <_io.BytesIO object` quase sempre significa que a biblioteca `Pillow` (ou uma de suas dependências internas para lidar com formatos de imagem) não está funcionando corretamente para decodificar os bytes da imagem que ela recebeu. A consistência do ambiente via `requirements.txt` é a solução mais confiável para esse tipo de problema quando o código funciona em outro lugar.
2025-06-28 13:26:29,931 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:26:29,932 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:26:29,933 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:26:29,935 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:26:29,936 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:26:29,938 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:26:29,939 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:26:29,950 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:26:29,952 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:26:29,953 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:26:29,955 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:26:29,956 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:26:29,959 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:26:29,965 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:26:29,970 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:28:47,295 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:28:47,296 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:28:47,299 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:28:47,301 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:28:47,302 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:28:47,307 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:28:47,309 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:28:47,311 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:28:47,314 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:28:47,315 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:28:47,316 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:28:47,318 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:28:47,319 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:28:47,321 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:28:47,321 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:28:47,673 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:28:47,676 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:28:47,677 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:28:47,678 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:28:47,679 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:28:47,681 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:28:47,685 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:28:47,687 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:28:47,689 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:28:47,690 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:28:47,692 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:28:47,694 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:28:47,696 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:28:47,697 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:28:47,698 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:28:48,536 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:28:54,885 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:28:55,811 [INFO] AFC remote call 1 is done.
2025-06-28 13:29:41,733 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:29:41,734 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:29:41,735 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:29:41,736 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:29:41,737 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:29:41,739 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:29:41,741 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:29:41,743 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:29:41,751 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:29:41,752 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:29:41,753 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:29:41,756 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:29:41,757 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:29:41,758 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:29:41,759 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:29:42,073 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:29:42,074 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:29:42,075 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:29:42,076 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:29:42,077 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:29:42,078 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:29:42,080 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:29:42,082 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:29:42,084 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:29:42,090 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:29:42,092 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:29:42,094 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:29:42,095 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:29:42,097 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:29:42,098 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:29:42,133 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628132942_clipboard_20250628132504.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: o que tem na imagem ?
Assistant: A imagem que você forneceu é uma captura de tela de uma aplicação de chat construída com Streamlit, que permite interagir com uma inteligência artificial e inclui funcionalidades de processamento de imagens.

### Descrição da Imagem

A imagem principal exibida na tela é um retrato de uma pessoa, provavelmente um homem jovem, usando um capuz escuro que cobre parcialmente seu rosto. Ele olha diretamente para a frente com uma expressão intensa e séria.

*   **Iluminação e Efeitos Visuais:** O rosto da pessoa é dramaticamente iluminado por uma luz amarela-alaranjada que parece vir de baixo ou de uma fonte interna. Isso cria um forte contraste com as sombras profundas sob o capuz e ao redor do rosto. O elemento mais notável são as linhas luminosas e pontilhadas, de cor âmbar (amarelo-alaranjado), que descem dos cantos internos dos olhos, assemelhando-se a lágrimas de energia, dados ou circuitos digitais. Ao redor dessas "lágrimas", há também algumas marcas escuras e borradas, como se fosse tinta escorrendo ou sujeira.
*   **Fundo:** O fundo é composto por padrões abstratos de linhas e pontos luminosos em tons de azul e verde, criando um efeito futurista e digital, semelhante a um código de computador, uma rede de dados ou o interior de um sistema cibernético. Há marcas d'água "Vecteezy" visíveis na imagem.
*   **Temática:** No geral, a imagem evoca temas de hacking, cibersegurança, tecnologia e mistério, com uma atmosfera sombria e intensa.

### Como o Sistema Lida com a Imagem (Baseado no Código Fornecido)

O código Python fornecido detalha como a aplicação Streamlit (`chat_streamlit.py`) e os serviços de IA (`services/gpt_services.py`, `core/handlers/gemini_handler.py`, `services/image_services.py`) interagem para processar e, agora, também **gerar** imagens.

Aqui está um resumo do fluxo:

1.  **Entrada da Imagem (UI - `chat_streamlit.py`):**
    *   **Upload de Arquivo:** O usuário pode carregar uma imagem (`.png`, `.jpg`, `.jpeg`) usando o `st.file_uploader`. O arquivo carregado é armazenado em `st.session_state.uploaded_image`.
    *   **Colar da Área de Transferência:** O botão "📋 Colar" utiliza a função `check_clipboard()` (que tenta acessar `PIL.ImageGrab.grabclipboard()`) para verificar se há uma imagem na área de transferência do sistema. Se encontrada, a imagem é convertida para bytes e armazenada em `st.session_state.clipboard_image_file` e `st.session_state.clipboard_image_preview`.
    *   **Geração de Imagem (NOVO):** Há uma nova seção na barra lateral ("Gerar Imagem") onde o usuário pode digitar um prompt e clicar em "Gerar Imagem". A função `generate_image` (em `chat_streamlit.py`, que chama `services.image_services.generate_image`) utiliza o modelo `gemini-2.0-flash-exp-image-generation` para criar uma imagem a partir do prompt. A imagem gerada é então exibida no histórico de mensagens e pode ser referenciada para futuras perguntas.
    *   **Limpar Imagem:** O botão "🗑️ Limpar" e a função `clear_all_images()` removem qualquer imagem carregada ou colada dos estados da sessão.

2.  **Preparação para o Processamento (`process_message`, `execute_processing` em `chat_streamlit.py`):**
    *   Quando o usuário digita uma pergunta no `st.chat_input` e há uma imagem (carregada, colada ou gerada) disponível (`current_image`), a função `process_message` é chamada.
    *   Esta função marca o estado `processing` como `True` e armazena o prompt e a imagem. Isso força uma reexecução do Streamlit.
    *   Na reexecução, se `processing` for `True`, a função `execute_processing` é ativada.
    *   `execute_processing` primeiramente salva a imagem de entrada (seja do upload, clipboard ou gerada) em um arquivo temporário dentro do diretório `assets` (`Config.ASSETS_DIR`). Isso é necessário porque o `GeminiHandler` trabalha com caminhos de arquivo.
    *   Adiciona a mensagem do usuário (e a imagem, se houver) ao `st.session_state.messages` (o histórico do chat).

3.  **Interação com a IA (`core/handlers/gemini_handler.py`, `services/gpt_services.py`):**
    *   O `GeminiHandler` é instanciado e gerencia a comunicação com a API do Google Gemini.
    *   Ele usa um `RateLimiter` (`core/rate_limiter.py`) para garantir que o número de requisições à API não exceda os limites definidos (ex: 15 requisições por minuto).
    *   A função `gemini_handler.generate_content()` é chamada:
        *   Se um `img_path` (caminho para a imagem temporária) é fornecido, ele invoca `self.handler.generate_content_from_image()`.
        *   `generate_content_from_image` (dentro de `services/gpt_services.py`) lê os bytes da imagem do arquivo temporário e os envia, juntamente com o prompt do usuário, para o modelo Gemini configurado (`gemini-2.5-flash`) usando `model.generate_content([{"mime_type": "image/png", "data": image_bytes}, prompt])`.
        *   Se não houver imagem, ele chama `generate_content_from_text()` para processar apenas o prompt de texto.

4.  **Resposta e Limpeza (`execute_processing` em `chat_streamlit.py`):**
    *   A resposta textual gerada pela IA é recebida.
    *   A resposta é adicionada ao `st.session_state.messages` (histórico do chat).
    *   O arquivo de imagem temporário salvo em `assets` é removido (`os.remove(img_path)`) para economizar espaço e manter o sistema limpo.
    *   O estado `processing` é definido como `False`, liberando o `st.chat_input` para uma nova entrada do usuário.

Em resumo, o sistema permite que o usuário forneça imagens (via upload, colar ou geração) para que o modelo Gemini as analise e responda com base no conteúdo visual e textual, mantendo um histórico da conversa e gerenciando as requisições à API de forma controlada.
User: a imagem representa o erro que está ocorrendo na aplicação que estou enviando o código.

qual Lib pode ser necessário installar ?

pois eu apenas fiz o git clone do mesmo respositório que estou usando em outro notebook na minha casa e lá funciona com o mesmo código. e não gera esse erro.
Assistant: O erro `Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x00000273AAB653A0>` indica que a biblioteca responsável por processar (neste caso, abrir) os dados de imagem recebidos da API do Gemini não conseguiu reconhecer o formato do arquivo.

Analisando o seu código, a linha crítica está na função `generate_image` dentro de `chat_streamlit.py`:

```python
                elif part.inline_data is not None:
                    image = Image.open(io.BytesIO(part.inline_data.data)) # <-- Acontece aqui
                    st.session_state.generated_image = image
                    return image
```

Essa linha utiliza a biblioteca `Pillow` (ou PIL) para converter os dados brutos da imagem (que vêm em um objeto `io.BytesIO`) em um objeto de imagem que pode ser exibido pelo Streamlit (`st.image`).

Se funciona em um notebook e não em outro com o mesmo código, a causa mais provável é uma **discrepância no ambiente de Python**, especificamente na instalação da biblioteca `Pillow` ou em suas dependências subjacentes.

### Lib Pode ser Necessário Instalar (ou Reinstalar)

A biblioteca principal envolvida neste erro é a **Pillow**.

**Passos para Resolver:**

1.  **Reinstalar Pillow:**
    É possível que a instalação do Pillow esteja corrompida ou incompleta no notebook onde o erro ocorre. Tente desinstalá-la e reinstalá-la:

    ```bash
    pip uninstall Pillow
    pip install Pillow
    ```

    Ou, para garantir que você está usando a versão mais recente e que todas as dependências são resolvidas:

    ```bash
    pip install --upgrade Pillow
    ```

2.  **Garantir Consistência de Ambiente (Melhor Prática):**
    A forma mais robusta de garantir que seu ambiente de desenvolvimento seja idêntico em todas as máquinas é usando um arquivo `requirements.txt`.

    *   **No notebook onde funciona (o "bom" ambiente):**
        Abra o terminal na pasta raiz do seu projeto (ou no ambiente virtual que você usa para o projeto) e execute:
        ```bash
        pip freeze > requirements.txt
        ```
        Isso criará um arquivo `requirements.txt` com todas as bibliotecas e suas versões exatas que estão instaladas e funcionando.

    *   **No notebook onde não funciona:**
        Primeiro, garanta que você está usando um ambiente virtual (é sempre uma boa prática). Se não estiver usando um, crie um e ative-o.
        ```bash
        python -m venv venv_project  # Cria um ambiente virtual (nome 'venv_project')
        source venv_project/bin/activate # No Linux/macOS
        # Ou venv_project\Scripts\activate # No Windows
        ```
        Em seguida, navegue até a pasta onde está o `requirements.txt` (que você gerou no outro notebook) e instale tudo:
        ```bash
        pip install -r requirements.txt
        ```
        Isso garantirá que você tenha *exatamente* as mesmas versões de todas as bibliotecas, incluindo `Pillow` e `google-generativeai`, que funcionam na sua outra máquina.

3.  **Verificar a Instalação do `google-generativeai`:**
    Embora o erro aponte mais para `Pillow`, o `google-generativeai` também está envolvido. Certifique-se de que ele esteja instalado corretamente.
    ```bash
    pip install google-generativeai
    ```
    Ou via `requirements.txt` como mencionado acima.

4.  **Verificar Versão do Python:**
    Confirme se a versão do Python é a mesma ou compatível nos dois notebooks. Diferenças significativas podem levar a problemas de compatibilidade com algumas bibliotecas.

5.  **Dependências do Sistema Operacional (Menos Comum para Pillow em geral, mas possível):**
    Em alguns sistemas Linux, bibliotecas de sistema para imagem (como `libjpeg-dev`, `zlib1g-dev`, `libpng-dev`) podem precisar ser instaladas antes que `Pillow` possa ser totalmente compilada com suporte a todos os formatos. Se você estiver usando Linux, tente:
    ```bash
    sudo apt-get update
    sudo apt-get install libjpeg-dev zlib1g-dev libpng-dev
    ```
    E então reinstale o `Pillow`.

**Resumo da Causa Provável:**

O erro `cannot identify image file <_io.BytesIO object` quase sempre significa que a biblioteca `Pillow` (ou uma de suas dependências internas para lidar com formatos de imagem) não está funcionando corretamente para decodificar os bytes da imagem que ela recebeu. A consistência do ambiente via `requirements.txt` é a solução mais confiável para esse tipo de problema quando o código funciona em outro lugar.
User: preciso dar um restart  na aplicação ?

porque no log do vscode chega essa mensagem de 200 OK 

2025-06-28 13:28:47,694 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:28:47,696 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:28:47,697 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:28:47,698 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:28:48,536 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:28:54,885 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent 
"HTTP/1.1 200 OK"
2025-06-28 13:28:55,811 [INFO] AFC remote call 1 is done.
A imagem mostrará um homem de 40 anos, no estilo Pixar, usando óculos e com cabelo castanho médio. Ele terá barba e bigode baixos e ralos cobrindo todo o rosto. O homem estará localizado em um campo de futebol vibrante, repleto de uma multidão animada de torcedores ao fundo.

2025-06-28 13:30:00,076 [INFO] Resposta da IA (imagem): Sim, você **precisa reiniciar a aplicação Streamlit** após fazer qualquer alteração ou reinstalação de bibliotecas Python.

**Por que você precisa reiniciar a aplicação, mesmo com "200 OK"?**

O log `HTTP/1.1 200 OK` é uma informação crucial e muito útil! Ele significa que:

1.  **A comunicação com a API do Gemini foi um sucesso:** Seu código conseguiu enviar o prompt para o Gemini.
2.  **O Gemini gerou a imagem e a enviou de volta:** O `200 OK` confirma que os dados da imagem foram transmitidos da Google para o seu ambiente.

O erro `cannot identify image file <_io.BytesIO object at 0x...>` ocorre **APÓS** a recepção desses dados. Ele acontece especificamente nesta linha do seu `chat_streamlit.py`:

```python
                elif part.inline_data is not None:
                    image = Image.open(io.BytesIO(part.inline_data.data)) # <-- O erro acontece aqui
                    st.session_state.generated_image = image
                    return image
```

Esta linha está tentando usar a biblioteca `Pillow` (através de `Image.open()`) para **interpretar os bytes da imagem** que foram recebidos da API do Gemini.

**A causa provável, reforçada pelo seu log, é que a sua instalação da biblioteca `Pillow` está de alguma forma corrompida ou incompleta no ambiente onde você está rodando a aplicação.** Ela recebeu os dados, mas não consegue "entender" o que são esses dados para transformá-los em uma imagem manipulável.

**Quando você instala ou reinstala uma biblioteca Python (`pip install ...`), o processo em execução do Streamlit (ou qualquer outro script Python) não "percebe" essas mudanças automaticamente.** Ele já carregou as bibliotecas em sua memória no momento em que foi iniciado. Para que as novas versões ou instalações corrigidas sejam utilizadas, você precisa:

1.  **Parar o Streamlit que está rodando** (geralmente pressionando `Ctrl+C` no terminal onde ele foi iniciado).
2.  **Executar os comandos de instalação/atualização da biblioteca `Pillow`** (conforme sugeri na resposta anterior, sendo `pip install --upgrade Pillow` o mais direto).
3.  **Iniciar a aplicação Streamlit novamente** (`streamlit run chat_streamlit.py`).

**Ações recomendadas, com base nas novas informações:**

1.  **Pare a aplicação Streamlit:** Vá para o terminal onde o Streamlit está rodando e pressione `Ctrl+C`.
2.  **Reinstale/Atualize o Pillow (novamente, para garantir):**
    ```bash
    pip install --upgrade Pillow
    ```
    Ou, se você criou o `requirements.txt` no seu outro notebook:
    ```bash
    pip install -r requirements.txt
    ```
3.  **Reinicie o Streamlit:**
    ```bash
    streamlit run chat_streamlit.py
    ```

Isso deve resolver o problema, pois garante que o ambiente Python que o Streamlit usa esteja com a versão funcional e completa do `Pillow` carregada.
2025-06-28 13:30:00,242 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:30:00,245 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:30:00,246 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:30:00,247 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:30:00,248 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:30:00,249 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:30:00,251 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:30:00,252 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:30:00,294 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:30:00,329 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:30:00,367 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:30:00,395 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:30:00,408 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:30:00,412 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:30:00,415 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:31:03,308 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:31:03,309 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:31:03,310 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:31:03,312 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:31:03,313 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:31:03,315 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:31:03,317 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:31:03,319 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:31:03,321 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:31:03,322 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:31:03,323 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:31:03,324 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:31:03,326 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:31:03,332 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:31:03,333 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:31:04,083 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 13:31:09,710 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:31:09,712 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:31:09,714 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:31:09,715 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:31:09,717 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:31:09,720 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:31:09,726 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:31:09,729 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:31:09,730 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:31:09,734 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:31:09,789 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:31:09,810 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:31:09,813 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:31:09,815 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:31:09,985 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:31:10,044 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:31:10,284 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:31:10,285 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:31:10,286 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:31:10,287 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:31:10,287 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:31:10,288 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:31:10,289 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:31:10,290 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:31:10,291 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:31:10,293 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:31:10,295 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:31:10,297 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:31:10,299 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:31:10,299 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:31:11,703 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:31:16,122 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:31:17,573 [INFO] AFC remote call 1 is done.
2025-06-28 13:33:22,602 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:33:22,603 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:33:22,604 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:33:22,605 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:33:22,606 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:33:22,607 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:33:22,607 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:33:22,612 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:33:22,613 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:33:22,614 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:33:22,615 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:33:22,616 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:33:22,617 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:33:22,618 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:33:22,620 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:33:23,304 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 13:33:26,291 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:33:26,295 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:33:26,296 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:33:26,298 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:33:26,304 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:33:26,305 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:33:26,314 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:33:26,315 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:33:26,320 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:33:26,323 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:33:26,325 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:33:26,326 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:33:26,328 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:33:26,349 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:33:26,361 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:33:28,767 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:33:28,769 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:33:28,770 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:33:28,771 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:33:28,772 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:33:28,773 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:33:28,774 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:33:28,775 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:33:28,776 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:33:28,779 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:33:28,785 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:33:28,787 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:33:28,789 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:33:28,790 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:33:28,791 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:33:30,065 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:33:34,878 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:33:34,883 [INFO] AFC remote call 1 is done.
2025-06-28 13:33:44,920 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:33:44,921 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:33:44,923 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:33:44,924 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:33:44,925 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:33:44,929 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:33:44,931 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:33:44,939 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:33:44,941 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:33:44,942 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:33:44,943 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:33:44,945 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:33:44,947 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:33:44,950 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:33:44,958 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:33:45,181 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:33:45,182 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:33:45,183 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:33:45,183 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:33:45,184 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:33:45,185 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:33:45,186 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:33:45,187 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:33:45,190 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:33:45,191 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:33:45,193 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:33:45,196 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:33:45,201 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:33:45,204 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:33:45,207 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:33:46,017 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:33:49,592 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:33:50,717 [INFO] AFC remote call 1 is done.
2025-06-28 13:34:31,093 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:34:31,094 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:34:31,094 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:34:31,095 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:34:31,096 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:34:31,096 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:34:31,097 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:34:31,098 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:34:31,099 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:34:31,100 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:34:31,101 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:34:31,104 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:34:31,106 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:34:31,107 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:34:31,108 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:34:31,912 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 13:34:35,264 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:34:35,267 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:34:35,269 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:34:35,282 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:34:35,295 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:34:35,297 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:34:35,302 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:34:35,306 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:34:35,308 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:34:35,309 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:34:35,310 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:34:35,328 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:34:35,341 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:34:35,342 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:34:35,343 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:34:35,576 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:34:35,577 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:34:35,578 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:34:35,579 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:34:35,580 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:34:35,581 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:34:35,582 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:34:35,583 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:34:35,586 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:34:35,590 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:34:35,592 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:34:35,593 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:34:35,594 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:34:35,595 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:34:35,596 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:34:36,756 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:34:40,485 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:34:41,475 [INFO] AFC remote call 1 is done.
2025-06-28 13:42:40,066 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:42:40,067 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:42:40,068 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:42:40,069 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:42:40,070 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:42:40,070 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:42:40,072 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:42:40,074 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:42:40,075 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:42:40,077 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:42:40,078 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:42:40,079 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:42:40,081 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:42:40,081 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:42:40,082 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:42:40,800 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:42:44,644 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 500 Internal Server Error"
2025-06-28 13:43:27,211 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:43:27,212 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:43:27,213 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:43:27,214 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:43:27,215 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:43:27,216 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:43:27,217 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:43:27,218 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:43:27,219 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:43:27,220 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:43:27,221 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:43:27,222 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:43:27,223 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:43:27,224 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:43:27,224 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:43:27,883 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 13:43:33,073 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:43:33,074 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:43:33,075 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:43:33,076 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:43:33,077 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:43:33,078 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:43:33,079 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:43:33,080 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:43:33,082 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:43:33,085 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:43:33,092 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:43:33,092 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:43:33,094 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:43:33,095 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:43:33,096 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:43:56,165 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:43:56,166 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:43:56,168 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:43:56,169 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:43:56,170 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:43:56,171 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:43:56,172 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:43:56,173 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:43:56,175 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:43:56,179 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:43:56,186 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:43:56,189 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:43:56,190 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:43:56,191 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:43:56,192 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:43:56,406 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:43:56,407 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:43:56,408 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:43:56,409 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:43:56,410 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:43:56,411 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:43:56,413 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:43:56,418 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:43:56,421 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:43:56,423 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:43:56,424 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:43:56,425 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:43:56,426 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:43:56,428 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:43:56,432 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:43:56,438 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: resuma a aplicação enviada
2025-06-28 13:44:03,407 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:44:03,410 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:44:03,411 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:44:03,413 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:44:03,415 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:44:03,418 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:44:03,434 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:44:03,435 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:44:03,436 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:44:03,438 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:44:03,439 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:44:03,445 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:44:03,449 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:44:03,452 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:44:03,454 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:44:04,245 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:44:04,247 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:44:04,248 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:44:04,249 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:44:04,250 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:44:04,251 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:44:04,253 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:44:04,264 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:44:04,265 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:44:04,266 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:44:04,267 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:44:04,268 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:44:04,269 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:44:04,279 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:44:04,280 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:44:05,438 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:44:11,500 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:44:12,630 [INFO] AFC remote call 1 is done.
2025-06-28 13:44:18,871 [INFO] Resposta da IA (texto): Esta aplicação é um **Assistente de IA Multimodal** versátil, construído principalmente com **Streamlit** para uma interface de usuário interativa e que se integra fortemente com a **API do Google Gemini**.

Aqui está um resumo detalhado de suas funcionalidades e arquitetura:

### 1. Funcionalidades Principais:

*   **Chat Interativo:**
    *   Permite conversas textuais com o modelo Gemini.
    *   O contexto do chat é enriquecido com o próprio código-fonte da aplicação, tornando o assistente "cônscio" de suas próprias funcionalidades.
*   **Análise de Imagens:**
    *   **Upload de Imagens:** Usuários podem carregar arquivos de imagem (PNG, JPG, JPEG) diretamente via `st.file_uploader`.
    *   **Colar da Área de Transferência:** Suporta colar imagens diretamente da área de transferência (clipboard), facilitando a análise rápida de capturas de tela.
    *   Integração com o Gemini para descrever, analisar ou responder perguntas sobre as imagens fornecidas.
*   **Geração de Imagens:**
    *   Os usuários podem fornecer um prompt de texto para gerar novas imagens usando o modelo `gemini-2.0-flash-exp-image-generation`.
    *   As imagens geradas são exibidas no histórico do chat.
*   **Histórico de Conversas:** Mantém um histórico das mensagens e imagens trocadas na sessão do chat, exibindo-as de forma clara na interface.
*   **Processamento em Lote de Imagens (Funcionalidade Backend/Offline):**
    *   Um componente `ImageProcessor` permite analisar programaticamente um diretório de imagens (`assets/`).
    *   Para cada imagem, gera um resumo detalhado usando o Gemini.
    *   Salva os resumos e as imagens processadas em um documento **Microsoft Word (.docx)** e um arquivo **Markdown (.md)**.
    *   Move as imagens processadas para um diretório separado (`processed_images/`).
    *   Mantém um histórico JSON das análises realizadas.

### 2. Arquitetura e Componentes-Chave:

*   **`chat_streamlit.py`:** O coração da interface de usuário. Gerencia o estado da sessão (mensagens, imagens), interage com o usuário, e orquestra as chamadas para os serviços de IA e manipulação de imagens.
*   **`main.py`:** O ponto de entrada para a funcionalidade de processamento em lote, utilizando o `ImageProcessor`.
*   **`config/config.py`:** Define variáveis de configuração globais, como caminhos de diretórios (ativos, logs, prompts, processados), nomes de arquivos de saída e configurações de rate limiting.
*   **`core/`:**
    *   **`gemini_handler.py`:** Uma camada de abstração para interagir com a API do Gemini, incorporando o `RateLimiter`.
    *   **`rate_limiter.py`:** Uma classe fundamental que implementa um limitador de taxa robusto (usando `deque` e `Lock` para thread-safety), garantindo que as chamadas à API do Gemini respeitem os limites de requisições por período de tempo. É usado tanto no chat interativo quanto no processamento em lote.
    *   **`logger_config.py`:** Configura um sistema de log centralizado para a aplicação, registrando eventos em arquivo e no console.
    *   **`signal_handler.py`:** Lida com sinais do sistema (e.g., Ctrl+C) para permitir um encerramento gracioso da aplicação.
*   **`services/`:**
    *   **`gpt_services.py`:** A classe principal para interação com a API Google Generative AI (Gemini), lidando com geração de conteúdo de texto e de imagens (análise multimodal).
    *   **`image_processor.py`:** Orquestra o fluxo de trabalho de processamento em lote de imagens: carrega prompts, chama o `gpt_handler` para análise, e usa os serviços de documento/markdown.
    *   **`document_service.py`:** Responsável por criar e manipular documentos `.docx`, adicionando títulos, imagens e os resumos gerados pela IA.
    *   **`markdown_service.py`:** Similar ao `DocumentService`, mas focado em gerar arquivos `.md`.
    *   **`image_services.py`:** Um serviço dedicado especificamente para a *geração* de imagens usando o modelo `gemini-2.0-flash-exp-image-generation`.
    *   **`search_files.py`:** Uma utilidade para ler o conteúdo de todos os arquivos Python do projeto, que é então injetado no prompt inicial do chat para fornecer contexto à IA sobre sua própria estrutura.
*   **`utils/file_utils.py`:** Funções auxiliares para manipulação de arquivos, como listar imagens em um diretório.
*   **`test_image_generation.py`:** Um script de teste para a funcionalidade de geração de imagens.

### 3. Principais Destaques:

*   **Interatividade e Usabilidade:** A interface Streamlit torna o assistente fácil de usar para usuários finais.
*   **Multimodalidade:** Capacidade de processar e interagir tanto com texto quanto com imagens.
*   **Autoconsciência (Contexto de Código):** A injeção do próprio código-fonte no prompt da IA é uma característica inteligente que permite ao modelo entender melhor suas capacidades e a estrutura da aplicação.
*   **Gerenciamento de API (Rate Limiting):** A implementação robusta de rate limiting é crucial para aplicações que dependem de APIs pagas ou com limites de uso.
*   **Relatórios Automatizados:** A capacidade de gerar documentos Word e Markdown com as análises é valiosa para documentação e compartilhamento de resultados.

Em resumo, a aplicação é uma solução abrangente para interação com IA, combinando um frontend de chat dinâmico com capacidades de processamento de imagem em segundo plano e geração de relatórios, tudo isso enquanto gerencia eficientemente o uso da API.
2025-06-28 13:44:19,038 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:44:19,039 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:44:19,040 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:44:19,041 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:44:19,042 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:44:19,043 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:44:19,045 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:44:19,047 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:44:19,050 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:44:19,053 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:44:19,054 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:44:19,055 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:44:19,056 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:44:19,058 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:44:19,063 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:44:29,486 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:44:29,486 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:44:29,488 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:44:29,489 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:44:29,490 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:44:29,491 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:44:29,507 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:44:29,513 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:44:29,518 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:44:29,519 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:44:29,520 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:44:29,521 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:44:29,522 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:44:29,523 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:44:29,524 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:44:31,031 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:44:31,032 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:44:31,033 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:44:31,034 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:44:31,035 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:44:31,035 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:44:31,044 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:44:31,048 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:44:31,049 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:44:31,051 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:44:31,052 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:44:31,053 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:44:31,054 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:44:31,055 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:44:31,056 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:44:31,768 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:44:36,412 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:44:37,556 [INFO] AFC remote call 1 is done.
2025-06-28 13:45:14,148 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:45:14,149 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:45:14,150 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:45:14,152 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:45:14,153 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:45:14,154 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:45:14,156 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:45:14,159 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:45:14,241 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:45:14,243 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:45:14,244 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:45:14,304 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:45:14,316 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:45:14,340 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:45:14,358 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:45:14,585 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:45:14,586 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:45:14,587 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:45:14,588 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:45:14,589 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:45:14,590 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:45:14,592 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:45:14,596 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:45:14,597 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:45:14,603 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:45:14,605 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:45:14,607 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:45:14,609 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:45:14,610 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:45:14,612 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:45:14,614 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque estou recebendo esse erro nessa aplicação sendo que no outro notebook a mesma aplicação roda perfeitamente ?
2025-06-28 13:45:35,746 [INFO] Resposta da IA (texto): É frustrante quando o mesmo código se comporta de forma diferente em máquinas distintas! Isso quase sempre aponta para **diferenças no ambiente de execução**, e não no código em si.

Analisando seu código, o principal suspeito para um erro que ocorre em um notebook e não em outro, especialmente se um for Windows e o outro Linux/macOS, é o uso de `PIL.ImageGrab`.

---

**O SUSPEITO PRINCIPAL: `PIL.ImageGrab` (Área de Transferência)**

No seu arquivo `chat_streamlit.py`, você tem a função `check_clipboard()` que usa `PIL.ImageGrab`:

```python
# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab # <--- AQUI ESTÁ O PROVÁVEL PROBLEMA

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        # ... (restante da função)
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}") # <--- ESSA É A MENSAGEM DE ERRO QUE VOCÊ DEVE ESTAR VENDO
        return None, None
```

A biblioteca `PIL.ImageGrab` (parte do Pillow) é altamente dependente do sistema operacional:

1.  **Windows:** Geralmente funciona sem problemas, pois o Windows possui APIs diretas para acesso à área de transferência de imagens.
2.  **Linux:** Requer utilitários de linha de comando como `scrot` ou `gnome-screenshot` instalados e configurados corretamente, e a biblioteca `Pillow` precisa ser compilada com suporte a essas ferramentas. Muitas distribuições Linux não vêm com isso pronto para o `ImageGrab`.
3.  **macOS:** Também tem suas próprias dependências, geralmente exigindo bibliotecas como `pyobjc` para interagir com o ambiente Cocoa.

**Se o seu "outro notebook" roda Windows e o notebook com problema roda Linux ou macOS, este é 99% a causa.** A mensagem de erro `"Erro ao acessar a área de transferência: {e}"` é exatamente o que você veria na barra lateral do Streamlit se essa parte falhasse.

---

**Passos para Diagnosticar e Resolver:**

1.  **Obtenha a Mensagem de Erro Exata:**
    *   No notebook onde a aplicação não roda, verifique a barra lateral do Streamlit. Você verá a mensagem exata do erro que está ocorrendo, começando com `"Erro ao acessar a área de transferência: ..."`. O que vem depois do `:` é crucial.
    *   Se não for `PIL.ImageGrab`, o erro estará em outro lugar.

2.  **Compare os Ambientes:**
    *   **Sistema Operacional:** Qual SO está rodando em cada notebook? (Ex: Windows 10 vs. Ubuntu 22.04 vs. macOS Ventura)
    *   **Versão do Python:** `python --version` em ambos.
    *   **Dependências Instaladas:**
        *   No notebook que funciona, execute: `pip freeze > requirements_working.txt`
        *   No notebook que não funciona, execute: `pip freeze > requirements_broken.txt`
        *   Compare os dois arquivos (`diff requirements_working.txt requirements_broken.txt`). Preste atenção especial à versão do `Pillow` e a presença de pacotes como `pywin32` (Windows) ou qualquer coisa relacionada a screenshots em Linux/macOS.

3.  **Soluções para o `PIL.ImageGrab`:**

    *   **Para Linux (Ubuntu/Debian):**
        *   Instale `scrot` (ou `gnome-screenshot`): `sudo apt-get update && sudo apt-get install scrot` (ou `sudo apt-get install gnome-screenshot`).
        *   Certifique-se de que o `Pillow` foi instalado *após* a instalação dessas ferramentas ou reinstale-o para que ele possa detectar as dependências.
        *   Pode ser necessário também `sudo apt-get install python3-tk python3-pil.imagetk` em algumas distribuições.

    *   **Para macOS:**
        *   Pode ser necessário instalar `pyobjc`: `pip install pyobjc`.

    *   **Solução Alternativa (Desativar ou Condicional):**
        Se você não precisa desesperadamente da funcionalidade de colar imagem da área de transferência e quer que a aplicação funcione em qualquer SO sem dependências externas complexas:
        Você já tem um `try-except` que lida com o erro. A aplicação deve continuar funcionando mesmo se a funcionalidade de colar não funcionar. No entanto, se o erro estiver parando a execução principal, isso é um problema.

        **Para evitar o erro na barra lateral em ambientes não-Windows:**
        Você pode adicionar uma verificação de sistema operacional:
        ```python
        import platform
        # ...

        def check_clipboard():
            # Só tenta importar ImageGrab se for Windows
            if platform.system() == "Windows":
                try:
                    from PIL import ImageGrab
                    img = ImageGrab.grabclipboard()
                    # ... (restante do código se img não for None)
                    if img is not None and isinstance(img, Image.Image):
                        # ... seu código para processar a imagem da área de transferência
                        # ...
                        return ClipboardFile(img_byte_arr), img
                    return None, None
                except Exception as e:
                    # A mensagem já é exibida no Streamlit, então não precisa de outro print aqui
                    st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
                    return None, None
            else:
                # Se não for Windows, não tenta acessar a área de transferência
                # E não mostra erro na barra lateral
                # st.sidebar.info("Colar da área de transferência disponível apenas no Windows.")
                return None, None
        ```
        Isso impediria que o erro específico de `ImageGrab` fosse tentado em sistemas não-Windows.

4.  **Verifique a Chave da API Gemini:**
    *   Certifique-se de que o arquivo `.env` existe no diretório raiz da aplicação no notebook problemático.
    *   Certifique-se de que `API_KEY_GEMINI` está definida corretamente dentro dele (Ex: `API_KEY_GEMINI="SUA_CHAVE_AQUI"`).
    *   Um erro relacionado à API Key geralmente resultaria em `ValueError("API Key não encontrada nas variáveis de ambiente")` ou erros da própria API (4xx, 5xx).

5.  **Verifique as Versões das Dependências:**
    *   É altamente recomendável ter um `requirements.txt` e instalá-lo em ambos os ambientes com `pip install -r requirements.txt`.
    *   Pode haver incompatibilidades sutis entre versões de `streamlit`, `google-generativeai`, `Pillow`, etc.

6.  **Permissões de Diretório:**
    *   Verifique se a aplicação tem permissão para criar e escrever nos diretórios `assets/`, `logs/` e `processed_images/` no notebook problemático. Isso é mais comum em Linux/macOS com permissões restritas.

7.  **Conectividade de Rede/Firewall:**
    *   O notebook problemático consegue acessar a internet? Existem restrições de firewall que podem estar bloqueando o acesso aos endpoints da API do Gemini?

---

**Resumo da Ação Mais Provável:**

O erro `Erro ao acessar a área de transferência: {e}` na barra lateral do Streamlit é quase uma certeza de que o problema está na tentativa de usar `PIL.ImageGrab` em um sistema operacional (provavelmente Linux/macOS) que não tem as dependências necessárias para isso.

Por favor, forneça a mensagem de erro exata da barra lateral e o sistema operacional de ambos os notebooks para uma análise mais precisa!
2025-06-28 13:45:35,983 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:45:35,984 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:45:35,986 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:45:35,989 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:45:35,990 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:45:35,992 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:45:35,996 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:45:36,004 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:45:36,006 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:45:36,007 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:45:36,010 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:45:36,011 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:45:36,013 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:45:36,014 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:45:36,017 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:47:19,204 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:47:19,205 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:47:19,206 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:47:19,207 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:47:19,208 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:47:19,209 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:47:19,210 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:47:19,212 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:47:19,215 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:47:19,221 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:47:19,222 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:47:19,223 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:47:19,224 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:47:19,224 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:47:19,225 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:47:19,478 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:47:19,479 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:47:19,480 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:47:19,481 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:47:19,482 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:47:19,483 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:47:19,484 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:47:19,486 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:47:19,489 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:47:19,491 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:47:19,492 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:47:19,493 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:47:19,495 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:47:19,496 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:47:19,498 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:47:19,502 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque estou recebendo esse erro nessa aplicação sendo que no outro notebook a mesma aplicação roda perfeitamente ?
Assistant: É frustrante quando o mesmo código se comporta de forma diferente em máquinas distintas! Isso quase sempre aponta para **diferenças no ambiente de execução**, e não no código em si.

Analisando seu código, o principal suspeito para um erro que ocorre em um notebook e não em outro, especialmente se um for Windows e o outro Linux/macOS, é o uso de `PIL.ImageGrab`.

---

**O SUSPEITO PRINCIPAL: `PIL.ImageGrab` (Área de Transferência)**

No seu arquivo `chat_streamlit.py`, você tem a função `check_clipboard()` que usa `PIL.ImageGrab`:

```python
# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab # <--- AQUI ESTÁ O PROVÁVEL PROBLEMA

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        # ... (restante da função)
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}") # <--- ESSA É A MENSAGEM DE ERRO QUE VOCÊ DEVE ESTAR VENDO
        return None, None
```

A biblioteca `PIL.ImageGrab` (parte do Pillow) é altamente dependente do sistema operacional:

1.  **Windows:** Geralmente funciona sem problemas, pois o Windows possui APIs diretas para acesso à área de transferência de imagens.
2.  **Linux:** Requer utilitários de linha de comando como `scrot` ou `gnome-screenshot` instalados e configurados corretamente, e a biblioteca `Pillow` precisa ser compilada com suporte a essas ferramentas. Muitas distribuições Linux não vêm com isso pronto para o `ImageGrab`.
3.  **macOS:** Também tem suas próprias dependências, geralmente exigindo bibliotecas como `pyobjc` para interagir com o ambiente Cocoa.

**Se o seu "outro notebook" roda Windows e o notebook com problema roda Linux ou macOS, este é 99% a causa.** A mensagem de erro `"Erro ao acessar a área de transferência: {e}"` é exatamente o que você veria na barra lateral do Streamlit se essa parte falhasse.

---

**Passos para Diagnosticar e Resolver:**

1.  **Obtenha a Mensagem de Erro Exata:**
    *   No notebook onde a aplicação não roda, verifique a barra lateral do Streamlit. Você verá a mensagem exata do erro que está ocorrendo, começando com `"Erro ao acessar a área de transferência: ..."`. O que vem depois do `:` é crucial.
    *   Se não for `PIL.ImageGrab`, o erro estará em outro lugar.

2.  **Compare os Ambientes:**
    *   **Sistema Operacional:** Qual SO está rodando em cada notebook? (Ex: Windows 10 vs. Ubuntu 22.04 vs. macOS Ventura)
    *   **Versão do Python:** `python --version` em ambos.
    *   **Dependências Instaladas:**
        *   No notebook que funciona, execute: `pip freeze > requirements_working.txt`
        *   No notebook que não funciona, execute: `pip freeze > requirements_broken.txt`
        *   Compare os dois arquivos (`diff requirements_working.txt requirements_broken.txt`). Preste atenção especial à versão do `Pillow` e a presença de pacotes como `pywin32` (Windows) ou qualquer coisa relacionada a screenshots em Linux/macOS.

3.  **Soluções para o `PIL.ImageGrab`:**

    *   **Para Linux (Ubuntu/Debian):**
        *   Instale `scrot` (ou `gnome-screenshot`): `sudo apt-get update && sudo apt-get install scrot` (ou `sudo apt-get install gnome-screenshot`).
        *   Certifique-se de que o `Pillow` foi instalado *após* a instalação dessas ferramentas ou reinstale-o para que ele possa detectar as dependências.
        *   Pode ser necessário também `sudo apt-get install python3-tk python3-pil.imagetk` em algumas distribuições.

    *   **Para macOS:**
        *   Pode ser necessário instalar `pyobjc`: `pip install pyobjc`.

    *   **Solução Alternativa (Desativar ou Condicional):**
        Se você não precisa desesperadamente da funcionalidade de colar imagem da área de transferência e quer que a aplicação funcione em qualquer SO sem dependências externas complexas:
        Você já tem um `try-except` que lida com o erro. A aplicação deve continuar funcionando mesmo se a funcionalidade de colar não funcionar. No entanto, se o erro estiver parando a execução principal, isso é um problema.

        **Para evitar o erro na barra lateral em ambientes não-Windows:**
        Você pode adicionar uma verificação de sistema operacional:
        ```python
        import platform
        # ...

        def check_clipboard():
            # Só tenta importar ImageGrab se for Windows
            if platform.system() == "Windows":
                try:
                    from PIL import ImageGrab
                    img = ImageGrab.grabclipboard()
                    # ... (restante do código se img não for None)
                    if img is not None and isinstance(img, Image.Image):
                        # ... seu código para processar a imagem da área de transferência
                        # ...
                        return ClipboardFile(img_byte_arr), img
                    return None, None
                except Exception as e:
                    # A mensagem já é exibida no Streamlit, então não precisa de outro print aqui
                    st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
                    return None, None
            else:
                # Se não for Windows, não tenta acessar a área de transferência
                # E não mostra erro na barra lateral
                # st.sidebar.info("Colar da área de transferência disponível apenas no Windows.")
                return None, None
        ```
        Isso impediria que o erro específico de `ImageGrab` fosse tentado em sistemas não-Windows.

4.  **Verifique a Chave da API Gemini:**
    *   Certifique-se de que o arquivo `.env` existe no diretório raiz da aplicação no notebook problemático.
    *   Certifique-se de que `API_KEY_GEMINI` está definida corretamente dentro dele (Ex: `API_KEY_GEMINI="SUA_CHAVE_AQUI"`).
    *   Um erro relacionado à API Key geralmente resultaria em `ValueError("API Key não encontrada nas variáveis de ambiente")` ou erros da própria API (4xx, 5xx).

5.  **Verifique as Versões das Dependências:**
    *   É altamente recomendável ter um `requirements.txt` e instalá-lo em ambos os ambientes com `pip install -r requirements.txt`.
    *   Pode haver incompatibilidades sutis entre versões de `streamlit`, `google-generativeai`, `Pillow`, etc.

6.  **Permissões de Diretório:**
    *   Verifique se a aplicação tem permissão para criar e escrever nos diretórios `assets/`, `logs/` e `processed_images/` no notebook problemático. Isso é mais comum em Linux/macOS com permissões restritas.

7.  **Conectividade de Rede/Firewall:**
    *   O notebook problemático consegue acessar a internet? Existem restrições de firewall que podem estar bloqueando o acesso aos endpoints da API do Gemini?

---

**Resumo da Ação Mais Provável:**

O erro `Erro ao acessar a área de transferência: {e}` na barra lateral do Streamlit é quase uma certeza de que o problema está na tentativa de usar `PIL.ImageGrab` em um sistema operacional (provavelmente Linux/macOS) que não tem as dependências necessárias para isso.

Por favor, forneça a mensagem de erro exata da barra lateral e o sistema operacional de ambos os notebooks para uma análise mais precisa!
User: ois dois ambientes são windows. 
2025-06-28 13:47:41,202 [INFO] Resposta da IA (texto): Certo, isso muda o cenário! Se ambos os ambientes são Windows, então o problema não é a incompatibilidade do `PIL.ImageGrab` com Linux/macOS. No entanto, o `PIL.ImageGrab` continua sendo o principal suspeito, mas por razões diferentes.

Quando `PIL.ImageGrab` falha no Windows, geralmente aponta para um problema com a biblioteca `pywin32`, que o Pillow usa para interagir com as APIs do sistema operacional Windows.

Vamos desvendar isso.

---

**Causas Mais Comuns do `PIL.ImageGrab` Falhar no Windows:**

1.  **`pywin32` Ausente ou Corrompido:** A biblioteca `pywin32` é essencial para que o Pillow (que inclui `PIL.ImageGrab`) acesse a área de transferência do Windows de forma nativa. Se ela não estiver instalada ou estiver corrompida, a função falhará.
2.  **Ambientes Python Diferentes:** Mesmo que ambos sejam Windows, você pode ter ambientes Python (globais ou virtuais) ligeiramente diferentes em cada máquina, onde uma tem `pywin32` e a outra não, ou tem uma versão incompatível.
3.  **Conflitos de Versão:** Raramente, pode haver um conflito entre a versão do `Pillow` e a versão do `pywin32`, ou até mesmo com a versão do Python.
4.  **Conteúdo da Área de Transferência:** Embora menos provável dado seu relato, o `ImageGrab.grabclipboard()` retornará `None` se o conteúdo da área de transferência não for uma imagem (ex: texto, arquivos). No entanto, o erro que você vê sugere que o *acesso* à área de transferência está falhando, não apenas que ela está vazia.

---

**Passos Detalhados de Diagnóstico e Solução:**

**1. Obtenha a Mensagem de Erro Exata:**
   Este é o passo mais crucial. Quando você executa a aplicação no notebook que está dando problema, qual é a *mensagem de erro completa* que aparece na barra lateral do Streamlit, após "Erro ao acessar a área de transferência: "?
   Ex: `"Erro ao acessar a área de transferência: [WinError 5] Acesso negado."` ou `"Erro ao acessar a área de transferência: No module named 'win32clipboard'"` etc.
   Esta mensagem detalhada nos dirá exatamente o que está acontecendo.

**2. Compare as Dependências `pip`:**

   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que NÃO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos:** Abra `requirements_working.txt` e `requirements_broken.txt` e procure por:
        *   `Pillow` (P-I-L-L-O-W): Verifique as versões.
        *   `pywin32`: **Este é o mais importante.** Ele deve estar presente no `requirements_working.txt`. Se não estiver no `requirements_broken.txt` ou se a versão for diferente, encontramos o provável culpado.
        *   `streamlit`: Verifique as versões.

**3. Instale/Reinstale `pywin32` (no notebook problemático):**

   Se `pywin32` estiver faltando ou com uma versão diferente, tente o seguinte:

   *   **Verifique o ambiente virtual:** Se você estiver usando um ambiente virtual (altamente recomendado), ative-o primeiro.
        ```bash
        # Exemplo para venv:
        cd C:\caminho\para\seu\projeto
        .\.venv\Scripts\activate  # ou .\venv\Scripts\activate
        ```
   *   **Instale `pywin32`:**
        ```bash
        pip install pywin32
        ```
   *   **Ou, se já estiver instalado, force a reinstalação:**
        ```bash
        pip install --upgrade --force-reinstall pywin32
        ```
   *   **Reinicie a aplicação Streamlit.**

**4. Teste a Funcionalidade `ImageGrab` Isoladamente:**

   Para confirmar que o problema é realmente com `ImageGrab` e não algo mais amplo com o Streamlit ou seu código:

   *   Crie um pequeno script Python (`test_clipboard.py`) no notebook problemático:
        ```python
        import sys
        from PIL import ImageGrab, Image
        import io
        import time

        try:
            # Tenta pegar imagem da área de transferência
            img = ImageGrab.grabclipboard()

            if img is not None and isinstance(img, Image.Image):
                print("Imagem encontrada na área de transferência!")
                # Opcional: Salvar a imagem para verificar
                # img.save("clipboard_test.png")
                # print("Imagem salva como clipboard_test.png")
                
                # Tenta converter para bytes (simular o que você faz no Streamlit)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format='PNG')
                img_byte_arr.seek(0)
                print(f"Tamanho dos bytes: {len(img_byte_arr.getvalue())} bytes")
                
            elif img is None:
                print("Nenhuma imagem na área de transferência ou o conteúdo não é uma imagem.")
            else:
                print(f"Tipo de conteúdo inesperado na área de transferência: {type(img)}")

        except ImportError as e:
            print(f"Erro: Dependência faltando para PIL.ImageGrab: {e}")
            print("Tente 'pip install pywin32' e certifique-se de que está no ambiente correto.")
        except Exception as e:
            print(f"Erro inesperado ao acessar a área de transferência: {e}")
            print("Verifique permissões ou possíveis conflitos.")

        print("\nTeste concluído.")
        ```
   *   **Copie uma imagem** (não texto) para a área de transferência no Windows.
   *   Execute este script no terminal (ativando seu ambiente virtual, se aplicável): `python test_clipboard.py`
   *   Observe a saída. Isso confirmará se o `ImageGrab` está funcionando fora do Streamlit e qual erro exato ele gera.

**5. Verifique a Versão do Python e Arquitetura:**

   Embora menos comum hoje em dia, um Python de 32 bits pode ter problemas com algumas bibliotecas de 64 bits.

   *   Execute `python -c "import platform; print(platform.architecture())"` em ambos os notebooks. Verifique se ambos são `('64bit', 'WindowsPE')`.

**6. Outras Verificações (Menos Prováveis):**

*   **Antivírus/Firewall:** Muito raramente, um software de segurança pode interferir no acesso a certas APIs do sistema. Tente desativá-lo temporariamente (se for seguro fazer isso) para testar.
*   **Permissões de Usuário:** Certifique-se de que o usuário que executa a aplicação Streamlit tem as permissões necessárias para interagir com o sistema.

---

**Em Resumo:**

A principal ação é obter a **mensagem de erro exata** da barra lateral do Streamlit e, em seguida, **comparar as dependências `pip freeze`** para identificar se `pywin32` está faltando ou tem uma versão diferente. O teste isolado com `test_clipboard.py` também é muito útil para isolar o problema.

Assim que você tiver a mensagem de erro exata, poderemos refinar ainda mais o diagnóstico!
2025-06-28 13:47:41,852 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:47:41,855 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:47:41,857 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:47:41,866 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:47:41,867 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:47:41,868 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:47:41,869 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:47:41,870 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:47:41,873 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:47:41,877 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:47:41,879 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:47:41,881 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:47:41,883 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:47:41,886 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:47:41,887 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:50:03,479 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:50:03,480 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:50:03,481 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:50:03,482 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:50:03,484 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:50:03,485 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:50:03,487 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:50:03,488 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:50:03,489 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:50:03,490 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:50:03,492 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:50:03,494 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:50:03,498 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:50:03,504 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:50:03,507 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:50:03,703 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:50:03,704 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:50:03,705 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:50:03,706 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:50:03,707 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:50:03,708 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:50:03,710 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:50:03,712 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:50:03,715 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:50:03,723 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:50:03,724 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:50:03,726 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:50:03,727 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:50:03,728 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:50:03,730 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:50:03,733 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: resuma a aplicação enviada
Assistant: Esta aplicação é um **Assistente de IA Multimodal** versátil, construído principalmente com **Streamlit** para uma interface de usuário interativa e que se integra fortemente com a **API do Google Gemini**.

Aqui está um resumo detalhado de suas funcionalidades e arquitetura:

### 1. Funcionalidades Principais:

*   **Chat Interativo:**
    *   Permite conversas textuais com o modelo Gemini.
    *   O contexto do chat é enriquecido com o próprio código-fonte da aplicação, tornando o assistente "cônscio" de suas próprias funcionalidades.
*   **Análise de Imagens:**
    *   **Upload de Imagens:** Usuários podem carregar arquivos de imagem (PNG, JPG, JPEG) diretamente via `st.file_uploader`.
    *   **Colar da Área de Transferência:** Suporta colar imagens diretamente da área de transferência (clipboard), facilitando a análise rápida de capturas de tela.
    *   Integração com o Gemini para descrever, analisar ou responder perguntas sobre as imagens fornecidas.
*   **Geração de Imagens:**
    *   Os usuários podem fornecer um prompt de texto para gerar novas imagens usando o modelo `gemini-2.0-flash-exp-image-generation`.
    *   As imagens geradas são exibidas no histórico do chat.
*   **Histórico de Conversas:** Mantém um histórico das mensagens e imagens trocadas na sessão do chat, exibindo-as de forma clara na interface.
*   **Processamento em Lote de Imagens (Funcionalidade Backend/Offline):**
    *   Um componente `ImageProcessor` permite analisar programaticamente um diretório de imagens (`assets/`).
    *   Para cada imagem, gera um resumo detalhado usando o Gemini.
    *   Salva os resumos e as imagens processadas em um documento **Microsoft Word (.docx)** e um arquivo **Markdown (.md)**.
    *   Move as imagens processadas para um diretório separado (`processed_images/`).
    *   Mantém um histórico JSON das análises realizadas.

### 2. Arquitetura e Componentes-Chave:

*   **`chat_streamlit.py`:** O coração da interface de usuário. Gerencia o estado da sessão (mensagens, imagens), interage com o usuário, e orquestra as chamadas para os serviços de IA e manipulação de imagens.
*   **`main.py`:** O ponto de entrada para a funcionalidade de processamento em lote, utilizando o `ImageProcessor`.
*   **`config/config.py`:** Define variáveis de configuração globais, como caminhos de diretórios (ativos, logs, prompts, processados), nomes de arquivos de saída e configurações de rate limiting.
*   **`core/`:**
    *   **`gemini_handler.py`:** Uma camada de abstração para interagir com a API do Gemini, incorporando o `RateLimiter`.
    *   **`rate_limiter.py`:** Uma classe fundamental que implementa um limitador de taxa robusto (usando `deque` e `Lock` para thread-safety), garantindo que as chamadas à API do Gemini respeitem os limites de requisições por período de tempo. É usado tanto no chat interativo quanto no processamento em lote.
    *   **`logger_config.py`:** Configura um sistema de log centralizado para a aplicação, registrando eventos em arquivo e no console.
    *   **`signal_handler.py`:** Lida com sinais do sistema (e.g., Ctrl+C) para permitir um encerramento gracioso da aplicação.
*   **`services/`:**
    *   **`gpt_services.py`:** A classe principal para interação com a API Google Generative AI (Gemini), lidando com geração de conteúdo de texto e de imagens (análise multimodal).
    *   **`image_processor.py`:** Orquestra o fluxo de trabalho de processamento em lote de imagens: carrega prompts, chama o `gpt_handler` para análise, e usa os serviços de documento/markdown.
    *   **`document_service.py`:** Responsável por criar e manipular documentos `.docx`, adicionando títulos, imagens e os resumos gerados pela IA.
    *   **`markdown_service.py`:** Similar ao `DocumentService`, mas focado em gerar arquivos `.md`.
    *   **`image_services.py`:** Um serviço dedicado especificamente para a *geração* de imagens usando o modelo `gemini-2.0-flash-exp-image-generation`.
    *   **`search_files.py`:** Uma utilidade para ler o conteúdo de todos os arquivos Python do projeto, que é então injetado no prompt inicial do chat para fornecer contexto à IA sobre sua própria estrutura.
*   **`utils/file_utils.py`:** Funções auxiliares para manipulação de arquivos, como listar imagens em um diretório.
*   **`test_image_generation.py`:** Um script de teste para a funcionalidade de geração de imagens.

### 3. Principais Destaques:

*   **Interatividade e Usabilidade:** A interface Streamlit torna o assistente fácil de usar para usuários finais.
*   **Multimodalidade:** Capacidade de processar e interagir tanto com texto quanto com imagens.
*   **Autoconsciência (Contexto de Código):** A injeção do próprio código-fonte no prompt da IA é uma característica inteligente que permite ao modelo entender melhor suas capacidades e a estrutura da aplicação.
*   **Gerenciamento de API (Rate Limiting):** A implementação robusta de rate limiting é crucial para aplicações que dependem de APIs pagas ou com limites de uso.
*   **Relatórios Automatizados:** A capacidade de gerar documentos Word e Markdown com as análises é valiosa para documentação e compartilhamento de resultados.

Em resumo, a aplicação é uma solução abrangente para interação com IA, combinando um frontend de chat dinâmico com capacidades de processamento de imagem em segundo plano e geração de relatórios, tudo isso enquanto gerencia eficientemente o uso da API.
User: quais lugares dessa aplicação ainda usa a gemini-2.0 ?
2025-06-28 13:50:12,143 [INFO] Resposta da IA (texto): A aplicação ainda usa `gemini-2.0` exclusivamente para a funcionalidade de **geração de imagens**.

Em todos os outros contextos (chat interativo e processamento em lote de imagens para análise), o modelo utilizado é o `gemini-2.5-flash`.

Os lugares específicos onde `gemini-2.0` é usado para geração de imagens são:

1.  **`chat_streamlit.py`** (na função `generate_image`):
    ```python
    def generate_image(prompt):
        # ...
        client = genai.Client(api_key=api_key)
        try:
            response = client.models.generate_content(
                model='gemini-2.0-flash-exp-image-generation', # <--- Aqui
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_modalities=['Text', 'Image']
                )
            )
            # ...
    ```

2.  **`services\image_services.py`** (na função `generate_image`):
    ```python
    def generate_image(prompt: str) -> Image.Image | None:
        try:
            model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation') # <--- Aqui
            response = model.generate_content(prompt)
            # ...
    ```

3.  **`test_image_generation.py`** (no script de teste):
    ```python
    response = client.models.generate_content(
        model='gemini-2.0-flash-exp-image-generation', # <--- Aqui
        contents=prompt,
        config=types.GenerateContentConfig(
            response_modalities=['Text', 'Image']
        )
    )
    ```

Isso significa que, para tarefas de conversação e análise de imagens existentes, o modelo mais recente (`gemini-2.5-flash`) é empregado, enquanto para a criação de novas imagens a partir do zero, o modelo `gemini-2.0-flash-exp-image-generation` é o responsável.
2025-06-28 13:50:12,309 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:50:12,310 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:50:12,311 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:50:12,312 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:50:12,313 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:50:12,314 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:50:12,315 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:50:12,317 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:50:12,318 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:50:12,323 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:50:12,324 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:50:12,325 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:50:12,327 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:50:12,330 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:50:12,333 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:52:27,983 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:52:27,985 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:52:27,985 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:52:27,986 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:52:27,987 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:52:27,988 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:52:27,989 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:52:27,990 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:52:27,992 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:52:27,994 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:52:27,995 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:52:27,996 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:52:27,997 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:52:27,998 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:52:27,999 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:52:28,193 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:52:28,194 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:52:28,195 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:52:28,197 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:52:28,201 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:52:28,203 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:52:28,204 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:52:28,205 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:52:28,207 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:52:28,208 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:52:28,210 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:52:28,211 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:52:28,212 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:52:28,213 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:52:28,215 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:52:28,218 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: resuma a aplicação enviada
Assistant: Esta aplicação é um **Assistente de IA Multimodal** versátil, construído principalmente com **Streamlit** para uma interface de usuário interativa e que se integra fortemente com a **API do Google Gemini**.

Aqui está um resumo detalhado de suas funcionalidades e arquitetura:

### 1. Funcionalidades Principais:

*   **Chat Interativo:**
    *   Permite conversas textuais com o modelo Gemini.
    *   O contexto do chat é enriquecido com o próprio código-fonte da aplicação, tornando o assistente "cônscio" de suas próprias funcionalidades.
*   **Análise de Imagens:**
    *   **Upload de Imagens:** Usuários podem carregar arquivos de imagem (PNG, JPG, JPEG) diretamente via `st.file_uploader`.
    *   **Colar da Área de Transferência:** Suporta colar imagens diretamente da área de transferência (clipboard), facilitando a análise rápida de capturas de tela.
    *   Integração com o Gemini para descrever, analisar ou responder perguntas sobre as imagens fornecidas.
*   **Geração de Imagens:**
    *   Os usuários podem fornecer um prompt de texto para gerar novas imagens usando o modelo `gemini-2.0-flash-exp-image-generation`.
    *   As imagens geradas são exibidas no histórico do chat.
*   **Histórico de Conversas:** Mantém um histórico das mensagens e imagens trocadas na sessão do chat, exibindo-as de forma clara na interface.
*   **Processamento em Lote de Imagens (Funcionalidade Backend/Offline):**
    *   Um componente `ImageProcessor` permite analisar programaticamente um diretório de imagens (`assets/`).
    *   Para cada imagem, gera um resumo detalhado usando o Gemini.
    *   Salva os resumos e as imagens processadas em um documento **Microsoft Word (.docx)** e um arquivo **Markdown (.md)**.
    *   Move as imagens processadas para um diretório separado (`processed_images/`).
    *   Mantém um histórico JSON das análises realizadas.

### 2. Arquitetura e Componentes-Chave:

*   **`chat_streamlit.py`:** O coração da interface de usuário. Gerencia o estado da sessão (mensagens, imagens), interage com o usuário, e orquestra as chamadas para os serviços de IA e manipulação de imagens.
*   **`main.py`:** O ponto de entrada para a funcionalidade de processamento em lote, utilizando o `ImageProcessor`.
*   **`config/config.py`:** Define variáveis de configuração globais, como caminhos de diretórios (ativos, logs, prompts, processados), nomes de arquivos de saída e configurações de rate limiting.
*   **`core/`:**
    *   **`gemini_handler.py`:** Uma camada de abstração para interagir com a API do Gemini, incorporando o `RateLimiter`.
    *   **`rate_limiter.py`:** Uma classe fundamental que implementa um limitador de taxa robusto (usando `deque` e `Lock` para thread-safety), garantindo que as chamadas à API do Gemini respeitem os limites de requisições por período de tempo. É usado tanto no chat interativo quanto no processamento em lote.
    *   **`logger_config.py`:** Configura um sistema de log centralizado para a aplicação, registrando eventos em arquivo e no console.
    *   **`signal_handler.py`:** Lida com sinais do sistema (e.g., Ctrl+C) para permitir um encerramento gracioso da aplicação.
*   **`services/`:**
    *   **`gpt_services.py`:** A classe principal para interação com a API Google Generative AI (Gemini), lidando com geração de conteúdo de texto e de imagens (análise multimodal).
    *   **`image_processor.py`:** Orquestra o fluxo de trabalho de processamento em lote de imagens: carrega prompts, chama o `gpt_handler` para análise, e usa os serviços de documento/markdown.
    *   **`document_service.py`:** Responsável por criar e manipular documentos `.docx`, adicionando títulos, imagens e os resumos gerados pela IA.
    *   **`markdown_service.py`:** Similar ao `DocumentService`, mas focado em gerar arquivos `.md`.
    *   **`image_services.py`:** Um serviço dedicado especificamente para a *geração* de imagens usando o modelo `gemini-2.0-flash-exp-image-generation`.
    *   **`search_files.py`:** Uma utilidade para ler o conteúdo de todos os arquivos Python do projeto, que é então injetado no prompt inicial do chat para fornecer contexto à IA sobre sua própria estrutura.
*   **`utils/file_utils.py`:** Funções auxiliares para manipulação de arquivos, como listar imagens em um diretório.
*   **`test_image_generation.py`:** Um script de teste para a funcionalidade de geração de imagens.

### 3. Principais Destaques:

*   **Interatividade e Usabilidade:** A interface Streamlit torna o assistente fácil de usar para usuários finais.
*   **Multimodalidade:** Capacidade de processar e interagir tanto com texto quanto com imagens.
*   **Autoconsciência (Contexto de Código):** A injeção do próprio código-fonte no prompt da IA é uma característica inteligente que permite ao modelo entender melhor suas capacidades e a estrutura da aplicação.
*   **Gerenciamento de API (Rate Limiting):** A implementação robusta de rate limiting é crucial para aplicações que dependem de APIs pagas ou com limites de uso.
*   **Relatórios Automatizados:** A capacidade de gerar documentos Word e Markdown com as análises é valiosa para documentação e compartilhamento de resultados.

Em resumo, a aplicação é uma solução abrangente para interação com IA, combinando um frontend de chat dinâmico com capacidades de processamento de imagem em segundo plano e geração de relatórios, tudo isso enquanto gerencia eficientemente o uso da API.
User: quais lugares dessa aplicação ainda usa a gemini-2.0 ?
Assistant: A aplicação ainda usa `gemini-2.0` exclusivamente para a funcionalidade de **geração de imagens**.

Em todos os outros contextos (chat interativo e processamento em lote de imagens para análise), o modelo utilizado é o `gemini-2.5-flash`.

Os lugares específicos onde `gemini-2.0` é usado para geração de imagens são:

1.  **`chat_streamlit.py`** (na função `generate_image`):
    ```python
    def generate_image(prompt):
        # ...
        client = genai.Client(api_key=api_key)
        try:
            response = client.models.generate_content(
                model='gemini-2.0-flash-exp-image-generation', # <--- Aqui
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_modalities=['Text', 'Image']
                )
            )
            # ...
    ```

2.  **`services\image_services.py`** (na função `generate_image`):
    ```python
    def generate_image(prompt: str) -> Image.Image | None:
        try:
            model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation') # <--- Aqui
            response = model.generate_content(prompt)
            # ...
    ```

3.  **`test_image_generation.py`** (no script de teste):
    ```python
    response = client.models.generate_content(
        model='gemini-2.0-flash-exp-image-generation', # <--- Aqui
        contents=prompt,
        config=types.GenerateContentConfig(
            response_modalities=['Text', 'Image']
        )
    )
    ```

Isso significa que, para tarefas de conversação e análise de imagens existentes, o modelo mais recente (`gemini-2.5-flash`) é empregado, enquanto para a criação de novas imagens a partir do zero, o modelo `gemini-2.0-flash-exp-image-generation` é o responsável.
User: e aonde usa a (2.0-flash)
2025-06-28 13:52:38,125 [INFO] Resposta da IA (texto): Você está certo em querer a precisão! Na verdade, a aplicação utiliza o modelo `gemini-2.0-flash-exp-image-generation` especificamente para a funcionalidade de **geração de imagens**.

O termo `gemini-2.0-flash` é a forma abreviada para se referir a esse modelo de geração de imagens, que é uma versão experimental (`exp`) otimizada para essa tarefa.

Portanto, os lugares onde você encontrará o uso do modelo de **geração de imagens** (`gemini-2.0-flash-exp-image-generation`, ou coloquialmente "2.0-flash" para geração) são exatamente os mesmos que mencionei na resposta anterior:

1.  **`chat_streamlit.py`** (na função `generate_image`):
    ```python
    def generate_image(prompt):
        # ...
        client = genai.Client(api_key=api_key)
        try:
            response = client.models.generate_content(
                model='gemini-2.0-flash-exp-image-generation', # <--- Aqui
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_modalities=['Text', 'Image']
                )
            )
            # ...
    ```

2.  **`services\image_services.py`** (na função `generate_image`):
    ```python
    def generate_image(prompt: str) -> Image.Image | None:
        try:
            model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation') # <--- Aqui
            response = model.generate_content(prompt)
            # ...
    ```

3.  **`test_image_generation.py`** (no script de teste):
    ```python
    response = client.models.generate_content(
        model='gemini-2.0-flash-exp-image-generation', # <--- Aqui
        contents=prompt,
        config=types.GenerateContentConfig(
            response_modalities=['Text', 'Image']
        )
    )
    ```

**Para todas as outras interações (chat conversacional e análise de imagens existentes), a aplicação utiliza o modelo `gemini-2.5-flash`.**

Essa distinção é importante: o `gemini-2.5-flash` é o modelo mais recente e mais geral para conversação e compreensão multimodal (análise de imagem e texto), enquanto o `gemini-2.0-flash-exp-image-generation` é um modelo especializado apenas para criar novas imagens a partir de prompts de texto.
2025-06-28 13:52:38,284 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:52:38,285 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:52:38,286 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:52:38,286 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:52:38,288 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:52:38,289 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:52:38,290 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:52:38,292 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:52:38,293 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:52:38,294 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:52:38,295 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:52:38,296 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:52:38,297 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:52:38,298 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:52:38,299 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:59:12,298 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:59:12,300 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:59:12,301 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:59:12,306 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:59:12,308 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:59:12,309 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:59:12,313 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:59:12,315 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:59:12,316 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:59:12,321 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:59:12,323 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:59:12,326 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:59:12,327 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:59:12,328 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:59:12,330 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:59:15,853 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:59:15,854 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:59:15,855 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:59:15,856 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:59:15,857 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:59:15,859 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:59:15,865 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:59:15,868 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:59:15,869 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:59:15,872 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:59:15,873 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:59:15,882 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:59:15,883 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:59:15,884 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:59:15,886 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:59:19,184 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 13:59:19,185 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 13:59:19,186 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 13:59:19,188 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 13:59:19,189 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 13:59:19,190 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 13:59:19,191 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 13:59:19,193 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 13:59:19,194 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 13:59:19,195 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 13:59:19,197 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 13:59:19,198 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 13:59:19,199 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 13:59:19,200 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 13:59:19,201 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 13:59:19,949 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 13:59:22,969 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 13:59:24,076 [INFO] AFC remote call 1 is done.
2025-06-28 14:01:49,459 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:01:49,460 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:01:49,461 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:01:49,462 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:01:49,463 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:01:49,464 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:01:49,465 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:01:49,466 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:01:49,467 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:01:49,469 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:01:49,470 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:01:49,471 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:01:49,473 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:01:49,474 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:01:49,483 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:39,802 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:39,866 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:39,870 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:39,882 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:39,884 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:39,934 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:39,935 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:40,097 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:40,123 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:40,126 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:40,127 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:40,130 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:40,131 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:40,138 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:40,143 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:40,147 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:40,149 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:40,165 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:40,195 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:40,210 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:40,214 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:40,214 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:40,216 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:40,230 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:40,262 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:40,263 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:40,264 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:40,266 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:40,267 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:40,269 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:40,270 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:40,270 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:40,279 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:40,289 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:40,333 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:40,333 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:40,334 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:40,335 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:40,337 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:40,337 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:40,339 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:40,349 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:40,524 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:40,525 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:40,529 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:40,541 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:40,756 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:40,759 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:40,764 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:40,765 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:40,766 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:40,768 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:40,780 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:40,781 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:40,807 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:40,836 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:40,840 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:40,847 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:40,848 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:40,851 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:40,865 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:40,865 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:40,869 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:40,900 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:40,926 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,095 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:41,096 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,097 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:41,098 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,103 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:41,153 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,166 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,182 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,184 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:41,184 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:41,190 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,196 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:41,198 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:41,199 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:41,202 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:41,203 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:41,245 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:41,245 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,266 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:41,267 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:41,268 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:41,275 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:41,280 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:41,284 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:41,284 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:41,286 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:41,468 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,469 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:41,475 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:41,476 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:41,477 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:41,477 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,478 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,479 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:41,480 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:41,481 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:41,482 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:41,482 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:41,483 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,484 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,485 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:41,485 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:41,486 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:41,487 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:41,488 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:41,490 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,492 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:41,493 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:41,496 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:41,497 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:41,498 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,499 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:41,500 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:41,500 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:41,501 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:41,502 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:41,502 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:41,503 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,504 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,505 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:02:41,506 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:41,507 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:41,508 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:41,512 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:41,513 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:41,513 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:41,514 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,515 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,515 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:02:41,515 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:41,516 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:41,517 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:41,517 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:41,774 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,775 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,775 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:41,776 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:02:41,777 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:41,916 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:41,916 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:41,918 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:41,918 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:41,918 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:41,919 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:02:41,919 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:41,920 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:41,921 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:41,921 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:41,922 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:41,923 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:41,925 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:02:42,070 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:42,071 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:42,072 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:42,072 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:42,073 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:42,073 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:02:42,074 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:42,075 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:42,075 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:42,076 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:42,076 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:42,077 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:02:42,077 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:42,078 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:42,079 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:42,079 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:42,081 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:42,084 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:02:42,085 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:42,226 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:42,227 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:42,228 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:42,228 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:02:42,355 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:42,355 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:42,356 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:42,356 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:02:42,357 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:42,357 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:42,358 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:42,358 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:02:42,359 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:42,361 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:42,362 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:02:42,617 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:42,617 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:02:42,618 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:02:42,619 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:02:42,746 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:04:53,466 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:04:53,468 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:04:53,469 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:04:53,469 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:04:53,470 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:04:53,471 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:04:53,473 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:04:53,474 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:04:53,478 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:04:53,482 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:04:53,483 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:04:53,484 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:04:53,485 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:04:53,486 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:04:53,490 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:04:54,261 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:04:57,530 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 14:04:58,629 [INFO] AFC remote call 1 is done.
2025-06-28 14:05:21,464 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:05:21,465 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:05:21,466 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:05:21,467 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:05:21,468 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:05:21,470 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:05:21,472 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:05:21,475 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:05:21,479 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:05:21,482 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:05:21,484 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:05:21,491 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:05:21,492 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:05:21,493 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:05:21,494 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:05:21,719 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:05:21,720 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:05:21,721 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:05:21,722 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:05:21,723 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:05:21,724 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:05:21,727 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:05:21,730 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:05:21,736 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:05:21,737 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:05:21,738 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:05:21,739 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:05:21,740 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:05:21,742 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:05:21,743 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:05:21,746 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque estou recebendo esse erro nessa aplicação sendo que no outro notebook a mesma aplicação roda perfeitamente ?
Assistant: É frustrante quando o mesmo código se comporta de forma diferente em máquinas distintas! Isso quase sempre aponta para **diferenças no ambiente de execução**, e não no código em si.

Analisando seu código, o principal suspeito para um erro que ocorre em um notebook e não em outro, especialmente se um for Windows e o outro Linux/macOS, é o uso de `PIL.ImageGrab`.

---

**O SUSPEITO PRINCIPAL: `PIL.ImageGrab` (Área de Transferência)**

No seu arquivo `chat_streamlit.py`, você tem a função `check_clipboard()` que usa `PIL.ImageGrab`:

```python
# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab # <--- AQUI ESTÁ O PROVÁVEL PROBLEMA

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        # ... (restante da função)
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}") # <--- ESSA É A MENSAGEM DE ERRO QUE VOCÊ DEVE ESTAR VENDO
        return None, None
```

A biblioteca `PIL.ImageGrab` (parte do Pillow) é altamente dependente do sistema operacional:

1.  **Windows:** Geralmente funciona sem problemas, pois o Windows possui APIs diretas para acesso à área de transferência de imagens.
2.  **Linux:** Requer utilitários de linha de comando como `scrot` ou `gnome-screenshot` instalados e configurados corretamente, e a biblioteca `Pillow` precisa ser compilada com suporte a essas ferramentas. Muitas distribuições Linux não vêm com isso pronto para o `ImageGrab`.
3.  **macOS:** Também tem suas próprias dependências, geralmente exigindo bibliotecas como `pyobjc` para interagir com o ambiente Cocoa.

**Se o seu "outro notebook" roda Windows e o notebook com problema roda Linux ou macOS, este é 99% a causa.** A mensagem de erro `"Erro ao acessar a área de transferência: {e}"` é exatamente o que você veria na barra lateral do Streamlit se essa parte falhasse.

---

**Passos para Diagnosticar e Resolver:**

1.  **Obtenha a Mensagem de Erro Exata:**
    *   No notebook onde a aplicação não roda, verifique a barra lateral do Streamlit. Você verá a mensagem exata do erro que está ocorrendo, começando com `"Erro ao acessar a área de transferência: ..."`. O que vem depois do `:` é crucial.
    *   Se não for `PIL.ImageGrab`, o erro estará em outro lugar.

2.  **Compare os Ambientes:**
    *   **Sistema Operacional:** Qual SO está rodando em cada notebook? (Ex: Windows 10 vs. Ubuntu 22.04 vs. macOS Ventura)
    *   **Versão do Python:** `python --version` em ambos.
    *   **Dependências Instaladas:**
        *   No notebook que funciona, execute: `pip freeze > requirements_working.txt`
        *   No notebook que não funciona, execute: `pip freeze > requirements_broken.txt`
        *   Compare os dois arquivos (`diff requirements_working.txt requirements_broken.txt`). Preste atenção especial à versão do `Pillow` e a presença de pacotes como `pywin32` (Windows) ou qualquer coisa relacionada a screenshots em Linux/macOS.

3.  **Soluções para o `PIL.ImageGrab`:**

    *   **Para Linux (Ubuntu/Debian):**
        *   Instale `scrot` (ou `gnome-screenshot`): `sudo apt-get update && sudo apt-get install scrot` (ou `sudo apt-get install gnome-screenshot`).
        *   Certifique-se de que o `Pillow` foi instalado *após* a instalação dessas ferramentas ou reinstale-o para que ele possa detectar as dependências.
        *   Pode ser necessário também `sudo apt-get install python3-tk python3-pil.imagetk` em algumas distribuições.

    *   **Para macOS:**
        *   Pode ser necessário instalar `pyobjc`: `pip install pyobjc`.

    *   **Solução Alternativa (Desativar ou Condicional):**
        Se você não precisa desesperadamente da funcionalidade de colar imagem da área de transferência e quer que a aplicação funcione em qualquer SO sem dependências externas complexas:
        Você já tem um `try-except` que lida com o erro. A aplicação deve continuar funcionando mesmo se a funcionalidade de colar não funcionar. No entanto, se o erro estiver parando a execução principal, isso é um problema.

        **Para evitar o erro na barra lateral em ambientes não-Windows:**
        Você pode adicionar uma verificação de sistema operacional:
        ```python
        import platform
        # ...

        def check_clipboard():
            # Só tenta importar ImageGrab se for Windows
            if platform.system() == "Windows":
                try:
                    from PIL import ImageGrab
                    img = ImageGrab.grabclipboard()
                    # ... (restante do código se img não for None)
                    if img is not None and isinstance(img, Image.Image):
                        # ... seu código para processar a imagem da área de transferência
                        # ...
                        return ClipboardFile(img_byte_arr), img
                    return None, None
                except Exception as e:
                    # A mensagem já é exibida no Streamlit, então não precisa de outro print aqui
                    st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
                    return None, None
            else:
                # Se não for Windows, não tenta acessar a área de transferência
                # E não mostra erro na barra lateral
                # st.sidebar.info("Colar da área de transferência disponível apenas no Windows.")
                return None, None
        ```
        Isso impediria que o erro específico de `ImageGrab` fosse tentado em sistemas não-Windows.

4.  **Verifique a Chave da API Gemini:**
    *   Certifique-se de que o arquivo `.env` existe no diretório raiz da aplicação no notebook problemático.
    *   Certifique-se de que `API_KEY_GEMINI` está definida corretamente dentro dele (Ex: `API_KEY_GEMINI="SUA_CHAVE_AQUI"`).
    *   Um erro relacionado à API Key geralmente resultaria em `ValueError("API Key não encontrada nas variáveis de ambiente")` ou erros da própria API (4xx, 5xx).

5.  **Verifique as Versões das Dependências:**
    *   É altamente recomendável ter um `requirements.txt` e instalá-lo em ambos os ambientes com `pip install -r requirements.txt`.
    *   Pode haver incompatibilidades sutis entre versões de `streamlit`, `google-generativeai`, `Pillow`, etc.

6.  **Permissões de Diretório:**
    *   Verifique se a aplicação tem permissão para criar e escrever nos diretórios `assets/`, `logs/` e `processed_images/` no notebook problemático. Isso é mais comum em Linux/macOS com permissões restritas.

7.  **Conectividade de Rede/Firewall:**
    *   O notebook problemático consegue acessar a internet? Existem restrições de firewall que podem estar bloqueando o acesso aos endpoints da API do Gemini?

---

**Resumo da Ação Mais Provável:**

O erro `Erro ao acessar a área de transferência: {e}` na barra lateral do Streamlit é quase uma certeza de que o problema está na tentativa de usar `PIL.ImageGrab` em um sistema operacional (provavelmente Linux/macOS) que não tem as dependências necessárias para isso.

Por favor, forneça a mensagem de erro exata da barra lateral e o sistema operacional de ambos os notebooks para uma análise mais precisa!
User: ois dois ambientes são windows. 
Assistant: Certo, isso muda o cenário! Se ambos os ambientes são Windows, então o problema não é a incompatibilidade do `PIL.ImageGrab` com Linux/macOS. No entanto, o `PIL.ImageGrab` continua sendo o principal suspeito, mas por razões diferentes.

Quando `PIL.ImageGrab` falha no Windows, geralmente aponta para um problema com a biblioteca `pywin32`, que o Pillow usa para interagir com as APIs do sistema operacional Windows.

Vamos desvendar isso.

---

**Causas Mais Comuns do `PIL.ImageGrab` Falhar no Windows:**

1.  **`pywin32` Ausente ou Corrompido:** A biblioteca `pywin32` é essencial para que o Pillow (que inclui `PIL.ImageGrab`) acesse a área de transferência do Windows de forma nativa. Se ela não estiver instalada ou estiver corrompida, a função falhará.
2.  **Ambientes Python Diferentes:** Mesmo que ambos sejam Windows, você pode ter ambientes Python (globais ou virtuais) ligeiramente diferentes em cada máquina, onde uma tem `pywin32` e a outra não, ou tem uma versão incompatível.
3.  **Conflitos de Versão:** Raramente, pode haver um conflito entre a versão do `Pillow` e a versão do `pywin32`, ou até mesmo com a versão do Python.
4.  **Conteúdo da Área de Transferência:** Embora menos provável dado seu relato, o `ImageGrab.grabclipboard()` retornará `None` se o conteúdo da área de transferência não for uma imagem (ex: texto, arquivos). No entanto, o erro que você vê sugere que o *acesso* à área de transferência está falhando, não apenas que ela está vazia.

---

**Passos Detalhados de Diagnóstico e Solução:**

**1. Obtenha a Mensagem de Erro Exata:**
   Este é o passo mais crucial. Quando você executa a aplicação no notebook que está dando problema, qual é a *mensagem de erro completa* que aparece na barra lateral do Streamlit, após "Erro ao acessar a área de transferência: "?
   Ex: `"Erro ao acessar a área de transferência: [WinError 5] Acesso negado."` ou `"Erro ao acessar a área de transferência: No module named 'win32clipboard'"` etc.
   Esta mensagem detalhada nos dirá exatamente o que está acontecendo.

**2. Compare as Dependências `pip`:**

   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que NÃO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos:** Abra `requirements_working.txt` e `requirements_broken.txt` e procure por:
        *   `Pillow` (P-I-L-L-O-W): Verifique as versões.
        *   `pywin32`: **Este é o mais importante.** Ele deve estar presente no `requirements_working.txt`. Se não estiver no `requirements_broken.txt` ou se a versão for diferente, encontramos o provável culpado.
        *   `streamlit`: Verifique as versões.

**3. Instale/Reinstale `pywin32` (no notebook problemático):**

   Se `pywin32` estiver faltando ou com uma versão diferente, tente o seguinte:

   *   **Verifique o ambiente virtual:** Se você estiver usando um ambiente virtual (altamente recomendado), ative-o primeiro.
        ```bash
        # Exemplo para venv:
        cd C:\caminho\para\seu\projeto
        .\.venv\Scripts\activate  # ou .\venv\Scripts\activate
        ```
   *   **Instale `pywin32`:**
        ```bash
        pip install pywin32
        ```
   *   **Ou, se já estiver instalado, force a reinstalação:**
        ```bash
        pip install --upgrade --force-reinstall pywin32
        ```
   *   **Reinicie a aplicação Streamlit.**

**4. Teste a Funcionalidade `ImageGrab` Isoladamente:**

   Para confirmar que o problema é realmente com `ImageGrab` e não algo mais amplo com o Streamlit ou seu código:

   *   Crie um pequeno script Python (`test_clipboard.py`) no notebook problemático:
        ```python
        import sys
        from PIL import ImageGrab, Image
        import io
        import time

        try:
            # Tenta pegar imagem da área de transferência
            img = ImageGrab.grabclipboard()

            if img is not None and isinstance(img, Image.Image):
                print("Imagem encontrada na área de transferência!")
                # Opcional: Salvar a imagem para verificar
                # img.save("clipboard_test.png")
                # print("Imagem salva como clipboard_test.png")
                
                # Tenta converter para bytes (simular o que você faz no Streamlit)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format='PNG')
                img_byte_arr.seek(0)
                print(f"Tamanho dos bytes: {len(img_byte_arr.getvalue())} bytes")
                
            elif img is None:
                print("Nenhuma imagem na área de transferência ou o conteúdo não é uma imagem.")
            else:
                print(f"Tipo de conteúdo inesperado na área de transferência: {type(img)}")

        except ImportError as e:
            print(f"Erro: Dependência faltando para PIL.ImageGrab: {e}")
            print("Tente 'pip install pywin32' e certifique-se de que está no ambiente correto.")
        except Exception as e:
            print(f"Erro inesperado ao acessar a área de transferência: {e}")
            print("Verifique permissões ou possíveis conflitos.")

        print("\nTeste concluído.")
        ```
   *   **Copie uma imagem** (não texto) para a área de transferência no Windows.
   *   Execute este script no terminal (ativando seu ambiente virtual, se aplicável): `python test_clipboard.py`
   *   Observe a saída. Isso confirmará se o `ImageGrab` está funcionando fora do Streamlit e qual erro exato ele gera.

**5. Verifique a Versão do Python e Arquitetura:**

   Embora menos comum hoje em dia, um Python de 32 bits pode ter problemas com algumas bibliotecas de 64 bits.

   *   Execute `python -c "import platform; print(platform.architecture())"` em ambos os notebooks. Verifique se ambos são `('64bit', 'WindowsPE')`.

**6. Outras Verificações (Menos Prováveis):**

*   **Antivírus/Firewall:** Muito raramente, um software de segurança pode interferir no acesso a certas APIs do sistema. Tente desativá-lo temporariamente (se for seguro fazer isso) para testar.
*   **Permissões de Usuário:** Certifique-se de que o usuário que executa a aplicação Streamlit tem as permissões necessárias para interagir com o sistema.

---

**Em Resumo:**

A principal ação é obter a **mensagem de erro exata** da barra lateral do Streamlit e, em seguida, **comparar as dependências `pip freeze`** para identificar se `pywin32` está faltando ou tem uma versão diferente. O teste isolado com `test_clipboard.py` também é muito útil para isolar o problema.

Assim que você tiver a mensagem de erro exata, poderemos refinar ainda mais o diagnóstico!
User: Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E13620360>
2025-06-28 14:05:48,121 [INFO] Resposta da IA (texto): Ok, essa nova informação muda completamente o diagnóstico! O erro `Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x...>` **não tem a ver com a área de transferência (`PIL.ImageGrab`)**.

Este erro significa que a biblioteca `Pillow` (PIL) tentou abrir algo que não reconheceu como uma imagem válida. A mensagem `_io.BytesIO object` indica que ela recebeu um fluxo de bytes, mas esses bytes não formam uma imagem (JPEG, PNG, etc.) que ela possa decodificar.

Vamos focar na função `generate_image` no `chat_streamlit.py`:

```python
# chat_streamlit.py

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # ... (verificação da API_KEY) ...

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text) # Se for só texto, imprime e continua o loop ou retorna None
            elif part.inline_data is not None:
                # <-- AQUI É O PONTO DO ERRO
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image # Retorna a imagem e sai da função

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # <-- É aqui que o erro é capturado e exibido
        return None
```

A linha `image = Image.open(io.BytesIO(part.inline_data.data))` é a que está falhando. Isso significa que, por algum motivo, o `part.inline_data.data` que o Gemini está retornando **não é um conjunto de bytes de uma imagem válida** no notebook problemático, ou está vazio.

---

**Possíveis Causas e Como Depurar:**

1.  **O Gemini não está gerando uma imagem para aquele prompt/requisição específica:**
    *   Às vezes, a API de geração de imagem do Gemini pode não conseguir gerar uma imagem para um prompt, ou pode bloquear a geração por motivos de segurança/conteúdo. Nesses casos, `part.inline_data` pode ser `None` ou `part.inline_data.data` pode estar vazio ou conter dados de erro/texto em vez de dados de imagem.
    *   Se o `response.prompt_feedback` tiver um `block_reason` (motivo de bloqueio), isso pode indicar o problema.

2.  **Problema de conectividade/API key:**
    *   Embora você já tenha verificado, é bom garantir que a `API_KEY_GEMINI` esteja configurada *exatamente* da mesma forma em ambos os notebooks. Um problema na chave pode levar a respostas malformadas da API, mesmo que não seja um erro óbvio de autenticação.
    *   Há alguma restrição de rede/firewall no notebook problemático que possa estar afetando a comunicação com a API do Gemini, fazendo com que as respostas venham incompletas ou corrompidas?

3.  **Versão da biblioteca `google-generativeai` ou `Pillow`:**
    *   Mesmo sendo Windows, pode haver pequenas diferenças nas versões dessas bibliotecas que causem um comportamento diferente.

---

**Passos para Diagnosticar e Resolver:**

**1. Teste o `test_image_generation.py` no notebook problemático:**

   Você tem um script de teste simples (`test_image_generation.py`) que faz uma chamada direta para a API de geração de imagem.
   *   Copie e execute o `test_image_generation.py` no notebook que está apresentando o erro.
   *   Ele deve tentar gerar "ed-image.png".
   *   **Resultado esperado:**
        *   **Se `test_image_generation.py` também falhar com o mesmo erro (`cannot identify image file`),** então o problema é mais fundamental na comunicação com o Gemini ou na forma como os dados são recebidos/processados pelo Pillow *naquele ambiente*.
        *   **Se `test_image_generation.py` funcionar e gerar a imagem corretamente,** então o problema está em algo mais específico no fluxo do `chat_streamlit.py` ao chamar `generate_image`.

**2. Adicione Logs Detalhados na Função `generate_image` (em `chat_streamlit.py`):**

   Vamos adicionar mais `print` statements (ou usar `logger.info` se preferir ver no logfile) para entender o que está vindo da API antes de chamar `Image.open()`.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Certifique-se que io está importado

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI não encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug print

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG ---
           print(f"\nDEBUG: Objeto de resposta Gemini completo: {response}")
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Razão de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Geração de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # Não há imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA não retornou candidatos de geração. Tente um prompt diferente.")
               return None

           found_image = False
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo: {type(part)}")
               if part.text is not None:
                   print(f"DEBUG: Conteúdo de texto recebido: '{part.text}'")
                   # Se houver texto, você pode decidir se quer usá-lo ou continuar procurando por imagem
               elif part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Tenta abrir a imagem
                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data é None ou vazio, apesar de inline_data não ser None.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline.")
           
           if not found_image:
               st.warning("O Gemini não retornou uma imagem para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este é o erro que você está vendo
           return None
   ```
   Execute a aplicação com estes prints e observe a saída no terminal onde você executa `streamlit run chat_streamlit.py`. Isso nos dará pistas vitais sobre o que o Gemini está realmente retornando.

**3. Compare as Dependências Novamente:**

   Embora menos provável, vale a pena uma revisão minuciosa.
   *   No notebook que FUNCIONA, execute: `pip freeze > requirements_working.txt`
   *   No notebook que NÃO FUNCIONA, execute: `pip freeze > requirements_broken.txt`
   *   Compare os arquivos, prestando atenção especial a:
        *   `Pillow` (ou PIL)
        *   `google-generativeai`
        *   `streamlit`
   *   Se houver diferença, tente instalar as versões do `requirements_working.txt` no notebook problemático.

**4. Reinstale `Pillow`:**

   Mesmo que as versões sejam as mesmas, uma instalação corrompida pode ser o problema.
   *   Ative seu ambiente virtual (se estiver usando).
   *   Execute: `pip install --upgrade --force-reinstall Pillow`
   *   Reinicie a aplicação Streamlit.

---

**Conclusão Preliminar:**

O erro `cannot identify image file` na função `generate_image` sugere fortemente que os dados que o Gemini retorna para `part.inline_data.data` **não são dados de imagem válidos** no ambiente problemático. Os passos de depuração acima, especialmente a execução do `test_image_generation.py` e os `print`s detalhados na função `generate_image` no `chat_streamlit.py`, nos darão as informações necessárias para identificar a causa raiz.
2025-06-28 14:05:48,351 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:05:48,352 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:05:48,353 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:05:48,354 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:05:48,355 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:05:48,356 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:05:48,358 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:05:48,359 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:05:48,360 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:05:48,361 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:05:48,363 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:05:48,364 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:05:48,365 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:05:48,370 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:05:48,372 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:07:06,186 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:07:06,188 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:07:06,189 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:07:06,190 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:07:06,191 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:07:06,192 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:07:06,193 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:07:06,198 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:07:06,199 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:07:06,201 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:07:06,202 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:07:06,204 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:07:06,205 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:07:06,206 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:07:06,208 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:07:06,726 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:07:06,727 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:07:06,728 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:07:06,729 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:07:06,729 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:07:06,730 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:07:06,731 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:07:06,733 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:07:06,734 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:07:06,736 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:07:06,738 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:07:06,739 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:07:06,741 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:07:06,742 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:07:06,744 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:07:06,750 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque estou recebendo esse erro nessa aplicação sendo que no outro notebook a mesma aplicação roda perfeitamente ?
Assistant: É frustrante quando o mesmo código se comporta de forma diferente em máquinas distintas! Isso quase sempre aponta para **diferenças no ambiente de execução**, e não no código em si.

Analisando seu código, o principal suspeito para um erro que ocorre em um notebook e não em outro, especialmente se um for Windows e o outro Linux/macOS, é o uso de `PIL.ImageGrab`.

---

**O SUSPEITO PRINCIPAL: `PIL.ImageGrab` (Área de Transferência)**

No seu arquivo `chat_streamlit.py`, você tem a função `check_clipboard()` que usa `PIL.ImageGrab`:

```python
# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab # <--- AQUI ESTÁ O PROVÁVEL PROBLEMA

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        # ... (restante da função)
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}") # <--- ESSA É A MENSAGEM DE ERRO QUE VOCÊ DEVE ESTAR VENDO
        return None, None
```

A biblioteca `PIL.ImageGrab` (parte do Pillow) é altamente dependente do sistema operacional:

1.  **Windows:** Geralmente funciona sem problemas, pois o Windows possui APIs diretas para acesso à área de transferência de imagens.
2.  **Linux:** Requer utilitários de linha de comando como `scrot` ou `gnome-screenshot` instalados e configurados corretamente, e a biblioteca `Pillow` precisa ser compilada com suporte a essas ferramentas. Muitas distribuições Linux não vêm com isso pronto para o `ImageGrab`.
3.  **macOS:** Também tem suas próprias dependências, geralmente exigindo bibliotecas como `pyobjc` para interagir com o ambiente Cocoa.

**Se o seu "outro notebook" roda Windows e o notebook com problema roda Linux ou macOS, este é 99% a causa.** A mensagem de erro `"Erro ao acessar a área de transferência: {e}"` é exatamente o que você veria na barra lateral do Streamlit se essa parte falhasse.

---

**Passos para Diagnosticar e Resolver:**

1.  **Obtenha a Mensagem de Erro Exata:**
    *   No notebook onde a aplicação não roda, verifique a barra lateral do Streamlit. Você verá a mensagem exata do erro que está ocorrendo, começando com `"Erro ao acessar a área de transferência: ..."`. O que vem depois do `:` é crucial.
    *   Se não for `PIL.ImageGrab`, o erro estará em outro lugar.

2.  **Compare os Ambientes:**
    *   **Sistema Operacional:** Qual SO está rodando em cada notebook? (Ex: Windows 10 vs. Ubuntu 22.04 vs. macOS Ventura)
    *   **Versão do Python:** `python --version` em ambos.
    *   **Dependências Instaladas:**
        *   No notebook que funciona, execute: `pip freeze > requirements_working.txt`
        *   No notebook que não funciona, execute: `pip freeze > requirements_broken.txt`
        *   Compare os dois arquivos (`diff requirements_working.txt requirements_broken.txt`). Preste atenção especial à versão do `Pillow` e a presença de pacotes como `pywin32` (Windows) ou qualquer coisa relacionada a screenshots em Linux/macOS.

3.  **Soluções para o `PIL.ImageGrab`:**

    *   **Para Linux (Ubuntu/Debian):**
        *   Instale `scrot` (ou `gnome-screenshot`): `sudo apt-get update && sudo apt-get install scrot` (ou `sudo apt-get install gnome-screenshot`).
        *   Certifique-se de que o `Pillow` foi instalado *após* a instalação dessas ferramentas ou reinstale-o para que ele possa detectar as dependências.
        *   Pode ser necessário também `sudo apt-get install python3-tk python3-pil.imagetk` em algumas distribuições.

    *   **Para macOS:**
        *   Pode ser necessário instalar `pyobjc`: `pip install pyobjc`.

    *   **Solução Alternativa (Desativar ou Condicional):**
        Se você não precisa desesperadamente da funcionalidade de colar imagem da área de transferência e quer que a aplicação funcione em qualquer SO sem dependências externas complexas:
        Você já tem um `try-except` que lida com o erro. A aplicação deve continuar funcionando mesmo se a funcionalidade de colar não funcionar. No entanto, se o erro estiver parando a execução principal, isso é um problema.

        **Para evitar o erro na barra lateral em ambientes não-Windows:**
        Você pode adicionar uma verificação de sistema operacional:
        ```python
        import platform
        # ...

        def check_clipboard():
            # Só tenta importar ImageGrab se for Windows
            if platform.system() == "Windows":
                try:
                    from PIL import ImageGrab
                    img = ImageGrab.grabclipboard()
                    # ... (restante do código se img não for None)
                    if img is not None and isinstance(img, Image.Image):
                        # ... seu código para processar a imagem da área de transferência
                        # ...
                        return ClipboardFile(img_byte_arr), img
                    return None, None
                except Exception as e:
                    # A mensagem já é exibida no Streamlit, então não precisa de outro print aqui
                    st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
                    return None, None
            else:
                # Se não for Windows, não tenta acessar a área de transferência
                # E não mostra erro na barra lateral
                # st.sidebar.info("Colar da área de transferência disponível apenas no Windows.")
                return None, None
        ```
        Isso impediria que o erro específico de `ImageGrab` fosse tentado em sistemas não-Windows.

4.  **Verifique a Chave da API Gemini:**
    *   Certifique-se de que o arquivo `.env` existe no diretório raiz da aplicação no notebook problemático.
    *   Certifique-se de que `API_KEY_GEMINI` está definida corretamente dentro dele (Ex: `API_KEY_GEMINI="SUA_CHAVE_AQUI"`).
    *   Um erro relacionado à API Key geralmente resultaria em `ValueError("API Key não encontrada nas variáveis de ambiente")` ou erros da própria API (4xx, 5xx).

5.  **Verifique as Versões das Dependências:**
    *   É altamente recomendável ter um `requirements.txt` e instalá-lo em ambos os ambientes com `pip install -r requirements.txt`.
    *   Pode haver incompatibilidades sutis entre versões de `streamlit`, `google-generativeai`, `Pillow`, etc.

6.  **Permissões de Diretório:**
    *   Verifique se a aplicação tem permissão para criar e escrever nos diretórios `assets/`, `logs/` e `processed_images/` no notebook problemático. Isso é mais comum em Linux/macOS com permissões restritas.

7.  **Conectividade de Rede/Firewall:**
    *   O notebook problemático consegue acessar a internet? Existem restrições de firewall que podem estar bloqueando o acesso aos endpoints da API do Gemini?

---

**Resumo da Ação Mais Provável:**

O erro `Erro ao acessar a área de transferência: {e}` na barra lateral do Streamlit é quase uma certeza de que o problema está na tentativa de usar `PIL.ImageGrab` em um sistema operacional (provavelmente Linux/macOS) que não tem as dependências necessárias para isso.

Por favor, forneça a mensagem de erro exata da barra lateral e o sistema operacional de ambos os notebooks para uma análise mais precisa!
User: ois dois ambientes são windows. 
Assistant: Certo, isso muda o cenário! Se ambos os ambientes são Windows, então o problema não é a incompatibilidade do `PIL.ImageGrab` com Linux/macOS. No entanto, o `PIL.ImageGrab` continua sendo o principal suspeito, mas por razões diferentes.

Quando `PIL.ImageGrab` falha no Windows, geralmente aponta para um problema com a biblioteca `pywin32`, que o Pillow usa para interagir com as APIs do sistema operacional Windows.

Vamos desvendar isso.

---

**Causas Mais Comuns do `PIL.ImageGrab` Falhar no Windows:**

1.  **`pywin32` Ausente ou Corrompido:** A biblioteca `pywin32` é essencial para que o Pillow (que inclui `PIL.ImageGrab`) acesse a área de transferência do Windows de forma nativa. Se ela não estiver instalada ou estiver corrompida, a função falhará.
2.  **Ambientes Python Diferentes:** Mesmo que ambos sejam Windows, você pode ter ambientes Python (globais ou virtuais) ligeiramente diferentes em cada máquina, onde uma tem `pywin32` e a outra não, ou tem uma versão incompatível.
3.  **Conflitos de Versão:** Raramente, pode haver um conflito entre a versão do `Pillow` e a versão do `pywin32`, ou até mesmo com a versão do Python.
4.  **Conteúdo da Área de Transferência:** Embora menos provável dado seu relato, o `ImageGrab.grabclipboard()` retornará `None` se o conteúdo da área de transferência não for uma imagem (ex: texto, arquivos). No entanto, o erro que você vê sugere que o *acesso* à área de transferência está falhando, não apenas que ela está vazia.

---

**Passos Detalhados de Diagnóstico e Solução:**

**1. Obtenha a Mensagem de Erro Exata:**
   Este é o passo mais crucial. Quando você executa a aplicação no notebook que está dando problema, qual é a *mensagem de erro completa* que aparece na barra lateral do Streamlit, após "Erro ao acessar a área de transferência: "?
   Ex: `"Erro ao acessar a área de transferência: [WinError 5] Acesso negado."` ou `"Erro ao acessar a área de transferência: No module named 'win32clipboard'"` etc.
   Esta mensagem detalhada nos dirá exatamente o que está acontecendo.

**2. Compare as Dependências `pip`:**

   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que NÃO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos:** Abra `requirements_working.txt` e `requirements_broken.txt` e procure por:
        *   `Pillow` (P-I-L-L-O-W): Verifique as versões.
        *   `pywin32`: **Este é o mais importante.** Ele deve estar presente no `requirements_working.txt`. Se não estiver no `requirements_broken.txt` ou se a versão for diferente, encontramos o provável culpado.
        *   `streamlit`: Verifique as versões.

**3. Instale/Reinstale `pywin32` (no notebook problemático):**

   Se `pywin32` estiver faltando ou com uma versão diferente, tente o seguinte:

   *   **Verifique o ambiente virtual:** Se você estiver usando um ambiente virtual (altamente recomendado), ative-o primeiro.
        ```bash
        # Exemplo para venv:
        cd C:\caminho\para\seu\projeto
        .\.venv\Scripts\activate  # ou .\venv\Scripts\activate
        ```
   *   **Instale `pywin32`:**
        ```bash
        pip install pywin32
        ```
   *   **Ou, se já estiver instalado, force a reinstalação:**
        ```bash
        pip install --upgrade --force-reinstall pywin32
        ```
   *   **Reinicie a aplicação Streamlit.**

**4. Teste a Funcionalidade `ImageGrab` Isoladamente:**

   Para confirmar que o problema é realmente com `ImageGrab` e não algo mais amplo com o Streamlit ou seu código:

   *   Crie um pequeno script Python (`test_clipboard.py`) no notebook problemático:
        ```python
        import sys
        from PIL import ImageGrab, Image
        import io
        import time

        try:
            # Tenta pegar imagem da área de transferência
            img = ImageGrab.grabclipboard()

            if img is not None and isinstance(img, Image.Image):
                print("Imagem encontrada na área de transferência!")
                # Opcional: Salvar a imagem para verificar
                # img.save("clipboard_test.png")
                # print("Imagem salva como clipboard_test.png")
                
                # Tenta converter para bytes (simular o que você faz no Streamlit)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format='PNG')
                img_byte_arr.seek(0)
                print(f"Tamanho dos bytes: {len(img_byte_arr.getvalue())} bytes")
                
            elif img is None:
                print("Nenhuma imagem na área de transferência ou o conteúdo não é uma imagem.")
            else:
                print(f"Tipo de conteúdo inesperado na área de transferência: {type(img)}")

        except ImportError as e:
            print(f"Erro: Dependência faltando para PIL.ImageGrab: {e}")
            print("Tente 'pip install pywin32' e certifique-se de que está no ambiente correto.")
        except Exception as e:
            print(f"Erro inesperado ao acessar a área de transferência: {e}")
            print("Verifique permissões ou possíveis conflitos.")

        print("\nTeste concluído.")
        ```
   *   **Copie uma imagem** (não texto) para a área de transferência no Windows.
   *   Execute este script no terminal (ativando seu ambiente virtual, se aplicável): `python test_clipboard.py`
   *   Observe a saída. Isso confirmará se o `ImageGrab` está funcionando fora do Streamlit e qual erro exato ele gera.

**5. Verifique a Versão do Python e Arquitetura:**

   Embora menos comum hoje em dia, um Python de 32 bits pode ter problemas com algumas bibliotecas de 64 bits.

   *   Execute `python -c "import platform; print(platform.architecture())"` em ambos os notebooks. Verifique se ambos são `('64bit', 'WindowsPE')`.

**6. Outras Verificações (Menos Prováveis):**

*   **Antivírus/Firewall:** Muito raramente, um software de segurança pode interferir no acesso a certas APIs do sistema. Tente desativá-lo temporariamente (se for seguro fazer isso) para testar.
*   **Permissões de Usuário:** Certifique-se de que o usuário que executa a aplicação Streamlit tem as permissões necessárias para interagir com o sistema.

---

**Em Resumo:**

A principal ação é obter a **mensagem de erro exata** da barra lateral do Streamlit e, em seguida, **comparar as dependências `pip freeze`** para identificar se `pywin32` está faltando ou tem uma versão diferente. O teste isolado com `test_clipboard.py` também é muito útil para isolar o problema.

Assim que você tiver a mensagem de erro exata, poderemos refinar ainda mais o diagnóstico!
User: Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E13620360>
Assistant: Ok, essa nova informação muda completamente o diagnóstico! O erro `Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x...>` **não tem a ver com a área de transferência (`PIL.ImageGrab`)**.

Este erro significa que a biblioteca `Pillow` (PIL) tentou abrir algo que não reconheceu como uma imagem válida. A mensagem `_io.BytesIO object` indica que ela recebeu um fluxo de bytes, mas esses bytes não formam uma imagem (JPEG, PNG, etc.) que ela possa decodificar.

Vamos focar na função `generate_image` no `chat_streamlit.py`:

```python
# chat_streamlit.py

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # ... (verificação da API_KEY) ...

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text) # Se for só texto, imprime e continua o loop ou retorna None
            elif part.inline_data is not None:
                # <-- AQUI É O PONTO DO ERRO
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image # Retorna a imagem e sai da função

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # <-- É aqui que o erro é capturado e exibido
        return None
```

A linha `image = Image.open(io.BytesIO(part.inline_data.data))` é a que está falhando. Isso significa que, por algum motivo, o `part.inline_data.data` que o Gemini está retornando **não é um conjunto de bytes de uma imagem válida** no notebook problemático, ou está vazio.

---

**Possíveis Causas e Como Depurar:**

1.  **O Gemini não está gerando uma imagem para aquele prompt/requisição específica:**
    *   Às vezes, a API de geração de imagem do Gemini pode não conseguir gerar uma imagem para um prompt, ou pode bloquear a geração por motivos de segurança/conteúdo. Nesses casos, `part.inline_data` pode ser `None` ou `part.inline_data.data` pode estar vazio ou conter dados de erro/texto em vez de dados de imagem.
    *   Se o `response.prompt_feedback` tiver um `block_reason` (motivo de bloqueio), isso pode indicar o problema.

2.  **Problema de conectividade/API key:**
    *   Embora você já tenha verificado, é bom garantir que a `API_KEY_GEMINI` esteja configurada *exatamente* da mesma forma em ambos os notebooks. Um problema na chave pode levar a respostas malformadas da API, mesmo que não seja um erro óbvio de autenticação.
    *   Há alguma restrição de rede/firewall no notebook problemático que possa estar afetando a comunicação com a API do Gemini, fazendo com que as respostas venham incompletas ou corrompidas?

3.  **Versão da biblioteca `google-generativeai` ou `Pillow`:**
    *   Mesmo sendo Windows, pode haver pequenas diferenças nas versões dessas bibliotecas que causem um comportamento diferente.

---

**Passos para Diagnosticar e Resolver:**

**1. Teste o `test_image_generation.py` no notebook problemático:**

   Você tem um script de teste simples (`test_image_generation.py`) que faz uma chamada direta para a API de geração de imagem.
   *   Copie e execute o `test_image_generation.py` no notebook que está apresentando o erro.
   *   Ele deve tentar gerar "ed-image.png".
   *   **Resultado esperado:**
        *   **Se `test_image_generation.py` também falhar com o mesmo erro (`cannot identify image file`),** então o problema é mais fundamental na comunicação com o Gemini ou na forma como os dados são recebidos/processados pelo Pillow *naquele ambiente*.
        *   **Se `test_image_generation.py` funcionar e gerar a imagem corretamente,** então o problema está em algo mais específico no fluxo do `chat_streamlit.py` ao chamar `generate_image`.

**2. Adicione Logs Detalhados na Função `generate_image` (em `chat_streamlit.py`):**

   Vamos adicionar mais `print` statements (ou usar `logger.info` se preferir ver no logfile) para entender o que está vindo da API antes de chamar `Image.open()`.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Certifique-se que io está importado

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI não encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug print

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG ---
           print(f"\nDEBUG: Objeto de resposta Gemini completo: {response}")
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Razão de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Geração de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # Não há imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA não retornou candidatos de geração. Tente um prompt diferente.")
               return None

           found_image = False
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo: {type(part)}")
               if part.text is not None:
                   print(f"DEBUG: Conteúdo de texto recebido: '{part.text}'")
                   # Se houver texto, você pode decidir se quer usá-lo ou continuar procurando por imagem
               elif part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Tenta abrir a imagem
                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data é None ou vazio, apesar de inline_data não ser None.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline.")
           
           if not found_image:
               st.warning("O Gemini não retornou uma imagem para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este é o erro que você está vendo
           return None
   ```
   Execute a aplicação com estes prints e observe a saída no terminal onde você executa `streamlit run chat_streamlit.py`. Isso nos dará pistas vitais sobre o que o Gemini está realmente retornando.

**3. Compare as Dependências Novamente:**

   Embora menos provável, vale a pena uma revisão minuciosa.
   *   No notebook que FUNCIONA, execute: `pip freeze > requirements_working.txt`
   *   No notebook que NÃO FUNCIONA, execute: `pip freeze > requirements_broken.txt`
   *   Compare os arquivos, prestando atenção especial a:
        *   `Pillow` (ou PIL)
        *   `google-generativeai`
        *   `streamlit`
   *   Se houver diferença, tente instalar as versões do `requirements_working.txt` no notebook problemático.

**4. Reinstale `Pillow`:**

   Mesmo que as versões sejam as mesmas, uma instalação corrompida pode ser o problema.
   *   Ative seu ambiente virtual (se estiver usando).
   *   Execute: `pip install --upgrade --force-reinstall Pillow`
   *   Reinicie a aplicação Streamlit.

---

**Conclusão Preliminar:**

O erro `cannot identify image file` na função `generate_image` sugere fortemente que os dados que o Gemini retorna para `part.inline_data.data` **não são dados de imagem válidos** no ambiente problemático. Os passos de depuração acima, especialmente a execução do `test_image_generation.py` e os `print`s detalhados na função `generate_image` no `chat_streamlit.py`, nos darão as informações necessárias para identificar a causa raiz.
User: mas essa mesma aplicação funciona no meu outro notebook sem erros.
2025-06-28 14:07:31,291 [INFO] Resposta da IA (texto): Entendido! Se o problema persiste no segundo notebook, mesmo ambos sendo Windows, e o erro é `cannot identify image file <_io.BytesIO object at 0x...>`, isso significa que a questão não é a compatibilidade do sistema operacional com `PIL.ImageGrab`, mas sim algo específico na forma como os **dados da imagem gerada pelo Gemini** estão chegando ou sendo processados no notebook problemático.

Este erro (`cannot identify image file`) ocorre quando a biblioteca `Pillow` (que é usada para abrir a imagem com `Image.open`) recebe um fluxo de bytes (`_io.BytesIO`) que ela não consegue reconhecer como um formato de imagem válido (PNG, JPG, etc.).

Vamos focar na função `generate_image` no `chat_streamlit.py` e nos dados que ela recebe da API do Gemini.

### Causas Prováveis do Erro `cannot identify image file` (mesmo em Windows):

1.  **Dados de Imagem Inválidos/Incompletos da API do Gemini:**
    *   **API Key com Problema:** Embora você tenha verificado, às vezes uma API Key pode ter restrições, ou a própria chamada de API pode estar retornando uma resposta malformada ou um erro sutil que não é um `HTTP 200 OK` com dados de imagem.
    *   **Bloqueio de Conteúdo:** O prompt pode estar sendo bloqueado pelos filtros de segurança do Gemini, e em vez de uma imagem, a API retorna uma resposta vazia, uma mensagem de erro em texto, ou um `prompt_feedback` indicando um bloqueio, mas o código tenta processar `inline_data.data` como se fosse uma imagem.
    *   **Problema de Conectividade de Rede:** No notebook problemático, pode haver alguma instabilidade de rede, firewall ou proxy que está corrompendo ou truncando os dados da imagem durante a transferência da API do Gemini para a sua aplicação. Os bytes chegam incompletos ou inválidos, e o `Pillow` não consegue decodificá-los.

2.  **Problema na Instalação ou Versão do `Pillow` (ou `google-generativeai`):**
    *   **Instalação Corrompida:** Apesar de ser Windows, a instalação do Pillow pode ter algum arquivo corrompido ou faltando no ambiente Python do notebook problemático.
    *   **Diferença de Versão:** Se as versões do `Pillow` ou `google-generativeai` forem ligeiramente diferentes entre os dois notebooks, pode haver uma incompatibilidade sutil. Ex: Uma versão antiga do Pillow pode não suportar um formato específico que o Gemini está retornando, ou uma versão diferente do `google-generativeai` pode lidar com a resposta da API de forma distinta.

### Passos de Diagnóstico e Solução Detalhados:

**1. Re-execute o `test_image_generation.py` no notebook problemático.**
   Este é o teste mais importante para isolar o problema. Ele usa a mesma lógica de chamada de API e de abertura de imagem.
   *   Execute `python test_image_generation.py` no terminal do notebook que está com problema.
   *   **Observe a saída:**
        *   Se ele **também falhar com o mesmo erro** (`cannot identify image file`), então o problema é fundamental na forma como os dados chegam da API ou como o `Pillow` os interpreta *naquele ambiente*.
        *   Se ele **funcionar e gerar a imagem**, então o problema é mais específico no fluxo do `chat_streamlit.py` (embora isso seja menos provável com base no erro que você relatou).

**2. Adicione Logs Detalhados na Função `generate_image` (em `chat_streamlit.py`)**

   Isso nos permitirá ver exatamente o que o Gemini está retornando antes que o `Pillow` tente abrir a imagem.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Verifique se 'io' está importado no topo do arquivo

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI não encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug no Streamlit
           print(f"\nDEBUG: Chamando API do Gemini para prompt: '{prompt}'") # Debug no terminal

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG (principalmente no terminal) ---
           print(f"\nDEBUG: Resposta completa do Gemini recebida. Tipo: {type(response)}")
           
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Razão de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Geração de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # Não há imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA não retornou candidatos de geração. Tente um prompt diferente.")
               return None

           found_image = False
           # Assumimos que sempre pegamos o primeiro candidato (response.candidates[0])
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo do objeto part: {type(part)}")
               
               if hasattr(part, 'text') and part.text is not None:
                   print(f"DEBUG: Conteúdo de texto recebido: '{part.text}'")
                   # Se houver texto e você esperar apenas imagem, pode ser um problema
               
               if hasattr(part, 'inline_data') and part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Para ter certeza, grave os bytes em um arquivo temporário para inspeção
                       # with open("debug_image_data.bin", "wb") as f:
                       #     f.write(part.inline_data.data)
                       # print("DEBUG: Dados brutos da imagem salvos em 'debug_image_data.bin'.")

                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image # Retorna a imagem e encerra a função
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}. Verifique o console para mais detalhes.")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data é None ou vazio, apesar de inline_data existir.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline relevantes.")
           
           if not found_image:
               st.warning("O Gemini não retornou uma imagem válida para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este é o erro que você está vendo
           print(f"DEBUG: Exceção principal capturada: {e}") # Log mais detalhado no terminal
           return None
   ```
   Execute a aplicação Streamlit com estas alterações (`streamlit run chat_streamlit.py`) no notebook problemático. Os `print`s extras aparecerão no terminal onde você iniciou o Streamlit e nos darão informações cruciais sobre o que o Gemini está realmente enviando.

**3. Compare as Versões das Dependências (`pip freeze`)**

   Mesmo em Windows, a versão das bibliotecas pode ser um problema.
   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que NÃO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos `requirements_working.txt` e `requirements_broken.txt` meticulosamente.** Preste atenção especial às versões de:
        *   `Pillow` (P-I-L-L-O-W)
        *   `google-generativeai`
        *   `streamlit`
   *   **Ação:** Se houver diferenças, tente instalar as versões do notebook que funciona no notebook problemático. Por exemplo, se `Pillow==9.5.0` funciona e `Pillow==10.0.0` falha, tente:
        ```bash
        pip install Pillow==[VERSAO_DO_FUNCIONANDO] google-generativeai==[VERSAO_DO_FUNCIONANDO]
        ```
        Sempre ative seu ambiente virtual antes de instalar!

**4. Verifique a Conectividade de Rede e Firewall**

   *   **Teste de Ping:** Abra o Prompt de Comando (CMD) ou PowerShell no notebook problemático e tente:
        ```bash
        ping generativelanguage.googleapis.com
        ```
        Você deve ver pacotes sendo enviados e recebidos. Se houver perda de pacotes ou o destino não for alcançado, pode ser um problema de rede.
   *   **Firewall/Antivírus:** Temporariamente (com cautela, se for seguro) desative o firewall do Windows ou o antivírus para ver se eles estão bloqueando a comunicação com a API do Gemini. Se funcionar, você precisará adicionar uma exceção.
   *   **Proxy:** Se você estiver em uma rede corporativa, verifique se há um proxy que precisa ser configurado para chamadas HTTP/HTTPS.

**5. Reinstale as Bibliotecas Chave (Forçado)**

   Se a comparação do `pip freeze` não revelar grandes diferenças ou se você quiser ter certeza de uma instalação limpa:
   *   Ative seu ambiente virtual.
   *   Execute:
        ```bash
        pip install --upgrade --force-reinstall Pillow google-generativeai
        ```
   *   Reinicie a aplicação Streamlit.

**6. Consistência do Ambiente Virtual**

   Certifique-se de que em ambos os notebooks você está usando ambientes virtuais e que os ativou corretamente antes de instalar as dependências e antes de executar o `streamlit run`. Usar o ambiente global do Python pode levar a conflitos de dependências.

---

A mensagem de erro é bem clara sobre onde o problema está (`Image.open(io.BytesIO(part.inline_data.data))`). A chave é descobrir *por que* os `bytes` em `part.inline_data.data` não estão representando uma imagem válida no notebook problemático. Os passos de debug com `print`s na função `generate_image` e o teste com `test_image_generation.py` são os mais promissores para desvendar isso.
2025-06-28 14:07:31,485 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:07:31,486 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:07:31,488 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:07:31,489 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:07:31,490 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:07:31,491 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:07:31,493 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:07:31,494 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:07:31,496 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:07:31,499 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:07:31,500 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:07:31,502 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:07:31,503 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:07:31,504 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:07:31,509 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:09:14,013 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:09:14,018 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:09:14,020 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:09:14,021 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:09:14,028 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:09:14,029 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:09:14,030 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:09:14,032 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:09:14,033 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:09:14,040 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:09:14,048 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:09:14,049 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:09:14,055 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:09:14,061 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:09:14,062 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:09:14,413 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:09:14,414 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:09:14,415 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:09:14,416 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:09:14,417 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:09:14,418 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:09:14,420 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:09:14,422 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:09:14,423 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:09:14,425 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:09:14,429 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:09:14,430 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:09:14,432 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:09:14,433 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:09:14,434 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:09:15,208 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:09:19,714 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 14:09:20,818 [INFO] AFC remote call 1 is done.
2025-06-28 14:09:57,569 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:09:57,571 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:09:57,572 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:09:57,573 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:09:57,574 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:09:57,575 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:09:57,578 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:09:57,579 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:09:57,580 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:09:57,581 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:09:57,587 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:09:57,589 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:09:57,590 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:09:57,591 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:09:57,593 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:09:57,850 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:09:57,851 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:09:57,852 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:09:57,853 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:09:57,854 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:09:57,856 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:09:57,857 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:09:57,859 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:09:57,860 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:09:57,861 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:09:57,862 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:09:57,863 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:09:57,865 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:09:57,866 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:09:57,867 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:09:57,870 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque estou recebendo esse erro nessa aplicação sendo que no outro notebook a mesma aplicação roda perfeitamente ?
Assistant: É frustrante quando o mesmo código se comporta de forma diferente em máquinas distintas! Isso quase sempre aponta para **diferenças no ambiente de execução**, e não no código em si.

Analisando seu código, o principal suspeito para um erro que ocorre em um notebook e não em outro, especialmente se um for Windows e o outro Linux/macOS, é o uso de `PIL.ImageGrab`.

---

**O SUSPEITO PRINCIPAL: `PIL.ImageGrab` (Área de Transferência)**

No seu arquivo `chat_streamlit.py`, você tem a função `check_clipboard()` que usa `PIL.ImageGrab`:

```python
# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab # <--- AQUI ESTÁ O PROVÁVEL PROBLEMA

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        # ... (restante da função)
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}") # <--- ESSA É A MENSAGEM DE ERRO QUE VOCÊ DEVE ESTAR VENDO
        return None, None
```

A biblioteca `PIL.ImageGrab` (parte do Pillow) é altamente dependente do sistema operacional:

1.  **Windows:** Geralmente funciona sem problemas, pois o Windows possui APIs diretas para acesso à área de transferência de imagens.
2.  **Linux:** Requer utilitários de linha de comando como `scrot` ou `gnome-screenshot` instalados e configurados corretamente, e a biblioteca `Pillow` precisa ser compilada com suporte a essas ferramentas. Muitas distribuições Linux não vêm com isso pronto para o `ImageGrab`.
3.  **macOS:** Também tem suas próprias dependências, geralmente exigindo bibliotecas como `pyobjc` para interagir com o ambiente Cocoa.

**Se o seu "outro notebook" roda Windows e o notebook com problema roda Linux ou macOS, este é 99% a causa.** A mensagem de erro `"Erro ao acessar a área de transferência: {e}"` é exatamente o que você veria na barra lateral do Streamlit se essa parte falhasse.

---

**Passos para Diagnosticar e Resolver:**

1.  **Obtenha a Mensagem de Erro Exata:**
    *   No notebook onde a aplicação não roda, verifique a barra lateral do Streamlit. Você verá a mensagem exata do erro que está ocorrendo, começando com `"Erro ao acessar a área de transferência: ..."`. O que vem depois do `:` é crucial.
    *   Se não for `PIL.ImageGrab`, o erro estará em outro lugar.

2.  **Compare os Ambientes:**
    *   **Sistema Operacional:** Qual SO está rodando em cada notebook? (Ex: Windows 10 vs. Ubuntu 22.04 vs. macOS Ventura)
    *   **Versão do Python:** `python --version` em ambos.
    *   **Dependências Instaladas:**
        *   No notebook que funciona, execute: `pip freeze > requirements_working.txt`
        *   No notebook que não funciona, execute: `pip freeze > requirements_broken.txt`
        *   Compare os dois arquivos (`diff requirements_working.txt requirements_broken.txt`). Preste atenção especial à versão do `Pillow` e a presença de pacotes como `pywin32` (Windows) ou qualquer coisa relacionada a screenshots em Linux/macOS.

3.  **Soluções para o `PIL.ImageGrab`:**

    *   **Para Linux (Ubuntu/Debian):**
        *   Instale `scrot` (ou `gnome-screenshot`): `sudo apt-get update && sudo apt-get install scrot` (ou `sudo apt-get install gnome-screenshot`).
        *   Certifique-se de que o `Pillow` foi instalado *após* a instalação dessas ferramentas ou reinstale-o para que ele possa detectar as dependências.
        *   Pode ser necessário também `sudo apt-get install python3-tk python3-pil.imagetk` em algumas distribuições.

    *   **Para macOS:**
        *   Pode ser necessário instalar `pyobjc`: `pip install pyobjc`.

    *   **Solução Alternativa (Desativar ou Condicional):**
        Se você não precisa desesperadamente da funcionalidade de colar imagem da área de transferência e quer que a aplicação funcione em qualquer SO sem dependências externas complexas:
        Você já tem um `try-except` que lida com o erro. A aplicação deve continuar funcionando mesmo se a funcionalidade de colar não funcionar. No entanto, se o erro estiver parando a execução principal, isso é um problema.

        **Para evitar o erro na barra lateral em ambientes não-Windows:**
        Você pode adicionar uma verificação de sistema operacional:
        ```python
        import platform
        # ...

        def check_clipboard():
            # Só tenta importar ImageGrab se for Windows
            if platform.system() == "Windows":
                try:
                    from PIL import ImageGrab
                    img = ImageGrab.grabclipboard()
                    # ... (restante do código se img não for None)
                    if img is not None and isinstance(img, Image.Image):
                        # ... seu código para processar a imagem da área de transferência
                        # ...
                        return ClipboardFile(img_byte_arr), img
                    return None, None
                except Exception as e:
                    # A mensagem já é exibida no Streamlit, então não precisa de outro print aqui
                    st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
                    return None, None
            else:
                # Se não for Windows, não tenta acessar a área de transferência
                # E não mostra erro na barra lateral
                # st.sidebar.info("Colar da área de transferência disponível apenas no Windows.")
                return None, None
        ```
        Isso impediria que o erro específico de `ImageGrab` fosse tentado em sistemas não-Windows.

4.  **Verifique a Chave da API Gemini:**
    *   Certifique-se de que o arquivo `.env` existe no diretório raiz da aplicação no notebook problemático.
    *   Certifique-se de que `API_KEY_GEMINI` está definida corretamente dentro dele (Ex: `API_KEY_GEMINI="SUA_CHAVE_AQUI"`).
    *   Um erro relacionado à API Key geralmente resultaria em `ValueError("API Key não encontrada nas variáveis de ambiente")` ou erros da própria API (4xx, 5xx).

5.  **Verifique as Versões das Dependências:**
    *   É altamente recomendável ter um `requirements.txt` e instalá-lo em ambos os ambientes com `pip install -r requirements.txt`.
    *   Pode haver incompatibilidades sutis entre versões de `streamlit`, `google-generativeai`, `Pillow`, etc.

6.  **Permissões de Diretório:**
    *   Verifique se a aplicação tem permissão para criar e escrever nos diretórios `assets/`, `logs/` e `processed_images/` no notebook problemático. Isso é mais comum em Linux/macOS com permissões restritas.

7.  **Conectividade de Rede/Firewall:**
    *   O notebook problemático consegue acessar a internet? Existem restrições de firewall que podem estar bloqueando o acesso aos endpoints da API do Gemini?

---

**Resumo da Ação Mais Provável:**

O erro `Erro ao acessar a área de transferência: {e}` na barra lateral do Streamlit é quase uma certeza de que o problema está na tentativa de usar `PIL.ImageGrab` em um sistema operacional (provavelmente Linux/macOS) que não tem as dependências necessárias para isso.

Por favor, forneça a mensagem de erro exata da barra lateral e o sistema operacional de ambos os notebooks para uma análise mais precisa!
User: ois dois ambientes são windows. 
Assistant: Certo, isso muda o cenário! Se ambos os ambientes são Windows, então o problema não é a incompatibilidade do `PIL.ImageGrab` com Linux/macOS. No entanto, o `PIL.ImageGrab` continua sendo o principal suspeito, mas por razões diferentes.

Quando `PIL.ImageGrab` falha no Windows, geralmente aponta para um problema com a biblioteca `pywin32`, que o Pillow usa para interagir com as APIs do sistema operacional Windows.

Vamos desvendar isso.

---

**Causas Mais Comuns do `PIL.ImageGrab` Falhar no Windows:**

1.  **`pywin32` Ausente ou Corrompido:** A biblioteca `pywin32` é essencial para que o Pillow (que inclui `PIL.ImageGrab`) acesse a área de transferência do Windows de forma nativa. Se ela não estiver instalada ou estiver corrompida, a função falhará.
2.  **Ambientes Python Diferentes:** Mesmo que ambos sejam Windows, você pode ter ambientes Python (globais ou virtuais) ligeiramente diferentes em cada máquina, onde uma tem `pywin32` e a outra não, ou tem uma versão incompatível.
3.  **Conflitos de Versão:** Raramente, pode haver um conflito entre a versão do `Pillow` e a versão do `pywin32`, ou até mesmo com a versão do Python.
4.  **Conteúdo da Área de Transferência:** Embora menos provável dado seu relato, o `ImageGrab.grabclipboard()` retornará `None` se o conteúdo da área de transferência não for uma imagem (ex: texto, arquivos). No entanto, o erro que você vê sugere que o *acesso* à área de transferência está falhando, não apenas que ela está vazia.

---

**Passos Detalhados de Diagnóstico e Solução:**

**1. Obtenha a Mensagem de Erro Exata:**
   Este é o passo mais crucial. Quando você executa a aplicação no notebook que está dando problema, qual é a *mensagem de erro completa* que aparece na barra lateral do Streamlit, após "Erro ao acessar a área de transferência: "?
   Ex: `"Erro ao acessar a área de transferência: [WinError 5] Acesso negado."` ou `"Erro ao acessar a área de transferência: No module named 'win32clipboard'"` etc.
   Esta mensagem detalhada nos dirá exatamente o que está acontecendo.

**2. Compare as Dependências `pip`:**

   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que NÃO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos:** Abra `requirements_working.txt` e `requirements_broken.txt` e procure por:
        *   `Pillow` (P-I-L-L-O-W): Verifique as versões.
        *   `pywin32`: **Este é o mais importante.** Ele deve estar presente no `requirements_working.txt`. Se não estiver no `requirements_broken.txt` ou se a versão for diferente, encontramos o provável culpado.
        *   `streamlit`: Verifique as versões.

**3. Instale/Reinstale `pywin32` (no notebook problemático):**

   Se `pywin32` estiver faltando ou com uma versão diferente, tente o seguinte:

   *   **Verifique o ambiente virtual:** Se você estiver usando um ambiente virtual (altamente recomendado), ative-o primeiro.
        ```bash
        # Exemplo para venv:
        cd C:\caminho\para\seu\projeto
        .\.venv\Scripts\activate  # ou .\venv\Scripts\activate
        ```
   *   **Instale `pywin32`:**
        ```bash
        pip install pywin32
        ```
   *   **Ou, se já estiver instalado, force a reinstalação:**
        ```bash
        pip install --upgrade --force-reinstall pywin32
        ```
   *   **Reinicie a aplicação Streamlit.**

**4. Teste a Funcionalidade `ImageGrab` Isoladamente:**

   Para confirmar que o problema é realmente com `ImageGrab` e não algo mais amplo com o Streamlit ou seu código:

   *   Crie um pequeno script Python (`test_clipboard.py`) no notebook problemático:
        ```python
        import sys
        from PIL import ImageGrab, Image
        import io
        import time

        try:
            # Tenta pegar imagem da área de transferência
            img = ImageGrab.grabclipboard()

            if img is not None and isinstance(img, Image.Image):
                print("Imagem encontrada na área de transferência!")
                # Opcional: Salvar a imagem para verificar
                # img.save("clipboard_test.png")
                # print("Imagem salva como clipboard_test.png")
                
                # Tenta converter para bytes (simular o que você faz no Streamlit)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format='PNG')
                img_byte_arr.seek(0)
                print(f"Tamanho dos bytes: {len(img_byte_arr.getvalue())} bytes")
                
            elif img is None:
                print("Nenhuma imagem na área de transferência ou o conteúdo não é uma imagem.")
            else:
                print(f"Tipo de conteúdo inesperado na área de transferência: {type(img)}")

        except ImportError as e:
            print(f"Erro: Dependência faltando para PIL.ImageGrab: {e}")
            print("Tente 'pip install pywin32' e certifique-se de que está no ambiente correto.")
        except Exception as e:
            print(f"Erro inesperado ao acessar a área de transferência: {e}")
            print("Verifique permissões ou possíveis conflitos.")

        print("\nTeste concluído.")
        ```
   *   **Copie uma imagem** (não texto) para a área de transferência no Windows.
   *   Execute este script no terminal (ativando seu ambiente virtual, se aplicável): `python test_clipboard.py`
   *   Observe a saída. Isso confirmará se o `ImageGrab` está funcionando fora do Streamlit e qual erro exato ele gera.

**5. Verifique a Versão do Python e Arquitetura:**

   Embora menos comum hoje em dia, um Python de 32 bits pode ter problemas com algumas bibliotecas de 64 bits.

   *   Execute `python -c "import platform; print(platform.architecture())"` em ambos os notebooks. Verifique se ambos são `('64bit', 'WindowsPE')`.

**6. Outras Verificações (Menos Prováveis):**

*   **Antivírus/Firewall:** Muito raramente, um software de segurança pode interferir no acesso a certas APIs do sistema. Tente desativá-lo temporariamente (se for seguro fazer isso) para testar.
*   **Permissões de Usuário:** Certifique-se de que o usuário que executa a aplicação Streamlit tem as permissões necessárias para interagir com o sistema.

---

**Em Resumo:**

A principal ação é obter a **mensagem de erro exata** da barra lateral do Streamlit e, em seguida, **comparar as dependências `pip freeze`** para identificar se `pywin32` está faltando ou tem uma versão diferente. O teste isolado com `test_clipboard.py` também é muito útil para isolar o problema.

Assim que você tiver a mensagem de erro exata, poderemos refinar ainda mais o diagnóstico!
User: Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E13620360>
Assistant: Ok, essa nova informação muda completamente o diagnóstico! O erro `Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x...>` **não tem a ver com a área de transferência (`PIL.ImageGrab`)**.

Este erro significa que a biblioteca `Pillow` (PIL) tentou abrir algo que não reconheceu como uma imagem válida. A mensagem `_io.BytesIO object` indica que ela recebeu um fluxo de bytes, mas esses bytes não formam uma imagem (JPEG, PNG, etc.) que ela possa decodificar.

Vamos focar na função `generate_image` no `chat_streamlit.py`:

```python
# chat_streamlit.py

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # ... (verificação da API_KEY) ...

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text) # Se for só texto, imprime e continua o loop ou retorna None
            elif part.inline_data is not None:
                # <-- AQUI É O PONTO DO ERRO
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image # Retorna a imagem e sai da função

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # <-- É aqui que o erro é capturado e exibido
        return None
```

A linha `image = Image.open(io.BytesIO(part.inline_data.data))` é a que está falhando. Isso significa que, por algum motivo, o `part.inline_data.data` que o Gemini está retornando **não é um conjunto de bytes de uma imagem válida** no notebook problemático, ou está vazio.

---

**Possíveis Causas e Como Depurar:**

1.  **O Gemini não está gerando uma imagem para aquele prompt/requisição específica:**
    *   Às vezes, a API de geração de imagem do Gemini pode não conseguir gerar uma imagem para um prompt, ou pode bloquear a geração por motivos de segurança/conteúdo. Nesses casos, `part.inline_data` pode ser `None` ou `part.inline_data.data` pode estar vazio ou conter dados de erro/texto em vez de dados de imagem.
    *   Se o `response.prompt_feedback` tiver um `block_reason` (motivo de bloqueio), isso pode indicar o problema.

2.  **Problema de conectividade/API key:**
    *   Embora você já tenha verificado, é bom garantir que a `API_KEY_GEMINI` esteja configurada *exatamente* da mesma forma em ambos os notebooks. Um problema na chave pode levar a respostas malformadas da API, mesmo que não seja um erro óbvio de autenticação.
    *   Há alguma restrição de rede/firewall no notebook problemático que possa estar afetando a comunicação com a API do Gemini, fazendo com que as respostas venham incompletas ou corrompidas?

3.  **Versão da biblioteca `google-generativeai` ou `Pillow`:**
    *   Mesmo sendo Windows, pode haver pequenas diferenças nas versões dessas bibliotecas que causem um comportamento diferente.

---

**Passos para Diagnosticar e Resolver:**

**1. Teste o `test_image_generation.py` no notebook problemático:**

   Você tem um script de teste simples (`test_image_generation.py`) que faz uma chamada direta para a API de geração de imagem.
   *   Copie e execute o `test_image_generation.py` no notebook que está apresentando o erro.
   *   Ele deve tentar gerar "ed-image.png".
   *   **Resultado esperado:**
        *   **Se `test_image_generation.py` também falhar com o mesmo erro (`cannot identify image file`),** então o problema é mais fundamental na comunicação com o Gemini ou na forma como os dados são recebidos/processados pelo Pillow *naquele ambiente*.
        *   **Se `test_image_generation.py` funcionar e gerar a imagem corretamente,** então o problema está em algo mais específico no fluxo do `chat_streamlit.py` ao chamar `generate_image`.

**2. Adicione Logs Detalhados na Função `generate_image` (em `chat_streamlit.py`):**

   Vamos adicionar mais `print` statements (ou usar `logger.info` se preferir ver no logfile) para entender o que está vindo da API antes de chamar `Image.open()`.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Certifique-se que io está importado

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI não encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug print

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG ---
           print(f"\nDEBUG: Objeto de resposta Gemini completo: {response}")
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Razão de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Geração de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # Não há imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA não retornou candidatos de geração. Tente um prompt diferente.")
               return None

           found_image = False
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo: {type(part)}")
               if part.text is not None:
                   print(f"DEBUG: Conteúdo de texto recebido: '{part.text}'")
                   # Se houver texto, você pode decidir se quer usá-lo ou continuar procurando por imagem
               elif part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Tenta abrir a imagem
                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data é None ou vazio, apesar de inline_data não ser None.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline.")
           
           if not found_image:
               st.warning("O Gemini não retornou uma imagem para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este é o erro que você está vendo
           return None
   ```
   Execute a aplicação com estes prints e observe a saída no terminal onde você executa `streamlit run chat_streamlit.py`. Isso nos dará pistas vitais sobre o que o Gemini está realmente retornando.

**3. Compare as Dependências Novamente:**

   Embora menos provável, vale a pena uma revisão minuciosa.
   *   No notebook que FUNCIONA, execute: `pip freeze > requirements_working.txt`
   *   No notebook que NÃO FUNCIONA, execute: `pip freeze > requirements_broken.txt`
   *   Compare os arquivos, prestando atenção especial a:
        *   `Pillow` (ou PIL)
        *   `google-generativeai`
        *   `streamlit`
   *   Se houver diferença, tente instalar as versões do `requirements_working.txt` no notebook problemático.

**4. Reinstale `Pillow`:**

   Mesmo que as versões sejam as mesmas, uma instalação corrompida pode ser o problema.
   *   Ative seu ambiente virtual (se estiver usando).
   *   Execute: `pip install --upgrade --force-reinstall Pillow`
   *   Reinicie a aplicação Streamlit.

---

**Conclusão Preliminar:**

O erro `cannot identify image file` na função `generate_image` sugere fortemente que os dados que o Gemini retorna para `part.inline_data.data` **não são dados de imagem válidos** no ambiente problemático. Os passos de depuração acima, especialmente a execução do `test_image_generation.py` e os `print`s detalhados na função `generate_image` no `chat_streamlit.py`, nos darão as informações necessárias para identificar a causa raiz.
User: mas essa mesma aplicação funciona no meu outro notebook sem erros.
Assistant: Entendido! Se o problema persiste no segundo notebook, mesmo ambos sendo Windows, e o erro é `cannot identify image file <_io.BytesIO object at 0x...>`, isso significa que a questão não é a compatibilidade do sistema operacional com `PIL.ImageGrab`, mas sim algo específico na forma como os **dados da imagem gerada pelo Gemini** estão chegando ou sendo processados no notebook problemático.

Este erro (`cannot identify image file`) ocorre quando a biblioteca `Pillow` (que é usada para abrir a imagem com `Image.open`) recebe um fluxo de bytes (`_io.BytesIO`) que ela não consegue reconhecer como um formato de imagem válido (PNG, JPG, etc.).

Vamos focar na função `generate_image` no `chat_streamlit.py` e nos dados que ela recebe da API do Gemini.

### Causas Prováveis do Erro `cannot identify image file` (mesmo em Windows):

1.  **Dados de Imagem Inválidos/Incompletos da API do Gemini:**
    *   **API Key com Problema:** Embora você tenha verificado, às vezes uma API Key pode ter restrições, ou a própria chamada de API pode estar retornando uma resposta malformada ou um erro sutil que não é um `HTTP 200 OK` com dados de imagem.
    *   **Bloqueio de Conteúdo:** O prompt pode estar sendo bloqueado pelos filtros de segurança do Gemini, e em vez de uma imagem, a API retorna uma resposta vazia, uma mensagem de erro em texto, ou um `prompt_feedback` indicando um bloqueio, mas o código tenta processar `inline_data.data` como se fosse uma imagem.
    *   **Problema de Conectividade de Rede:** No notebook problemático, pode haver alguma instabilidade de rede, firewall ou proxy que está corrompendo ou truncando os dados da imagem durante a transferência da API do Gemini para a sua aplicação. Os bytes chegam incompletos ou inválidos, e o `Pillow` não consegue decodificá-los.

2.  **Problema na Instalação ou Versão do `Pillow` (ou `google-generativeai`):**
    *   **Instalação Corrompida:** Apesar de ser Windows, a instalação do Pillow pode ter algum arquivo corrompido ou faltando no ambiente Python do notebook problemático.
    *   **Diferença de Versão:** Se as versões do `Pillow` ou `google-generativeai` forem ligeiramente diferentes entre os dois notebooks, pode haver uma incompatibilidade sutil. Ex: Uma versão antiga do Pillow pode não suportar um formato específico que o Gemini está retornando, ou uma versão diferente do `google-generativeai` pode lidar com a resposta da API de forma distinta.

### Passos de Diagnóstico e Solução Detalhados:

**1. Re-execute o `test_image_generation.py` no notebook problemático.**
   Este é o teste mais importante para isolar o problema. Ele usa a mesma lógica de chamada de API e de abertura de imagem.
   *   Execute `python test_image_generation.py` no terminal do notebook que está com problema.
   *   **Observe a saída:**
        *   Se ele **também falhar com o mesmo erro** (`cannot identify image file`), então o problema é fundamental na forma como os dados chegam da API ou como o `Pillow` os interpreta *naquele ambiente*.
        *   Se ele **funcionar e gerar a imagem**, então o problema é mais específico no fluxo do `chat_streamlit.py` (embora isso seja menos provável com base no erro que você relatou).

**2. Adicione Logs Detalhados na Função `generate_image` (em `chat_streamlit.py`)**

   Isso nos permitirá ver exatamente o que o Gemini está retornando antes que o `Pillow` tente abrir a imagem.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Verifique se 'io' está importado no topo do arquivo

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI não encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug no Streamlit
           print(f"\nDEBUG: Chamando API do Gemini para prompt: '{prompt}'") # Debug no terminal

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG (principalmente no terminal) ---
           print(f"\nDEBUG: Resposta completa do Gemini recebida. Tipo: {type(response)}")
           
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Razão de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Geração de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # Não há imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA não retornou candidatos de geração. Tente um prompt diferente.")
               return None

           found_image = False
           # Assumimos que sempre pegamos o primeiro candidato (response.candidates[0])
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo do objeto part: {type(part)}")
               
               if hasattr(part, 'text') and part.text is not None:
                   print(f"DEBUG: Conteúdo de texto recebido: '{part.text}'")
                   # Se houver texto e você esperar apenas imagem, pode ser um problema
               
               if hasattr(part, 'inline_data') and part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Para ter certeza, grave os bytes em um arquivo temporário para inspeção
                       # with open("debug_image_data.bin", "wb") as f:
                       #     f.write(part.inline_data.data)
                       # print("DEBUG: Dados brutos da imagem salvos em 'debug_image_data.bin'.")

                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image # Retorna a imagem e encerra a função
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}. Verifique o console para mais detalhes.")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data é None ou vazio, apesar de inline_data existir.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline relevantes.")
           
           if not found_image:
               st.warning("O Gemini não retornou uma imagem válida para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este é o erro que você está vendo
           print(f"DEBUG: Exceção principal capturada: {e}") # Log mais detalhado no terminal
           return None
   ```
   Execute a aplicação Streamlit com estas alterações (`streamlit run chat_streamlit.py`) no notebook problemático. Os `print`s extras aparecerão no terminal onde você iniciou o Streamlit e nos darão informações cruciais sobre o que o Gemini está realmente enviando.

**3. Compare as Versões das Dependências (`pip freeze`)**

   Mesmo em Windows, a versão das bibliotecas pode ser um problema.
   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que NÃO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos `requirements_working.txt` e `requirements_broken.txt` meticulosamente.** Preste atenção especial às versões de:
        *   `Pillow` (P-I-L-L-O-W)
        *   `google-generativeai`
        *   `streamlit`
   *   **Ação:** Se houver diferenças, tente instalar as versões do notebook que funciona no notebook problemático. Por exemplo, se `Pillow==9.5.0` funciona e `Pillow==10.0.0` falha, tente:
        ```bash
        pip install Pillow==[VERSAO_DO_FUNCIONANDO] google-generativeai==[VERSAO_DO_FUNCIONANDO]
        ```
        Sempre ative seu ambiente virtual antes de instalar!

**4. Verifique a Conectividade de Rede e Firewall**

   *   **Teste de Ping:** Abra o Prompt de Comando (CMD) ou PowerShell no notebook problemático e tente:
        ```bash
        ping generativelanguage.googleapis.com
        ```
        Você deve ver pacotes sendo enviados e recebidos. Se houver perda de pacotes ou o destino não for alcançado, pode ser um problema de rede.
   *   **Firewall/Antivírus:** Temporariamente (com cautela, se for seguro) desative o firewall do Windows ou o antivírus para ver se eles estão bloqueando a comunicação com a API do Gemini. Se funcionar, você precisará adicionar uma exceção.
   *   **Proxy:** Se você estiver em uma rede corporativa, verifique se há um proxy que precisa ser configurado para chamadas HTTP/HTTPS.

**5. Reinstale as Bibliotecas Chave (Forçado)**

   Se a comparação do `pip freeze` não revelar grandes diferenças ou se você quiser ter certeza de uma instalação limpa:
   *   Ative seu ambiente virtual.
   *   Execute:
        ```bash
        pip install --upgrade --force-reinstall Pillow google-generativeai
        ```
   *   Reinicie a aplicação Streamlit.

**6. Consistência do Ambiente Virtual**

   Certifique-se de que em ambos os notebooks você está usando ambientes virtuais e que os ativou corretamente antes de instalar as dependências e antes de executar o `streamlit run`. Usar o ambiente global do Python pode levar a conflitos de dependências.

---

A mensagem de erro é bem clara sobre onde o problema está (`Image.open(io.BytesIO(part.inline_data.data))`). A chave é descobrir *por que* os `bytes` em `part.inline_data.data` não estão representando uma imagem válida no notebook problemático. Os passos de debug com `print`s na função `generate_image` e o teste com `test_image_generation.py` são os mais promissores para desvendar isso.
User: 2025-06-28 14:09:15,208 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:09:19,714 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent 
"HTTP/1.1 200 OK"
2025-06-28 14:09:20,818 [INFO] AFC remote call 1 is done.
I will generate an image of a tiny tabby kitten with wide eyes, standing on a miniature surfboard amidst the towering, foamy crest of a colossal ocean wave. The kitten will be mid-bite into a comically large slice of steaming lasagna, complete with visible layers of pasta, sauce, cheese, and ground meat, held precariously in its small paws.


parece que está gerando mas ainda ocorre o erro na lateral do streamlit 

Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E136676A0>


2025-06-28 14:10:36,822 [INFO] Resposta da IA (texto): Essa é uma informação crucial!

O fato de você ver:

```
I will generate an image of a tiny tabby kitten with wide eyes, standing on a miniature surfboard amidst the towering, foamy crest of a colossal ocean wave. The kitten will be mid-bite into a comically large slice of steaming lasagna, complete with visible layers of pasta, sauce, cheese, and ground meat, held precariously in its small paws.
```
e
```
2025-06-28 14:09:19,714 [INFO] HTTP Request: POST ... HTTP/1.1 200 OK
```

Indica que:

1.  A comunicação com a API do Gemini está **funcionando** (`HTTP/1.1 200 OK`).
2.  A API do Gemini está **respondendo** com conteúdo.
3.  O conteúdo inclui uma **parte de texto** (`part.text`) que descreve a imagem que será gerada.

No entanto, o erro `cannot identify image file <_io.BytesIO object at 0x...>` ainda significa que, quando a biblioteca `Pillow` (PIL) tenta abrir os bytes que deveriam ser a imagem, ela **não os reconhece como uma imagem válida**.

### O que isso significa?

A API de geração de imagem do Gemini (como `gemini-2.0-flash-exp-image-generation`) geralmente retorna uma resposta multi-parte:
*   Pode ter uma parte `text` (como o que você viu, descrevendo o que será gerado).
*   E *depois*, uma parte `inline_data` que contém os bytes da imagem gerada.

Se você está vendo o texto mas o erro na imagem persiste, isso pode indicar que:

1.  **A parte da imagem está faltando:** O Gemini retornou o texto de "confirmar" o prompt, mas por algum motivo (talvez o prompt seja muito complexo/inviável, ou algum filtro de segurança) **não conseguiu gerar ou enviar a imagem em si**. Então, `part.inline_data.data` estaria vazio ou `None`, mas o seu código *ainda tenta* processá-lo de alguma forma que leva ao erro.
2.  **Os dados da imagem estão corrompidos:** A API tentou enviar a imagem, mas os bytes chegaram corrompidos (talvez por um problema de rede muito sutil *após* o `200 OK`, durante a transferência do corpo da resposta), e o `Pillow` não consegue abri-los.
3.  **Lógica no loop `for part in response.candidates[0].content.parts:`:** Sua lógica atual está apenas `print`ando o `part.text` e, em seguida, esperando encontrar o `part.inline_data`. Se o `part.inline_data` está vazio ou corrompido, a função chega ao `Image.open` com dados ruins.

### Ações para Diagnosticar e Resolver:

Vamos refinar a função `generate_image` para ser mais robusta e para que os logs capturem *exatamente* o que está acontecendo com os dados da imagem.

**1. Adicione logs detalhados e um salvamento de bytes para inspeção (no `chat_streamlit.py`):**

Vamos ajustar a função `generate_image` para capturar e inspecionar os bytes da imagem.

```python
# chat_streamlit.py

# ... (seus imports e início da função)

def generate_image(prompt):
    api_key = os.getenv("API_KEY_GEMINI")
    if not api_key:
        st.error("API_KEY_GEMINI não encontrada no arquivo .env")
        return None

    client = genai.Client(api_key=api_key)

    try:
        st.info(f"Tentando gerar imagem com prompt: '{prompt}'")
        print(f"\n[DEBUG] Chamando API do Gemini para prompt: '{prompt}'") # Log para o terminal

        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        print(f"[DEBUG] Resposta completa do Gemini recebida. Tipo do objeto: {type(response)}")
        
        # Verificar feedback de bloqueio
        if response.prompt_feedback and response.prompt_feedback.block_reason:
            print(f"[DEBUG] Razão de Bloqueio da API: {response.prompt_feedback.block_reason}")
            st.warning(f"Geração de imagem bloqueada pelo Gemini: {response.prompt_feedback.block_reason}")
            return None
        
        if not response.candidates:
            print("[DEBUG] Nenhuma candidate encontrada na resposta do Gemini.")
            st.error("A IA não retornou candidatos de geração. Tente um prompt diferente.")
            return None

        # Variáveis para armazenar texto e dados da imagem
        text_feedback_from_gemini = ""
        image_data_bytes = None

        # Itera sobre todas as partes da resposta
        for part in response.candidates[0].content.parts:
            print(f"[DEBUG] Processando parte. Tipo do objeto 'part': {type(part)}")
            
            # Se a parte for texto
            if hasattr(part, 'text') and part.text is not None:
                text_feedback_from_gemini += part.text + "\n"
                print(f"[DEBUG] Conteúdo de texto recebido: '{part.text}'")
            
            # Se a parte for dados inline (imagem)
            if hasattr(part, 'inline_data') and part.inline_data is not None:
                print(f"[DEBUG] Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                if part.inline_data.data:
                    image_data_bytes = part.inline_data.data
                    print(f"[DEBUG] Tamanho dos dados da imagem: {len(image_data_bytes)} bytes.")
                    
                    # --- NOVO: Salve os bytes brutos para inspeção ---
                    # Isso ajudará a verificar se os bytes são realmente de uma imagem.
                    # Eles serão salvos no mesmo diretório do script.
                    with open("temp_gemini_image_debug.bin", "wb") as f:
                        f.write(image_data_bytes)
                    print("[DEBUG] Dados brutos da imagem salvos em 'temp_gemini_image_debug.bin' para análise.")
                    
                    # Interrompa o loop assim que encontrar os dados da imagem (assumindo que só há uma)
                    break
                else:
                    print("[DEBUG] part.inline_data.data é None ou vazio, apesar de inline_data existir.")
            else:
                print("[DEBUG] Parte sem texto ou dados inline relevantes.")
        
        # Tentar abrir a imagem se os bytes foram encontrados
        if image_data_bytes:
            try:
                image = Image.open(io.BytesIO(image_data_bytes))
                st.session_state.generated_image = image
                
                # Se o Gemini forneceu algum texto além da imagem, adicione-o ao chat history
                # junto com a imagem, se for relevante.
                # (Ajuste onde isso é adicionado ao chat history se o seu fluxo for diferente)
                if text_feedback_from_gemini.strip():
                     st.session_state.messages.append({"role": "assistant", "image": image, "content": text_feedback_from_gemini.strip()})
                else:
                    st.session_state.messages.append({"role": "assistant", "image": image, "content": f"Imagem gerada com o prompt: {prompt}"})
                
                st.session_state.generated_image = None # Limpa para não exibir em cima
                return image # Retorna a imagem e encerra a função
            except Exception as img_err:
                print(f"[DEBUG] ERRO da Pillow ao abrir imagem: {img_err}. Provavelmente dados de imagem corrompidos/inválidos.")
                st.error(f"Erro interno ao processar a imagem gerada: {img_err}. A imagem recebida pode estar corrompida.")
                return None
        else:
            # Se não encontrou dados de imagem
            msg = "O Gemini não retornou uma imagem válida para o prompt fornecido."
            if text_feedback_from_gemini.strip():
                msg += f" Feedback da IA: {text_feedback_from_gemini.strip()}"
            st.warning(msg)
            print(f"[DEBUG] {msg}")
            return None

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # Este é o erro que você está vendo
        print(f"[DEBUG] Exceção principal capturada em generate_image: {e}") # Log mais detalhado no terminal
        return None
```

**2. Execute a Aplicação e Analise os Logs e o Arquivo `temp_gemini_image_debug.bin`:**

*   Salve as alterações no seu `chat_streamlit.py`.
*   Execute `streamlit run chat_streamlit.py` no terminal do notebook problemático.
*   Tente gerar uma imagem com o prompt que causou o problema.
*   **Observe a saída no terminal:** Preste atenção aos `[DEBUG]` prints que você adicionou, especialmente o `Tamanho dos dados da imagem`.
    *   Se o tamanho for **zero ou muito pequeno** (ex: 0 a 100 bytes), isso indica que o Gemini não enviou os dados da imagem, ou enviou um "nada" corrompido.
    *   Se o tamanho for **razoável** (ex: dezenas ou centenas de KB), isso indica que os dados chegaram, mas o `Pillow` não os reconhece.
*   **Inspecione o arquivo `temp_gemini_image_debug.bin`:**
    *   Após o erro, localize este arquivo no diretório do seu script.
    *   Tente abri-lo com um visualizador de imagens (renomeie para `.png` ou `.jpg` se quiser tentar).
    *   Se ele não abrir ou estiver corrompido, você confirmou que o problema está nos bytes que o Gemini enviou para você.
    *   Se for um arquivo de imagem válido (o que seria surpreendente), o problema seria então na instalação do `Pillow` ou em alguma incompatibilidade no seu ambiente.

### O que esperar com base no seu `200 OK` e `text` part:

É muito provável que você veja o `[DEBUG] Tamanho dos dados da imagem: X bytes`, onde X é **zero** ou um número muito pequeno e **não há imagem válida no `temp_gemini_image_debug.bin`**.

Isso significaria que a API do Gemini, por algum motivo interno (complexidade do prompt, filtros de segurança etc.), não conseguiu materializar a imagem, apesar de ter confirmado o prompt com a parte textual. Se esse for o caso, não há muito que você possa fazer a nível de código para "consertar" uma imagem que não foi gerada corretamente pela API. A solução seria:

*   **Tentar prompts mais simples ou diferentes.**
*   **Verificar a documentação da API Gemini** para possíveis restrições ou limitações de conteúdo.
*   **Reportar o problema ao Google** se você acreditar que é um bug na API para o tipo de prompt que está usando.

Os logs detalhados e a inspeção do arquivo binário serão o "veredicto final" sobre a natureza do problema.
2025-06-28 14:10:37,036 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:10:37,037 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:10:37,038 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:10:37,039 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:10:37,039 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:10:37,040 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:10:37,041 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:10:37,042 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:10:37,043 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:10:37,044 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:10:37,045 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:10:37,047 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:10:37,051 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:10:37,052 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:10:37,053 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:12:52,157 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:12:52,158 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:12:52,160 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:12:52,161 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:12:52,165 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:12:52,167 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:12:52,169 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:12:52,180 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:12:52,181 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:12:52,182 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:12:52,183 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:12:52,184 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:12:52,186 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:12:52,191 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:12:52,192 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:12:52,439 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:12:52,440 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:12:52,441 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:12:52,443 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:12:52,443 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:12:52,444 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:12:52,445 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:12:52,447 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:12:52,448 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:12:52,450 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:12:52,451 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:12:52,455 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:12:52,457 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:12:52,459 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:12:52,460 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:12:52,464 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque estou recebendo esse erro nessa aplicação sendo que no outro notebook a mesma aplicação roda perfeitamente ?
Assistant: É frustrante quando o mesmo código se comporta de forma diferente em máquinas distintas! Isso quase sempre aponta para **diferenças no ambiente de execução**, e não no código em si.

Analisando seu código, o principal suspeito para um erro que ocorre em um notebook e não em outro, especialmente se um for Windows e o outro Linux/macOS, é o uso de `PIL.ImageGrab`.

---

**O SUSPEITO PRINCIPAL: `PIL.ImageGrab` (Área de Transferência)**

No seu arquivo `chat_streamlit.py`, você tem a função `check_clipboard()` que usa `PIL.ImageGrab`:

```python
# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab # <--- AQUI ESTÁ O PROVÁVEL PROBLEMA

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        # ... (restante da função)
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}") # <--- ESSA É A MENSAGEM DE ERRO QUE VOCÊ DEVE ESTAR VENDO
        return None, None
```

A biblioteca `PIL.ImageGrab` (parte do Pillow) é altamente dependente do sistema operacional:

1.  **Windows:** Geralmente funciona sem problemas, pois o Windows possui APIs diretas para acesso à área de transferência de imagens.
2.  **Linux:** Requer utilitários de linha de comando como `scrot` ou `gnome-screenshot` instalados e configurados corretamente, e a biblioteca `Pillow` precisa ser compilada com suporte a essas ferramentas. Muitas distribuições Linux não vêm com isso pronto para o `ImageGrab`.
3.  **macOS:** Também tem suas próprias dependências, geralmente exigindo bibliotecas como `pyobjc` para interagir com o ambiente Cocoa.

**Se o seu "outro notebook" roda Windows e o notebook com problema roda Linux ou macOS, este é 99% a causa.** A mensagem de erro `"Erro ao acessar a área de transferência: {e}"` é exatamente o que você veria na barra lateral do Streamlit se essa parte falhasse.

---

**Passos para Diagnosticar e Resolver:**

1.  **Obtenha a Mensagem de Erro Exata:**
    *   No notebook onde a aplicação não roda, verifique a barra lateral do Streamlit. Você verá a mensagem exata do erro que está ocorrendo, começando com `"Erro ao acessar a área de transferência: ..."`. O que vem depois do `:` é crucial.
    *   Se não for `PIL.ImageGrab`, o erro estará em outro lugar.

2.  **Compare os Ambientes:**
    *   **Sistema Operacional:** Qual SO está rodando em cada notebook? (Ex: Windows 10 vs. Ubuntu 22.04 vs. macOS Ventura)
    *   **Versão do Python:** `python --version` em ambos.
    *   **Dependências Instaladas:**
        *   No notebook que funciona, execute: `pip freeze > requirements_working.txt`
        *   No notebook que não funciona, execute: `pip freeze > requirements_broken.txt`
        *   Compare os dois arquivos (`diff requirements_working.txt requirements_broken.txt`). Preste atenção especial à versão do `Pillow` e a presença de pacotes como `pywin32` (Windows) ou qualquer coisa relacionada a screenshots em Linux/macOS.

3.  **Soluções para o `PIL.ImageGrab`:**

    *   **Para Linux (Ubuntu/Debian):**
        *   Instale `scrot` (ou `gnome-screenshot`): `sudo apt-get update && sudo apt-get install scrot` (ou `sudo apt-get install gnome-screenshot`).
        *   Certifique-se de que o `Pillow` foi instalado *após* a instalação dessas ferramentas ou reinstale-o para que ele possa detectar as dependências.
        *   Pode ser necessário também `sudo apt-get install python3-tk python3-pil.imagetk` em algumas distribuições.

    *   **Para macOS:**
        *   Pode ser necessário instalar `pyobjc`: `pip install pyobjc`.

    *   **Solução Alternativa (Desativar ou Condicional):**
        Se você não precisa desesperadamente da funcionalidade de colar imagem da área de transferência e quer que a aplicação funcione em qualquer SO sem dependências externas complexas:
        Você já tem um `try-except` que lida com o erro. A aplicação deve continuar funcionando mesmo se a funcionalidade de colar não funcionar. No entanto, se o erro estiver parando a execução principal, isso é um problema.

        **Para evitar o erro na barra lateral em ambientes não-Windows:**
        Você pode adicionar uma verificação de sistema operacional:
        ```python
        import platform
        # ...

        def check_clipboard():
            # Só tenta importar ImageGrab se for Windows
            if platform.system() == "Windows":
                try:
                    from PIL import ImageGrab
                    img = ImageGrab.grabclipboard()
                    # ... (restante do código se img não for None)
                    if img is not None and isinstance(img, Image.Image):
                        # ... seu código para processar a imagem da área de transferência
                        # ...
                        return ClipboardFile(img_byte_arr), img
                    return None, None
                except Exception as e:
                    # A mensagem já é exibida no Streamlit, então não precisa de outro print aqui
                    st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
                    return None, None
            else:
                # Se não for Windows, não tenta acessar a área de transferência
                # E não mostra erro na barra lateral
                # st.sidebar.info("Colar da área de transferência disponível apenas no Windows.")
                return None, None
        ```
        Isso impediria que o erro específico de `ImageGrab` fosse tentado em sistemas não-Windows.

4.  **Verifique a Chave da API Gemini:**
    *   Certifique-se de que o arquivo `.env` existe no diretório raiz da aplicação no notebook problemático.
    *   Certifique-se de que `API_KEY_GEMINI` está definida corretamente dentro dele (Ex: `API_KEY_GEMINI="SUA_CHAVE_AQUI"`).
    *   Um erro relacionado à API Key geralmente resultaria em `ValueError("API Key não encontrada nas variáveis de ambiente")` ou erros da própria API (4xx, 5xx).

5.  **Verifique as Versões das Dependências:**
    *   É altamente recomendável ter um `requirements.txt` e instalá-lo em ambos os ambientes com `pip install -r requirements.txt`.
    *   Pode haver incompatibilidades sutis entre versões de `streamlit`, `google-generativeai`, `Pillow`, etc.

6.  **Permissões de Diretório:**
    *   Verifique se a aplicação tem permissão para criar e escrever nos diretórios `assets/`, `logs/` e `processed_images/` no notebook problemático. Isso é mais comum em Linux/macOS com permissões restritas.

7.  **Conectividade de Rede/Firewall:**
    *   O notebook problemático consegue acessar a internet? Existem restrições de firewall que podem estar bloqueando o acesso aos endpoints da API do Gemini?

---

**Resumo da Ação Mais Provável:**

O erro `Erro ao acessar a área de transferência: {e}` na barra lateral do Streamlit é quase uma certeza de que o problema está na tentativa de usar `PIL.ImageGrab` em um sistema operacional (provavelmente Linux/macOS) que não tem as dependências necessárias para isso.

Por favor, forneça a mensagem de erro exata da barra lateral e o sistema operacional de ambos os notebooks para uma análise mais precisa!
User: ois dois ambientes são windows. 
Assistant: Certo, isso muda o cenário! Se ambos os ambientes são Windows, então o problema não é a incompatibilidade do `PIL.ImageGrab` com Linux/macOS. No entanto, o `PIL.ImageGrab` continua sendo o principal suspeito, mas por razões diferentes.

Quando `PIL.ImageGrab` falha no Windows, geralmente aponta para um problema com a biblioteca `pywin32`, que o Pillow usa para interagir com as APIs do sistema operacional Windows.

Vamos desvendar isso.

---

**Causas Mais Comuns do `PIL.ImageGrab` Falhar no Windows:**

1.  **`pywin32` Ausente ou Corrompido:** A biblioteca `pywin32` é essencial para que o Pillow (que inclui `PIL.ImageGrab`) acesse a área de transferência do Windows de forma nativa. Se ela não estiver instalada ou estiver corrompida, a função falhará.
2.  **Ambientes Python Diferentes:** Mesmo que ambos sejam Windows, você pode ter ambientes Python (globais ou virtuais) ligeiramente diferentes em cada máquina, onde uma tem `pywin32` e a outra não, ou tem uma versão incompatível.
3.  **Conflitos de Versão:** Raramente, pode haver um conflito entre a versão do `Pillow` e a versão do `pywin32`, ou até mesmo com a versão do Python.
4.  **Conteúdo da Área de Transferência:** Embora menos provável dado seu relato, o `ImageGrab.grabclipboard()` retornará `None` se o conteúdo da área de transferência não for uma imagem (ex: texto, arquivos). No entanto, o erro que você vê sugere que o *acesso* à área de transferência está falhando, não apenas que ela está vazia.

---

**Passos Detalhados de Diagnóstico e Solução:**

**1. Obtenha a Mensagem de Erro Exata:**
   Este é o passo mais crucial. Quando você executa a aplicação no notebook que está dando problema, qual é a *mensagem de erro completa* que aparece na barra lateral do Streamlit, após "Erro ao acessar a área de transferência: "?
   Ex: `"Erro ao acessar a área de transferência: [WinError 5] Acesso negado."` ou `"Erro ao acessar a área de transferência: No module named 'win32clipboard'"` etc.
   Esta mensagem detalhada nos dirá exatamente o que está acontecendo.

**2. Compare as Dependências `pip`:**

   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que NÃO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos:** Abra `requirements_working.txt` e `requirements_broken.txt` e procure por:
        *   `Pillow` (P-I-L-L-O-W): Verifique as versões.
        *   `pywin32`: **Este é o mais importante.** Ele deve estar presente no `requirements_working.txt`. Se não estiver no `requirements_broken.txt` ou se a versão for diferente, encontramos o provável culpado.
        *   `streamlit`: Verifique as versões.

**3. Instale/Reinstale `pywin32` (no notebook problemático):**

   Se `pywin32` estiver faltando ou com uma versão diferente, tente o seguinte:

   *   **Verifique o ambiente virtual:** Se você estiver usando um ambiente virtual (altamente recomendado), ative-o primeiro.
        ```bash
        # Exemplo para venv:
        cd C:\caminho\para\seu\projeto
        .\.venv\Scripts\activate  # ou .\venv\Scripts\activate
        ```
   *   **Instale `pywin32`:**
        ```bash
        pip install pywin32
        ```
   *   **Ou, se já estiver instalado, force a reinstalação:**
        ```bash
        pip install --upgrade --force-reinstall pywin32
        ```
   *   **Reinicie a aplicação Streamlit.**

**4. Teste a Funcionalidade `ImageGrab` Isoladamente:**

   Para confirmar que o problema é realmente com `ImageGrab` e não algo mais amplo com o Streamlit ou seu código:

   *   Crie um pequeno script Python (`test_clipboard.py`) no notebook problemático:
        ```python
        import sys
        from PIL import ImageGrab, Image
        import io
        import time

        try:
            # Tenta pegar imagem da área de transferência
            img = ImageGrab.grabclipboard()

            if img is not None and isinstance(img, Image.Image):
                print("Imagem encontrada na área de transferência!")
                # Opcional: Salvar a imagem para verificar
                # img.save("clipboard_test.png")
                # print("Imagem salva como clipboard_test.png")
                
                # Tenta converter para bytes (simular o que você faz no Streamlit)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format='PNG')
                img_byte_arr.seek(0)
                print(f"Tamanho dos bytes: {len(img_byte_arr.getvalue())} bytes")
                
            elif img is None:
                print("Nenhuma imagem na área de transferência ou o conteúdo não é uma imagem.")
            else:
                print(f"Tipo de conteúdo inesperado na área de transferência: {type(img)}")

        except ImportError as e:
            print(f"Erro: Dependência faltando para PIL.ImageGrab: {e}")
            print("Tente 'pip install pywin32' e certifique-se de que está no ambiente correto.")
        except Exception as e:
            print(f"Erro inesperado ao acessar a área de transferência: {e}")
            print("Verifique permissões ou possíveis conflitos.")

        print("\nTeste concluído.")
        ```
   *   **Copie uma imagem** (não texto) para a área de transferência no Windows.
   *   Execute este script no terminal (ativando seu ambiente virtual, se aplicável): `python test_clipboard.py`
   *   Observe a saída. Isso confirmará se o `ImageGrab` está funcionando fora do Streamlit e qual erro exato ele gera.

**5. Verifique a Versão do Python e Arquitetura:**

   Embora menos comum hoje em dia, um Python de 32 bits pode ter problemas com algumas bibliotecas de 64 bits.

   *   Execute `python -c "import platform; print(platform.architecture())"` em ambos os notebooks. Verifique se ambos são `('64bit', 'WindowsPE')`.

**6. Outras Verificações (Menos Prováveis):**

*   **Antivírus/Firewall:** Muito raramente, um software de segurança pode interferir no acesso a certas APIs do sistema. Tente desativá-lo temporariamente (se for seguro fazer isso) para testar.
*   **Permissões de Usuário:** Certifique-se de que o usuário que executa a aplicação Streamlit tem as permissões necessárias para interagir com o sistema.

---

**Em Resumo:**

A principal ação é obter a **mensagem de erro exata** da barra lateral do Streamlit e, em seguida, **comparar as dependências `pip freeze`** para identificar se `pywin32` está faltando ou tem uma versão diferente. O teste isolado com `test_clipboard.py` também é muito útil para isolar o problema.

Assim que você tiver a mensagem de erro exata, poderemos refinar ainda mais o diagnóstico!
User: Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E13620360>
Assistant: Ok, essa nova informação muda completamente o diagnóstico! O erro `Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x...>` **não tem a ver com a área de transferência (`PIL.ImageGrab`)**.

Este erro significa que a biblioteca `Pillow` (PIL) tentou abrir algo que não reconheceu como uma imagem válida. A mensagem `_io.BytesIO object` indica que ela recebeu um fluxo de bytes, mas esses bytes não formam uma imagem (JPEG, PNG, etc.) que ela possa decodificar.

Vamos focar na função `generate_image` no `chat_streamlit.py`:

```python
# chat_streamlit.py

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # ... (verificação da API_KEY) ...

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text) # Se for só texto, imprime e continua o loop ou retorna None
            elif part.inline_data is not None:
                # <-- AQUI É O PONTO DO ERRO
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image # Retorna a imagem e sai da função

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # <-- É aqui que o erro é capturado e exibido
        return None
```

A linha `image = Image.open(io.BytesIO(part.inline_data.data))` é a que está falhando. Isso significa que, por algum motivo, o `part.inline_data.data` que o Gemini está retornando **não é um conjunto de bytes de uma imagem válida** no notebook problemático, ou está vazio.

---

**Possíveis Causas e Como Depurar:**

1.  **O Gemini não está gerando uma imagem para aquele prompt/requisição específica:**
    *   Às vezes, a API de geração de imagem do Gemini pode não conseguir gerar uma imagem para um prompt, ou pode bloquear a geração por motivos de segurança/conteúdo. Nesses casos, `part.inline_data` pode ser `None` ou `part.inline_data.data` pode estar vazio ou conter dados de erro/texto em vez de dados de imagem.
    *   Se o `response.prompt_feedback` tiver um `block_reason` (motivo de bloqueio), isso pode indicar o problema.

2.  **Problema de conectividade/API key:**
    *   Embora você já tenha verificado, é bom garantir que a `API_KEY_GEMINI` esteja configurada *exatamente* da mesma forma em ambos os notebooks. Um problema na chave pode levar a respostas malformadas da API, mesmo que não seja um erro óbvio de autenticação.
    *   Há alguma restrição de rede/firewall no notebook problemático que possa estar afetando a comunicação com a API do Gemini, fazendo com que as respostas venham incompletas ou corrompidas?

3.  **Versão da biblioteca `google-generativeai` ou `Pillow`:**
    *   Mesmo sendo Windows, pode haver pequenas diferenças nas versões dessas bibliotecas que causem um comportamento diferente.

---

**Passos para Diagnosticar e Resolver:**

**1. Teste o `test_image_generation.py` no notebook problemático:**

   Você tem um script de teste simples (`test_image_generation.py`) que faz uma chamada direta para a API de geração de imagem.
   *   Copie e execute o `test_image_generation.py` no notebook que está apresentando o erro.
   *   Ele deve tentar gerar "ed-image.png".
   *   **Resultado esperado:**
        *   **Se `test_image_generation.py` também falhar com o mesmo erro (`cannot identify image file`),** então o problema é mais fundamental na comunicação com o Gemini ou na forma como os dados são recebidos/processados pelo Pillow *naquele ambiente*.
        *   **Se `test_image_generation.py` funcionar e gerar a imagem corretamente,** então o problema está em algo mais específico no fluxo do `chat_streamlit.py` ao chamar `generate_image`.

**2. Adicione Logs Detalhados na Função `generate_image` (em `chat_streamlit.py`):**

   Vamos adicionar mais `print` statements (ou usar `logger.info` se preferir ver no logfile) para entender o que está vindo da API antes de chamar `Image.open()`.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Certifique-se que io está importado

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI não encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug print

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG ---
           print(f"\nDEBUG: Objeto de resposta Gemini completo: {response}")
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Razão de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Geração de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # Não há imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA não retornou candidatos de geração. Tente um prompt diferente.")
               return None

           found_image = False
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo: {type(part)}")
               if part.text is not None:
                   print(f"DEBUG: Conteúdo de texto recebido: '{part.text}'")
                   # Se houver texto, você pode decidir se quer usá-lo ou continuar procurando por imagem
               elif part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Tenta abrir a imagem
                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data é None ou vazio, apesar de inline_data não ser None.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline.")
           
           if not found_image:
               st.warning("O Gemini não retornou uma imagem para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este é o erro que você está vendo
           return None
   ```
   Execute a aplicação com estes prints e observe a saída no terminal onde você executa `streamlit run chat_streamlit.py`. Isso nos dará pistas vitais sobre o que o Gemini está realmente retornando.

**3. Compare as Dependências Novamente:**

   Embora menos provável, vale a pena uma revisão minuciosa.
   *   No notebook que FUNCIONA, execute: `pip freeze > requirements_working.txt`
   *   No notebook que NÃO FUNCIONA, execute: `pip freeze > requirements_broken.txt`
   *   Compare os arquivos, prestando atenção especial a:
        *   `Pillow` (ou PIL)
        *   `google-generativeai`
        *   `streamlit`
   *   Se houver diferença, tente instalar as versões do `requirements_working.txt` no notebook problemático.

**4. Reinstale `Pillow`:**

   Mesmo que as versões sejam as mesmas, uma instalação corrompida pode ser o problema.
   *   Ative seu ambiente virtual (se estiver usando).
   *   Execute: `pip install --upgrade --force-reinstall Pillow`
   *   Reinicie a aplicação Streamlit.

---

**Conclusão Preliminar:**

O erro `cannot identify image file` na função `generate_image` sugere fortemente que os dados que o Gemini retorna para `part.inline_data.data` **não são dados de imagem válidos** no ambiente problemático. Os passos de depuração acima, especialmente a execução do `test_image_generation.py` e os `print`s detalhados na função `generate_image` no `chat_streamlit.py`, nos darão as informações necessárias para identificar a causa raiz.
User: mas essa mesma aplicação funciona no meu outro notebook sem erros.
Assistant: Entendido! Se o problema persiste no segundo notebook, mesmo ambos sendo Windows, e o erro é `cannot identify image file <_io.BytesIO object at 0x...>`, isso significa que a questão não é a compatibilidade do sistema operacional com `PIL.ImageGrab`, mas sim algo específico na forma como os **dados da imagem gerada pelo Gemini** estão chegando ou sendo processados no notebook problemático.

Este erro (`cannot identify image file`) ocorre quando a biblioteca `Pillow` (que é usada para abrir a imagem com `Image.open`) recebe um fluxo de bytes (`_io.BytesIO`) que ela não consegue reconhecer como um formato de imagem válido (PNG, JPG, etc.).

Vamos focar na função `generate_image` no `chat_streamlit.py` e nos dados que ela recebe da API do Gemini.

### Causas Prováveis do Erro `cannot identify image file` (mesmo em Windows):

1.  **Dados de Imagem Inválidos/Incompletos da API do Gemini:**
    *   **API Key com Problema:** Embora você tenha verificado, às vezes uma API Key pode ter restrições, ou a própria chamada de API pode estar retornando uma resposta malformada ou um erro sutil que não é um `HTTP 200 OK` com dados de imagem.
    *   **Bloqueio de Conteúdo:** O prompt pode estar sendo bloqueado pelos filtros de segurança do Gemini, e em vez de uma imagem, a API retorna uma resposta vazia, uma mensagem de erro em texto, ou um `prompt_feedback` indicando um bloqueio, mas o código tenta processar `inline_data.data` como se fosse uma imagem.
    *   **Problema de Conectividade de Rede:** No notebook problemático, pode haver alguma instabilidade de rede, firewall ou proxy que está corrompendo ou truncando os dados da imagem durante a transferência da API do Gemini para a sua aplicação. Os bytes chegam incompletos ou inválidos, e o `Pillow` não consegue decodificá-los.

2.  **Problema na Instalação ou Versão do `Pillow` (ou `google-generativeai`):**
    *   **Instalação Corrompida:** Apesar de ser Windows, a instalação do Pillow pode ter algum arquivo corrompido ou faltando no ambiente Python do notebook problemático.
    *   **Diferença de Versão:** Se as versões do `Pillow` ou `google-generativeai` forem ligeiramente diferentes entre os dois notebooks, pode haver uma incompatibilidade sutil. Ex: Uma versão antiga do Pillow pode não suportar um formato específico que o Gemini está retornando, ou uma versão diferente do `google-generativeai` pode lidar com a resposta da API de forma distinta.

### Passos de Diagnóstico e Solução Detalhados:

**1. Re-execute o `test_image_generation.py` no notebook problemático.**
   Este é o teste mais importante para isolar o problema. Ele usa a mesma lógica de chamada de API e de abertura de imagem.
   *   Execute `python test_image_generation.py` no terminal do notebook que está com problema.
   *   **Observe a saída:**
        *   Se ele **também falhar com o mesmo erro** (`cannot identify image file`), então o problema é fundamental na forma como os dados chegam da API ou como o `Pillow` os interpreta *naquele ambiente*.
        *   Se ele **funcionar e gerar a imagem**, então o problema é mais específico no fluxo do `chat_streamlit.py` (embora isso seja menos provável com base no erro que você relatou).

**2. Adicione Logs Detalhados na Função `generate_image` (em `chat_streamlit.py`)**

   Isso nos permitirá ver exatamente o que o Gemini está retornando antes que o `Pillow` tente abrir a imagem.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Verifique se 'io' está importado no topo do arquivo

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI não encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug no Streamlit
           print(f"\nDEBUG: Chamando API do Gemini para prompt: '{prompt}'") # Debug no terminal

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG (principalmente no terminal) ---
           print(f"\nDEBUG: Resposta completa do Gemini recebida. Tipo: {type(response)}")
           
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Razão de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Geração de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # Não há imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA não retornou candidatos de geração. Tente um prompt diferente.")
               return None

           found_image = False
           # Assumimos que sempre pegamos o primeiro candidato (response.candidates[0])
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo do objeto part: {type(part)}")
               
               if hasattr(part, 'text') and part.text is not None:
                   print(f"DEBUG: Conteúdo de texto recebido: '{part.text}'")
                   # Se houver texto e você esperar apenas imagem, pode ser um problema
               
               if hasattr(part, 'inline_data') and part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Para ter certeza, grave os bytes em um arquivo temporário para inspeção
                       # with open("debug_image_data.bin", "wb") as f:
                       #     f.write(part.inline_data.data)
                       # print("DEBUG: Dados brutos da imagem salvos em 'debug_image_data.bin'.")

                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image # Retorna a imagem e encerra a função
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}. Verifique o console para mais detalhes.")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data é None ou vazio, apesar de inline_data existir.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline relevantes.")
           
           if not found_image:
               st.warning("O Gemini não retornou uma imagem válida para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este é o erro que você está vendo
           print(f"DEBUG: Exceção principal capturada: {e}") # Log mais detalhado no terminal
           return None
   ```
   Execute a aplicação Streamlit com estas alterações (`streamlit run chat_streamlit.py`) no notebook problemático. Os `print`s extras aparecerão no terminal onde você iniciou o Streamlit e nos darão informações cruciais sobre o que o Gemini está realmente enviando.

**3. Compare as Versões das Dependências (`pip freeze`)**

   Mesmo em Windows, a versão das bibliotecas pode ser um problema.
   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que NÃO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos `requirements_working.txt` e `requirements_broken.txt` meticulosamente.** Preste atenção especial às versões de:
        *   `Pillow` (P-I-L-L-O-W)
        *   `google-generativeai`
        *   `streamlit`
   *   **Ação:** Se houver diferenças, tente instalar as versões do notebook que funciona no notebook problemático. Por exemplo, se `Pillow==9.5.0` funciona e `Pillow==10.0.0` falha, tente:
        ```bash
        pip install Pillow==[VERSAO_DO_FUNCIONANDO] google-generativeai==[VERSAO_DO_FUNCIONANDO]
        ```
        Sempre ative seu ambiente virtual antes de instalar!

**4. Verifique a Conectividade de Rede e Firewall**

   *   **Teste de Ping:** Abra o Prompt de Comando (CMD) ou PowerShell no notebook problemático e tente:
        ```bash
        ping generativelanguage.googleapis.com
        ```
        Você deve ver pacotes sendo enviados e recebidos. Se houver perda de pacotes ou o destino não for alcançado, pode ser um problema de rede.
   *   **Firewall/Antivírus:** Temporariamente (com cautela, se for seguro) desative o firewall do Windows ou o antivírus para ver se eles estão bloqueando a comunicação com a API do Gemini. Se funcionar, você precisará adicionar uma exceção.
   *   **Proxy:** Se você estiver em uma rede corporativa, verifique se há um proxy que precisa ser configurado para chamadas HTTP/HTTPS.

**5. Reinstale as Bibliotecas Chave (Forçado)**

   Se a comparação do `pip freeze` não revelar grandes diferenças ou se você quiser ter certeza de uma instalação limpa:
   *   Ative seu ambiente virtual.
   *   Execute:
        ```bash
        pip install --upgrade --force-reinstall Pillow google-generativeai
        ```
   *   Reinicie a aplicação Streamlit.

**6. Consistência do Ambiente Virtual**

   Certifique-se de que em ambos os notebooks você está usando ambientes virtuais e que os ativou corretamente antes de instalar as dependências e antes de executar o `streamlit run`. Usar o ambiente global do Python pode levar a conflitos de dependências.

---

A mensagem de erro é bem clara sobre onde o problema está (`Image.open(io.BytesIO(part.inline_data.data))`). A chave é descobrir *por que* os `bytes` em `part.inline_data.data` não estão representando uma imagem válida no notebook problemático. Os passos de debug com `print`s na função `generate_image` e o teste com `test_image_generation.py` são os mais promissores para desvendar isso.
User: 2025-06-28 14:09:15,208 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:09:19,714 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent 
"HTTP/1.1 200 OK"
2025-06-28 14:09:20,818 [INFO] AFC remote call 1 is done.
I will generate an image of a tiny tabby kitten with wide eyes, standing on a miniature surfboard amidst the towering, foamy crest of a colossal ocean wave. The kitten will be mid-bite into a comically large slice of steaming lasagna, complete with visible layers of pasta, sauce, cheese, and ground meat, held precariously in its small paws.


parece que está gerando mas ainda ocorre o erro na lateral do streamlit 

Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E136676A0>


Assistant: Essa é uma informação crucial!

O fato de você ver:

```
I will generate an image of a tiny tabby kitten with wide eyes, standing on a miniature surfboard amidst the towering, foamy crest of a colossal ocean wave. The kitten will be mid-bite into a comically large slice of steaming lasagna, complete with visible layers of pasta, sauce, cheese, and ground meat, held precariously in its small paws.
```
e
```
2025-06-28 14:09:19,714 [INFO] HTTP Request: POST ... HTTP/1.1 200 OK
```

Indica que:

1.  A comunicação com a API do Gemini está **funcionando** (`HTTP/1.1 200 OK`).
2.  A API do Gemini está **respondendo** com conteúdo.
3.  O conteúdo inclui uma **parte de texto** (`part.text`) que descreve a imagem que será gerada.

No entanto, o erro `cannot identify image file <_io.BytesIO object at 0x...>` ainda significa que, quando a biblioteca `Pillow` (PIL) tenta abrir os bytes que deveriam ser a imagem, ela **não os reconhece como uma imagem válida**.

### O que isso significa?

A API de geração de imagem do Gemini (como `gemini-2.0-flash-exp-image-generation`) geralmente retorna uma resposta multi-parte:
*   Pode ter uma parte `text` (como o que você viu, descrevendo o que será gerado).
*   E *depois*, uma parte `inline_data` que contém os bytes da imagem gerada.

Se você está vendo o texto mas o erro na imagem persiste, isso pode indicar que:

1.  **A parte da imagem está faltando:** O Gemini retornou o texto de "confirmar" o prompt, mas por algum motivo (talvez o prompt seja muito complexo/inviável, ou algum filtro de segurança) **não conseguiu gerar ou enviar a imagem em si**. Então, `part.inline_data.data` estaria vazio ou `None`, mas o seu código *ainda tenta* processá-lo de alguma forma que leva ao erro.
2.  **Os dados da imagem estão corrompidos:** A API tentou enviar a imagem, mas os bytes chegaram corrompidos (talvez por um problema de rede muito sutil *após* o `200 OK`, durante a transferência do corpo da resposta), e o `Pillow` não consegue abri-los.
3.  **Lógica no loop `for part in response.candidates[0].content.parts:`:** Sua lógica atual está apenas `print`ando o `part.text` e, em seguida, esperando encontrar o `part.inline_data`. Se o `part.inline_data` está vazio ou corrompido, a função chega ao `Image.open` com dados ruins.

### Ações para Diagnosticar e Resolver:

Vamos refinar a função `generate_image` para ser mais robusta e para que os logs capturem *exatamente* o que está acontecendo com os dados da imagem.

**1. Adicione logs detalhados e um salvamento de bytes para inspeção (no `chat_streamlit.py`):**

Vamos ajustar a função `generate_image` para capturar e inspecionar os bytes da imagem.

```python
# chat_streamlit.py

# ... (seus imports e início da função)

def generate_image(prompt):
    api_key = os.getenv("API_KEY_GEMINI")
    if not api_key:
        st.error("API_KEY_GEMINI não encontrada no arquivo .env")
        return None

    client = genai.Client(api_key=api_key)

    try:
        st.info(f"Tentando gerar imagem com prompt: '{prompt}'")
        print(f"\n[DEBUG] Chamando API do Gemini para prompt: '{prompt}'") # Log para o terminal

        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        print(f"[DEBUG] Resposta completa do Gemini recebida. Tipo do objeto: {type(response)}")
        
        # Verificar feedback de bloqueio
        if response.prompt_feedback and response.prompt_feedback.block_reason:
            print(f"[DEBUG] Razão de Bloqueio da API: {response.prompt_feedback.block_reason}")
            st.warning(f"Geração de imagem bloqueada pelo Gemini: {response.prompt_feedback.block_reason}")
            return None
        
        if not response.candidates:
            print("[DEBUG] Nenhuma candidate encontrada na resposta do Gemini.")
            st.error("A IA não retornou candidatos de geração. Tente um prompt diferente.")
            return None

        # Variáveis para armazenar texto e dados da imagem
        text_feedback_from_gemini = ""
        image_data_bytes = None

        # Itera sobre todas as partes da resposta
        for part in response.candidates[0].content.parts:
            print(f"[DEBUG] Processando parte. Tipo do objeto 'part': {type(part)}")
            
            # Se a parte for texto
            if hasattr(part, 'text') and part.text is not None:
                text_feedback_from_gemini += part.text + "\n"
                print(f"[DEBUG] Conteúdo de texto recebido: '{part.text}'")
            
            # Se a parte for dados inline (imagem)
            if hasattr(part, 'inline_data') and part.inline_data is not None:
                print(f"[DEBUG] Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                if part.inline_data.data:
                    image_data_bytes = part.inline_data.data
                    print(f"[DEBUG] Tamanho dos dados da imagem: {len(image_data_bytes)} bytes.")
                    
                    # --- NOVO: Salve os bytes brutos para inspeção ---
                    # Isso ajudará a verificar se os bytes são realmente de uma imagem.
                    # Eles serão salvos no mesmo diretório do script.
                    with open("temp_gemini_image_debug.bin", "wb") as f:
                        f.write(image_data_bytes)
                    print("[DEBUG] Dados brutos da imagem salvos em 'temp_gemini_image_debug.bin' para análise.")
                    
                    # Interrompa o loop assim que encontrar os dados da imagem (assumindo que só há uma)
                    break
                else:
                    print("[DEBUG] part.inline_data.data é None ou vazio, apesar de inline_data existir.")
            else:
                print("[DEBUG] Parte sem texto ou dados inline relevantes.")
        
        # Tentar abrir a imagem se os bytes foram encontrados
        if image_data_bytes:
            try:
                image = Image.open(io.BytesIO(image_data_bytes))
                st.session_state.generated_image = image
                
                # Se o Gemini forneceu algum texto além da imagem, adicione-o ao chat history
                # junto com a imagem, se for relevante.
                # (Ajuste onde isso é adicionado ao chat history se o seu fluxo for diferente)
                if text_feedback_from_gemini.strip():
                     st.session_state.messages.append({"role": "assistant", "image": image, "content": text_feedback_from_gemini.strip()})
                else:
                    st.session_state.messages.append({"role": "assistant", "image": image, "content": f"Imagem gerada com o prompt: {prompt}"})
                
                st.session_state.generated_image = None # Limpa para não exibir em cima
                return image # Retorna a imagem e encerra a função
            except Exception as img_err:
                print(f"[DEBUG] ERRO da Pillow ao abrir imagem: {img_err}. Provavelmente dados de imagem corrompidos/inválidos.")
                st.error(f"Erro interno ao processar a imagem gerada: {img_err}. A imagem recebida pode estar corrompida.")
                return None
        else:
            # Se não encontrou dados de imagem
            msg = "O Gemini não retornou uma imagem válida para o prompt fornecido."
            if text_feedback_from_gemini.strip():
                msg += f" Feedback da IA: {text_feedback_from_gemini.strip()}"
            st.warning(msg)
            print(f"[DEBUG] {msg}")
            return None

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # Este é o erro que você está vendo
        print(f"[DEBUG] Exceção principal capturada em generate_image: {e}") # Log mais detalhado no terminal
        return None
```

**2. Execute a Aplicação e Analise os Logs e o Arquivo `temp_gemini_image_debug.bin`:**

*   Salve as alterações no seu `chat_streamlit.py`.
*   Execute `streamlit run chat_streamlit.py` no terminal do notebook problemático.
*   Tente gerar uma imagem com o prompt que causou o problema.
*   **Observe a saída no terminal:** Preste atenção aos `[DEBUG]` prints que você adicionou, especialmente o `Tamanho dos dados da imagem`.
    *   Se o tamanho for **zero ou muito pequeno** (ex: 0 a 100 bytes), isso indica que o Gemini não enviou os dados da imagem, ou enviou um "nada" corrompido.
    *   Se o tamanho for **razoável** (ex: dezenas ou centenas de KB), isso indica que os dados chegaram, mas o `Pillow` não os reconhece.
*   **Inspecione o arquivo `temp_gemini_image_debug.bin`:**
    *   Após o erro, localize este arquivo no diretório do seu script.
    *   Tente abri-lo com um visualizador de imagens (renomeie para `.png` ou `.jpg` se quiser tentar).
    *   Se ele não abrir ou estiver corrompido, você confirmou que o problema está nos bytes que o Gemini enviou para você.
    *   Se for um arquivo de imagem válido (o que seria surpreendente), o problema seria então na instalação do `Pillow` ou em alguma incompatibilidade no seu ambiente.

### O que esperar com base no seu `200 OK` e `text` part:

É muito provável que você veja o `[DEBUG] Tamanho dos dados da imagem: X bytes`, onde X é **zero** ou um número muito pequeno e **não há imagem válida no `temp_gemini_image_debug.bin`**.

Isso significaria que a API do Gemini, por algum motivo interno (complexidade do prompt, filtros de segurança etc.), não conseguiu materializar a imagem, apesar de ter confirmado o prompt com a parte textual. Se esse for o caso, não há muito que você possa fazer a nível de código para "consertar" uma imagem que não foi gerada corretamente pela API. A solução seria:

*   **Tentar prompts mais simples ou diferentes.**
*   **Verificar a documentação da API Gemini** para possíveis restrições ou limitações de conteúdo.
*   **Reportar o problema ao Google** se você acreditar que é um bug na API para o tipo de prompt que está usando.

Os logs detalhados e a inspeção do arquivo binário serão o "veredicto final" sobre a natureza do problema.
User: mas quero entender porque na outra máquina funciona... com o mesmo sistema operacional... 
tudo rodando sem ambiente virtual ?
2025-06-28 14:13:16,124 [INFO] Resposta da IA (texto): Entendi perfeitamente sua frustração! Essa é uma das situações mais irritantes no desenvolvimento de software: o famoso "funciona na minha máquina". Quando você tem o mesmo sistema operacional (Windows em ambos) e o mesmo código, e um funciona e o outro não, com o erro `cannot identify image file`, e ainda por cima, ambos estão rodando **sem ambiente virtual**, a resposta é quase sempre:

**Problemas de Consistência de Ambiente devido à Falta de Ambientes Virtuais.**

Vamos desdobrar isso:

### O Que Acontece Sem Ambientes Virtuais?

Quando você instala pacotes Python via `pip` sem ativar um ambiente virtual, eles são instalados no **ambiente global do seu Python**.

1.  **Versões Conflitantes e Inconsistentes:**
    *   Em uma máquina, você pode ter instalado `Pillow` versão X e `google-generativeai` versão Y.
    *   Na outra máquina, você pode ter instalado `Pillow` versão A e `google-generativeai` versão B (ou até mesmo `Pillow` X, mas `google-generativeai` B).
    *   Isso acontece porque, ao longo do tempo, você pode ter instalado outras ferramentas ou bibliotecas que **automaticamente atualizam ou instalam versões diferentes** das que seu projeto precisa, causando um "efeito cascata" de incompatibilidades.
    *   Pode ser que uma versão específica do `Pillow` no notebook problemático tenha um bug sutil ou uma incompatibilidade com o formato de imagem que a API do Gemini está retornando, ou que uma versão do `google-generativeai` esteja tratando os dados binários da resposta de forma ligeiramente diferente.

2.  **Instalações Corrompidas/Parciais:**
    *   Instalações globais são mais suscetíveis a ficarem "bagunçadas". Um `pip install` que falhou parcialmente, uma interrupção, ou até mesmo um erro de disco pode deixar uma biblioteca em um estado inconsistente.

3.  **Conflitos de Dependências Transitorias:**
    *   Se seu projeto A precisa da `LibZ==1.0` e seu projeto B precisa da `LibZ==2.0`, e ambos são instalados globalmente, o que acontece? A última instalação "ganha", ou você fica com um estado indefinido que pode causar comportamentos imprevisíveis.

### O Que o Erro `cannot identify image file` Significa Neste Contexto?

Mesmo com um `HTTP/1.1 200 OK`, o problema está na etapa de **decodificação dos bytes da imagem** que o Gemini enviou.

1.  **Bytes Incompletos ou Malformados:** A resposta do Gemini pode estar chegando com os bytes da imagem corrompidos ou truncados no notebook problemático. Mesmo com o 200 OK, o corpo da resposta (onde os dados binários da imagem estão) pode ter sido afetado por:
    *   **Problemas de Rede Sutil:** Uma instabilidade muito pequena na rede do notebook problemático pode corromper os dados binários durante a transferência, mesmo que o cabeçalho HTTP esteja correto.
    *   **Antivírus/Firewall:** Alguns softwares de segurança são excessivamente zelosos e podem interferir na transmissão de dados binários de APIs externas, alterando-os ou bloqueando partes.
    *   **Proxy:** Se um dos notebooks estiver atrás de um proxy corporativo que o outro não está, isso pode afetar a integridade dos dados.

2.  **`Pillow` Incapaz de Decodificar:** A versão ou instalação do `Pillow` no notebook problemático não consegue entender os bytes que recebeu, mesmo que eles *possam* estar teoricamente corretos. Isso aponta para uma incompatibilidade ou corrupção na própria biblioteca `Pillow`.

### Passos de Diagnóstico e Solução (Priorizando "Sem Ambiente Virtual"):

**Ação Mais Importante:**

**0. CRIE E USE AMBIENTES VIRTUAIS IMEDIATAMENTE!**
   Esta é a solução de longo prazo para evitar esse tipo de dor de cabeça.
   *   No notebook que **não funciona**:
        ```bash
        # 1. Navegue até o diretório raiz do seu projeto
        cd C:\caminho\para\seu\projeto

        # 2. Crie um ambiente virtual (se ainda não tiver um)
        python -m venv .venv

        # 3. Ative o ambiente virtual
        .\.venv\Scripts\activate
        ```
   *   No notebook que **funciona**:
        ```bash
        # 1. Navegue até o diretório raiz do seu projeto
        cd C:\caminho\para\seu\projeto

        # 2. Ative o ambiente virtual (se existir, caso contrário, crie um)
        .\.venv\Scripts\activate

        # 3. Gere um arquivo de requisitos (depois de ativar o ambiente)
        pip freeze > requirements.txt
        ```
        Este `requirements.txt` conterá as versões **exatas** das bibliotecas que *sabidamente funcionam* com sua aplicação.

   *   No notebook que **não funciona** (com o ambiente virtual ATIVADO):
        ```bash
        # Certifique-se de que o ambiente virtual está ativado!
        # Você verá (.venv) no início da linha de comando
        (.venv) C:\caminho\para\seu\projeto>

        # 4. Desinstale as dependências existentes para garantir uma instalação limpa
        # (Isso é crucial se você rodava globalmente e agora quer limpar)
        # Pode não ser estritamente necessário se você acabou de criar o venv,
        # mas é uma boa prática para evitar conflitos se você já tinha coisas lá.
        pip uninstall -y Pillow google-generativeai streamlit  # Adicione outras libs do seu projeto, se souber

        # 5. Instale as dependências exatamente como estão no 'requirements.txt'
        pip install -r requirements.txt
        ```

   *   **Tente rodar a aplicação Streamlit no ambiente virtual ativado:**
        ```bash
        (.venv) C:\caminho\para\seu\projeto> streamlit run chat_streamlit.py
        ```
   Isso por si só pode resolver o problema, pois garante que você está usando as mesmas versões das bibliotecas em ambos os notebooks.

---

**Se o problema persistir APÓS usar ambientes virtuais e alinhar `requirements.txt`, então continue com os passos de diagnóstico:**

**1. Re-execute o `test_image_generation.py` no ambiente virtual do notebook problemático.**
   *   Certifique-se que o ambiente virtual está ativado.
   *   `python test_image_generation.py`
   *   O script `test_image_generation.py` é um teste isolado crucial. Se ele falhar com o mesmo erro (`cannot identify image file`), isso aponta para um problema mais fundamental com a forma como os dados chegam da API ou como a biblioteca `Pillow` está instalada/configurada *naquele ambiente virtual*.

**2. Use os Logs Detalhados na Função `generate_image` (que adicionamos na resposta anterior):**
   *   Certifique-se de que os `print`s de depuração na função `generate_image` no `chat_streamlit.py` estão ativos e execute o Streamlit (`streamlit run chat_streamlit.py`) no ambiente virtual do notebook problemático.
   *   Preste atenção especial ao `[DEBUG] Tamanho dos dados da imagem: {len(image_data_bytes)} bytes`.
   *   **Análise do `temp_gemini_image_debug.bin`:** Se este arquivo for gerado, tente abri-lo com um editor hexadecimal (como HxD) ou até mesmo renomeá-lo para `.png` ou `.jpg` e tentar abri-lo com um visualizador de imagens.
        *   Se o tamanho for 0, ou muito pequeno, ou o arquivo não abrir, os bytes estão de fato corrompidos ou ausentes. Isso aponta para um problema na transmissão da API.
        *   Se o arquivo *parecer* ser uma imagem válida (o que é muito improvável com o erro que você está vendo), então o problema estaria na instalação do `Pillow` que não consegue decodificá-lo.

**3. Verifique Firewall, Antivírus e Conectividade de Rede Novamente:**
   *   Mesmo com um `200 OK`, a integridade do **corpo da resposta** pode ser comprometida.
   *   **Teste de Ping/Traceroute:**
        *   Abra o CMD: `ping generativelanguage.googleapis.com`
        *   Abra o CMD: `tracert generativelanguage.googleapis.com` (no Windows)
        *   Compare os resultados entre os dois notebooks. Veja se há diferenças significativas na latência, perda de pacotes ou rotas de rede que possam indicar um problema.
   *   **Desativar Temporariamente:** Se possível e seguro (apenas para teste rápido), desative o firewall do Windows e/ou o software antivírus no notebook problemático. Tente gerar a imagem novamente. Se funcionar, você terá que adicionar uma exceção para o Python/aplicação no software de segurança.

**4. Reinstalação Forçada de Bibliotecas (dentro do ambiente virtual):**
   Se o `requirements.txt` não resolver, e você tiver certeza de que não é um problema de rede, tente uma reinstalação forçada das bibliotecas chave:
   *   Ative o ambiente virtual.
   *   `pip install --upgrade --force-reinstall Pillow google-generativeai`
   *   Reinicie o Streamlit.

**5. Consistência da Instalação do Python:**
   Embora menos comum para esse tipo de erro, certifique-se de que ambas as instalações do Python (a base, antes do ambiente virtual) são de 64 bits (executar `python -c "import platform; print(platform.architecture())"`).

### Resumo da Ação Essencial:

A falta de ambientes virtuais é a causa mais provável para comportamentos inconsistentes em diferentes máquinas, mesmo com o mesmo sistema operacional. A primeira e mais crucial ação é **implementar e usar ambientes virtuais** e alinhar as dependências via `requirements.txt`.

Se mesmo assim o problema persistir, os logs detalhados e a inspeção do arquivo `temp_gemini_image_debug.bin` serão os seus melhores amigos para entender *o que exatamente* a API do Gemini está enviando e por que o `Pillow` não consegue decodificá-lo no notebook problemático.
2025-06-28 14:13:16,324 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:13:16,326 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:13:16,327 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:13:16,328 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:13:16,328 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:13:16,329 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:13:16,330 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:13:16,334 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:13:16,338 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:13:16,341 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:13:16,342 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:13:16,343 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:13:16,344 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:13:16,345 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:13:16,352 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:15:59,979 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:15:59,998 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:00,013 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:00,225 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:00,227 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:00,232 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:00,237 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:00,239 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:00,381 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:00,383 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:00,397 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:00,398 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:00,413 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:00,530 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:00,532 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:00,533 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:00,575 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:00,581 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:00,598 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:00,642 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:00,797 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:00,799 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:00,811 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:00,814 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:00,840 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:00,932 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:00,944 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:00,948 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:00,955 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:00,967 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:01,147 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:01,153 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:01,157 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:01,165 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:01,166 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:01,252 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:01,286 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:01,314 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:01,329 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:01,331 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:01,344 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:01,345 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:01,431 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:01,436 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:01,440 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:01,456 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:01,462 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:01,463 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:01,464 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:01,465 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:01,475 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:01,476 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:01,476 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:01,477 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:01,478 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:01,479 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:01,479 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:01,480 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:01,482 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:01,483 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:01,485 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:01,490 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:01,491 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:01,492 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:01,492 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:01,494 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:01,495 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:01,498 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:01,499 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:01,505 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:01,508 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:01,509 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:01,509 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:01,510 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:01,511 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:01,513 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:01,738 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:01,739 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:01,743 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:01,744 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:01,746 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:01,747 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:01,749 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:01,754 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:01,755 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:01,757 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:02,003 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:02,006 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:02,054 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:02,058 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:02,064 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:02,068 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:02,080 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:02,082 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:02,092 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:02,330 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:02,330 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:02,331 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:02,332 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:02,333 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:02,333 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:02,335 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:02,337 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:02,338 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:02,339 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:02,340 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:02,340 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:02,341 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:02,341 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:02,342 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:02,343 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:02,344 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:02,345 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:02,345 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:02,346 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:02,550 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:02,554 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:02,840 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:02,841 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:02,842 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:19,453 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:19,457 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:19,458 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:19,459 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:19,461 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:19,488 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:19,490 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:19,492 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:19,538 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:19,590 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:19,640 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:19,680 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:19,729 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:19,875 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:20,005 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:41,597 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:41,598 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:41,599 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:41,599 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:41,600 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:41,609 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:41,612 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:41,616 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:41,617 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:41,620 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:41,621 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:41,622 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:41,623 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:41,624 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:41,625 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:41,882 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:16:41,883 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:16:41,884 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:16:41,885 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:16:41,886 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:16:41,887 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:16:41,888 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:16:41,890 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:16:41,891 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:16:41,898 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:16:41,900 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:16:41,902 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:16:41,903 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:16:41,904 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:16:41,906 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:16:41,908 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

User: porque estou recebendo esse erro nessa aplicação sendo que no outro notebook a mesma aplicação roda perfeitamente ?
Assistant: É frustrante quando o mesmo código se comporta de forma diferente em máquinas distintas! Isso quase sempre aponta para **diferenças no ambiente de execução**, e não no código em si.

Analisando seu código, o principal suspeito para um erro que ocorre em um notebook e não em outro, especialmente se um for Windows e o outro Linux/macOS, é o uso de `PIL.ImageGrab`.

---

**O SUSPEITO PRINCIPAL: `PIL.ImageGrab` (Área de Transferência)**

No seu arquivo `chat_streamlit.py`, você tem a função `check_clipboard()` que usa `PIL.ImageGrab`:

```python
# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab # <--- AQUI ESTÁ O PROVÁVEL PROBLEMA

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        # ... (restante da função)
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}") # <--- ESSA É A MENSAGEM DE ERRO QUE VOCÊ DEVE ESTAR VENDO
        return None, None
```

A biblioteca `PIL.ImageGrab` (parte do Pillow) é altamente dependente do sistema operacional:

1.  **Windows:** Geralmente funciona sem problemas, pois o Windows possui APIs diretas para acesso à área de transferência de imagens.
2.  **Linux:** Requer utilitários de linha de comando como `scrot` ou `gnome-screenshot` instalados e configurados corretamente, e a biblioteca `Pillow` precisa ser compilada com suporte a essas ferramentas. Muitas distribuições Linux não vêm com isso pronto para o `ImageGrab`.
3.  **macOS:** Também tem suas próprias dependências, geralmente exigindo bibliotecas como `pyobjc` para interagir com o ambiente Cocoa.

**Se o seu "outro notebook" roda Windows e o notebook com problema roda Linux ou macOS, este é 99% a causa.** A mensagem de erro `"Erro ao acessar a área de transferência: {e}"` é exatamente o que você veria na barra lateral do Streamlit se essa parte falhasse.

---

**Passos para Diagnosticar e Resolver:**

1.  **Obtenha a Mensagem de Erro Exata:**
    *   No notebook onde a aplicação não roda, verifique a barra lateral do Streamlit. Você verá a mensagem exata do erro que está ocorrendo, começando com `"Erro ao acessar a área de transferência: ..."`. O que vem depois do `:` é crucial.
    *   Se não for `PIL.ImageGrab`, o erro estará em outro lugar.

2.  **Compare os Ambientes:**
    *   **Sistema Operacional:** Qual SO está rodando em cada notebook? (Ex: Windows 10 vs. Ubuntu 22.04 vs. macOS Ventura)
    *   **Versão do Python:** `python --version` em ambos.
    *   **Dependências Instaladas:**
        *   No notebook que funciona, execute: `pip freeze > requirements_working.txt`
        *   No notebook que não funciona, execute: `pip freeze > requirements_broken.txt`
        *   Compare os dois arquivos (`diff requirements_working.txt requirements_broken.txt`). Preste atenção especial à versão do `Pillow` e a presença de pacotes como `pywin32` (Windows) ou qualquer coisa relacionada a screenshots em Linux/macOS.

3.  **Soluções para o `PIL.ImageGrab`:**

    *   **Para Linux (Ubuntu/Debian):**
        *   Instale `scrot` (ou `gnome-screenshot`): `sudo apt-get update && sudo apt-get install scrot` (ou `sudo apt-get install gnome-screenshot`).
        *   Certifique-se de que o `Pillow` foi instalado *após* a instalação dessas ferramentas ou reinstale-o para que ele possa detectar as dependências.
        *   Pode ser necessário também `sudo apt-get install python3-tk python3-pil.imagetk` em algumas distribuições.

    *   **Para macOS:**
        *   Pode ser necessário instalar `pyobjc`: `pip install pyobjc`.

    *   **Solução Alternativa (Desativar ou Condicional):**
        Se você não precisa desesperadamente da funcionalidade de colar imagem da área de transferência e quer que a aplicação funcione em qualquer SO sem dependências externas complexas:
        Você já tem um `try-except` que lida com o erro. A aplicação deve continuar funcionando mesmo se a funcionalidade de colar não funcionar. No entanto, se o erro estiver parando a execução principal, isso é um problema.

        **Para evitar o erro na barra lateral em ambientes não-Windows:**
        Você pode adicionar uma verificação de sistema operacional:
        ```python
        import platform
        # ...

        def check_clipboard():
            # Só tenta importar ImageGrab se for Windows
            if platform.system() == "Windows":
                try:
                    from PIL import ImageGrab
                    img = ImageGrab.grabclipboard()
                    # ... (restante do código se img não for None)
                    if img is not None and isinstance(img, Image.Image):
                        # ... seu código para processar a imagem da área de transferência
                        # ...
                        return ClipboardFile(img_byte_arr), img
                    return None, None
                except Exception as e:
                    # A mensagem já é exibida no Streamlit, então não precisa de outro print aqui
                    st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
                    return None, None
            else:
                # Se não for Windows, não tenta acessar a área de transferência
                # E não mostra erro na barra lateral
                # st.sidebar.info("Colar da área de transferência disponível apenas no Windows.")
                return None, None
        ```
        Isso impediria que o erro específico de `ImageGrab` fosse tentado em sistemas não-Windows.

4.  **Verifique a Chave da API Gemini:**
    *   Certifique-se de que o arquivo `.env` existe no diretório raiz da aplicação no notebook problemático.
    *   Certifique-se de que `API_KEY_GEMINI` está definida corretamente dentro dele (Ex: `API_KEY_GEMINI="SUA_CHAVE_AQUI"`).
    *   Um erro relacionado à API Key geralmente resultaria em `ValueError("API Key não encontrada nas variáveis de ambiente")` ou erros da própria API (4xx, 5xx).

5.  **Verifique as Versões das Dependências:**
    *   É altamente recomendável ter um `requirements.txt` e instalá-lo em ambos os ambientes com `pip install -r requirements.txt`.
    *   Pode haver incompatibilidades sutis entre versões de `streamlit`, `google-generativeai`, `Pillow`, etc.

6.  **Permissões de Diretório:**
    *   Verifique se a aplicação tem permissão para criar e escrever nos diretórios `assets/`, `logs/` e `processed_images/` no notebook problemático. Isso é mais comum em Linux/macOS com permissões restritas.

7.  **Conectividade de Rede/Firewall:**
    *   O notebook problemático consegue acessar a internet? Existem restrições de firewall que podem estar bloqueando o acesso aos endpoints da API do Gemini?

---

**Resumo da Ação Mais Provável:**

O erro `Erro ao acessar a área de transferência: {e}` na barra lateral do Streamlit é quase uma certeza de que o problema está na tentativa de usar `PIL.ImageGrab` em um sistema operacional (provavelmente Linux/macOS) que não tem as dependências necessárias para isso.

Por favor, forneça a mensagem de erro exata da barra lateral e o sistema operacional de ambos os notebooks para uma análise mais precisa!
User: ois dois ambientes são windows. 
Assistant: Certo, isso muda o cenário! Se ambos os ambientes são Windows, então o problema não é a incompatibilidade do `PIL.ImageGrab` com Linux/macOS. No entanto, o `PIL.ImageGrab` continua sendo o principal suspeito, mas por razões diferentes.

Quando `PIL.ImageGrab` falha no Windows, geralmente aponta para um problema com a biblioteca `pywin32`, que o Pillow usa para interagir com as APIs do sistema operacional Windows.

Vamos desvendar isso.

---

**Causas Mais Comuns do `PIL.ImageGrab` Falhar no Windows:**

1.  **`pywin32` Ausente ou Corrompido:** A biblioteca `pywin32` é essencial para que o Pillow (que inclui `PIL.ImageGrab`) acesse a área de transferência do Windows de forma nativa. Se ela não estiver instalada ou estiver corrompida, a função falhará.
2.  **Ambientes Python Diferentes:** Mesmo que ambos sejam Windows, você pode ter ambientes Python (globais ou virtuais) ligeiramente diferentes em cada máquina, onde uma tem `pywin32` e a outra não, ou tem uma versão incompatível.
3.  **Conflitos de Versão:** Raramente, pode haver um conflito entre a versão do `Pillow` e a versão do `pywin32`, ou até mesmo com a versão do Python.
4.  **Conteúdo da Área de Transferência:** Embora menos provável dado seu relato, o `ImageGrab.grabclipboard()` retornará `None` se o conteúdo da área de transferência não for uma imagem (ex: texto, arquivos). No entanto, o erro que você vê sugere que o *acesso* à área de transferência está falhando, não apenas que ela está vazia.

---

**Passos Detalhados de Diagnóstico e Solução:**

**1. Obtenha a Mensagem de Erro Exata:**
   Este é o passo mais crucial. Quando você executa a aplicação no notebook que está dando problema, qual é a *mensagem de erro completa* que aparece na barra lateral do Streamlit, após "Erro ao acessar a área de transferência: "?
   Ex: `"Erro ao acessar a área de transferência: [WinError 5] Acesso negado."` ou `"Erro ao acessar a área de transferência: No module named 'win32clipboard'"` etc.
   Esta mensagem detalhada nos dirá exatamente o que está acontecendo.

**2. Compare as Dependências `pip`:**

   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que NÃO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos:** Abra `requirements_working.txt` e `requirements_broken.txt` e procure por:
        *   `Pillow` (P-I-L-L-O-W): Verifique as versões.
        *   `pywin32`: **Este é o mais importante.** Ele deve estar presente no `requirements_working.txt`. Se não estiver no `requirements_broken.txt` ou se a versão for diferente, encontramos o provável culpado.
        *   `streamlit`: Verifique as versões.

**3. Instale/Reinstale `pywin32` (no notebook problemático):**

   Se `pywin32` estiver faltando ou com uma versão diferente, tente o seguinte:

   *   **Verifique o ambiente virtual:** Se você estiver usando um ambiente virtual (altamente recomendado), ative-o primeiro.
        ```bash
        # Exemplo para venv:
        cd C:\caminho\para\seu\projeto
        .\.venv\Scripts\activate  # ou .\venv\Scripts\activate
        ```
   *   **Instale `pywin32`:**
        ```bash
        pip install pywin32
        ```
   *   **Ou, se já estiver instalado, force a reinstalação:**
        ```bash
        pip install --upgrade --force-reinstall pywin32
        ```
   *   **Reinicie a aplicação Streamlit.**

**4. Teste a Funcionalidade `ImageGrab` Isoladamente:**

   Para confirmar que o problema é realmente com `ImageGrab` e não algo mais amplo com o Streamlit ou seu código:

   *   Crie um pequeno script Python (`test_clipboard.py`) no notebook problemático:
        ```python
        import sys
        from PIL import ImageGrab, Image
        import io
        import time

        try:
            # Tenta pegar imagem da área de transferência
            img = ImageGrab.grabclipboard()

            if img is not None and isinstance(img, Image.Image):
                print("Imagem encontrada na área de transferência!")
                # Opcional: Salvar a imagem para verificar
                # img.save("clipboard_test.png")
                # print("Imagem salva como clipboard_test.png")
                
                # Tenta converter para bytes (simular o que você faz no Streamlit)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format='PNG')
                img_byte_arr.seek(0)
                print(f"Tamanho dos bytes: {len(img_byte_arr.getvalue())} bytes")
                
            elif img is None:
                print("Nenhuma imagem na área de transferência ou o conteúdo não é uma imagem.")
            else:
                print(f"Tipo de conteúdo inesperado na área de transferência: {type(img)}")

        except ImportError as e:
            print(f"Erro: Dependência faltando para PIL.ImageGrab: {e}")
            print("Tente 'pip install pywin32' e certifique-se de que está no ambiente correto.")
        except Exception as e:
            print(f"Erro inesperado ao acessar a área de transferência: {e}")
            print("Verifique permissões ou possíveis conflitos.")

        print("\nTeste concluído.")
        ```
   *   **Copie uma imagem** (não texto) para a área de transferência no Windows.
   *   Execute este script no terminal (ativando seu ambiente virtual, se aplicável): `python test_clipboard.py`
   *   Observe a saída. Isso confirmará se o `ImageGrab` está funcionando fora do Streamlit e qual erro exato ele gera.

**5. Verifique a Versão do Python e Arquitetura:**

   Embora menos comum hoje em dia, um Python de 32 bits pode ter problemas com algumas bibliotecas de 64 bits.

   *   Execute `python -c "import platform; print(platform.architecture())"` em ambos os notebooks. Verifique se ambos são `('64bit', 'WindowsPE')`.

**6. Outras Verificações (Menos Prováveis):**

*   **Antivírus/Firewall:** Muito raramente, um software de segurança pode interferir no acesso a certas APIs do sistema. Tente desativá-lo temporariamente (se for seguro fazer isso) para testar.
*   **Permissões de Usuário:** Certifique-se de que o usuário que executa a aplicação Streamlit tem as permissões necessárias para interagir com o sistema.

---

**Em Resumo:**

A principal ação é obter a **mensagem de erro exata** da barra lateral do Streamlit e, em seguida, **comparar as dependências `pip freeze`** para identificar se `pywin32` está faltando ou tem uma versão diferente. O teste isolado com `test_clipboard.py` também é muito útil para isolar o problema.

Assim que você tiver a mensagem de erro exata, poderemos refinar ainda mais o diagnóstico!
User: Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E13620360>
Assistant: Ok, essa nova informação muda completamente o diagnóstico! O erro `Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x...>` **não tem a ver com a área de transferência (`PIL.ImageGrab`)**.

Este erro significa que a biblioteca `Pillow` (PIL) tentou abrir algo que não reconheceu como uma imagem válida. A mensagem `_io.BytesIO object` indica que ela recebeu um fluxo de bytes, mas esses bytes não formam uma imagem (JPEG, PNG, etc.) que ela possa decodificar.

Vamos focar na função `generate_image` no `chat_streamlit.py`:

```python
# chat_streamlit.py

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # ... (verificação da API_KEY) ...

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text) # Se for só texto, imprime e continua o loop ou retorna None
            elif part.inline_data is not None:
                # <-- AQUI É O PONTO DO ERRO
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image # Retorna a imagem e sai da função

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # <-- É aqui que o erro é capturado e exibido
        return None
```

A linha `image = Image.open(io.BytesIO(part.inline_data.data))` é a que está falhando. Isso significa que, por algum motivo, o `part.inline_data.data` que o Gemini está retornando **não é um conjunto de bytes de uma imagem válida** no notebook problemático, ou está vazio.

---

**Possíveis Causas e Como Depurar:**

1.  **O Gemini não está gerando uma imagem para aquele prompt/requisição específica:**
    *   Às vezes, a API de geração de imagem do Gemini pode não conseguir gerar uma imagem para um prompt, ou pode bloquear a geração por motivos de segurança/conteúdo. Nesses casos, `part.inline_data` pode ser `None` ou `part.inline_data.data` pode estar vazio ou conter dados de erro/texto em vez de dados de imagem.
    *   Se o `response.prompt_feedback` tiver um `block_reason` (motivo de bloqueio), isso pode indicar o problema.

2.  **Problema de conectividade/API key:**
    *   Embora você já tenha verificado, é bom garantir que a `API_KEY_GEMINI` esteja configurada *exatamente* da mesma forma em ambos os notebooks. Um problema na chave pode levar a respostas malformadas da API, mesmo que não seja um erro óbvio de autenticação.
    *   Há alguma restrição de rede/firewall no notebook problemático que possa estar afetando a comunicação com a API do Gemini, fazendo com que as respostas venham incompletas ou corrompidas?

3.  **Versão da biblioteca `google-generativeai` ou `Pillow`:**
    *   Mesmo sendo Windows, pode haver pequenas diferenças nas versões dessas bibliotecas que causem um comportamento diferente.

---

**Passos para Diagnosticar e Resolver:**

**1. Teste o `test_image_generation.py` no notebook problemático:**

   Você tem um script de teste simples (`test_image_generation.py`) que faz uma chamada direta para a API de geração de imagem.
   *   Copie e execute o `test_image_generation.py` no notebook que está apresentando o erro.
   *   Ele deve tentar gerar "ed-image.png".
   *   **Resultado esperado:**
        *   **Se `test_image_generation.py` também falhar com o mesmo erro (`cannot identify image file`),** então o problema é mais fundamental na comunicação com o Gemini ou na forma como os dados são recebidos/processados pelo Pillow *naquele ambiente*.
        *   **Se `test_image_generation.py` funcionar e gerar a imagem corretamente,** então o problema está em algo mais específico no fluxo do `chat_streamlit.py` ao chamar `generate_image`.

**2. Adicione Logs Detalhados na Função `generate_image` (em `chat_streamlit.py`):**

   Vamos adicionar mais `print` statements (ou usar `logger.info` se preferir ver no logfile) para entender o que está vindo da API antes de chamar `Image.open()`.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Certifique-se que io está importado

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI não encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug print

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG ---
           print(f"\nDEBUG: Objeto de resposta Gemini completo: {response}")
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Razão de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Geração de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # Não há imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA não retornou candidatos de geração. Tente um prompt diferente.")
               return None

           found_image = False
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo: {type(part)}")
               if part.text is not None:
                   print(f"DEBUG: Conteúdo de texto recebido: '{part.text}'")
                   # Se houver texto, você pode decidir se quer usá-lo ou continuar procurando por imagem
               elif part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Tenta abrir a imagem
                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data é None ou vazio, apesar de inline_data não ser None.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline.")
           
           if not found_image:
               st.warning("O Gemini não retornou uma imagem para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este é o erro que você está vendo
           return None
   ```
   Execute a aplicação com estes prints e observe a saída no terminal onde você executa `streamlit run chat_streamlit.py`. Isso nos dará pistas vitais sobre o que o Gemini está realmente retornando.

**3. Compare as Dependências Novamente:**

   Embora menos provável, vale a pena uma revisão minuciosa.
   *   No notebook que FUNCIONA, execute: `pip freeze > requirements_working.txt`
   *   No notebook que NÃO FUNCIONA, execute: `pip freeze > requirements_broken.txt`
   *   Compare os arquivos, prestando atenção especial a:
        *   `Pillow` (ou PIL)
        *   `google-generativeai`
        *   `streamlit`
   *   Se houver diferença, tente instalar as versões do `requirements_working.txt` no notebook problemático.

**4. Reinstale `Pillow`:**

   Mesmo que as versões sejam as mesmas, uma instalação corrompida pode ser o problema.
   *   Ative seu ambiente virtual (se estiver usando).
   *   Execute: `pip install --upgrade --force-reinstall Pillow`
   *   Reinicie a aplicação Streamlit.

---

**Conclusão Preliminar:**

O erro `cannot identify image file` na função `generate_image` sugere fortemente que os dados que o Gemini retorna para `part.inline_data.data` **não são dados de imagem válidos** no ambiente problemático. Os passos de depuração acima, especialmente a execução do `test_image_generation.py` e os `print`s detalhados na função `generate_image` no `chat_streamlit.py`, nos darão as informações necessárias para identificar a causa raiz.
User: mas essa mesma aplicação funciona no meu outro notebook sem erros.
Assistant: Entendido! Se o problema persiste no segundo notebook, mesmo ambos sendo Windows, e o erro é `cannot identify image file <_io.BytesIO object at 0x...>`, isso significa que a questão não é a compatibilidade do sistema operacional com `PIL.ImageGrab`, mas sim algo específico na forma como os **dados da imagem gerada pelo Gemini** estão chegando ou sendo processados no notebook problemático.

Este erro (`cannot identify image file`) ocorre quando a biblioteca `Pillow` (que é usada para abrir a imagem com `Image.open`) recebe um fluxo de bytes (`_io.BytesIO`) que ela não consegue reconhecer como um formato de imagem válido (PNG, JPG, etc.).

Vamos focar na função `generate_image` no `chat_streamlit.py` e nos dados que ela recebe da API do Gemini.

### Causas Prováveis do Erro `cannot identify image file` (mesmo em Windows):

1.  **Dados de Imagem Inválidos/Incompletos da API do Gemini:**
    *   **API Key com Problema:** Embora você tenha verificado, às vezes uma API Key pode ter restrições, ou a própria chamada de API pode estar retornando uma resposta malformada ou um erro sutil que não é um `HTTP 200 OK` com dados de imagem.
    *   **Bloqueio de Conteúdo:** O prompt pode estar sendo bloqueado pelos filtros de segurança do Gemini, e em vez de uma imagem, a API retorna uma resposta vazia, uma mensagem de erro em texto, ou um `prompt_feedback` indicando um bloqueio, mas o código tenta processar `inline_data.data` como se fosse uma imagem.
    *   **Problema de Conectividade de Rede:** No notebook problemático, pode haver alguma instabilidade de rede, firewall ou proxy que está corrompendo ou truncando os dados da imagem durante a transferência da API do Gemini para a sua aplicação. Os bytes chegam incompletos ou inválidos, e o `Pillow` não consegue decodificá-los.

2.  **Problema na Instalação ou Versão do `Pillow` (ou `google-generativeai`):**
    *   **Instalação Corrompida:** Apesar de ser Windows, a instalação do Pillow pode ter algum arquivo corrompido ou faltando no ambiente Python do notebook problemático.
    *   **Diferença de Versão:** Se as versões do `Pillow` ou `google-generativeai` forem ligeiramente diferentes entre os dois notebooks, pode haver uma incompatibilidade sutil. Ex: Uma versão antiga do Pillow pode não suportar um formato específico que o Gemini está retornando, ou uma versão diferente do `google-generativeai` pode lidar com a resposta da API de forma distinta.

### Passos de Diagnóstico e Solução Detalhados:

**1. Re-execute o `test_image_generation.py` no notebook problemático.**
   Este é o teste mais importante para isolar o problema. Ele usa a mesma lógica de chamada de API e de abertura de imagem.
   *   Execute `python test_image_generation.py` no terminal do notebook que está com problema.
   *   **Observe a saída:**
        *   Se ele **também falhar com o mesmo erro** (`cannot identify image file`), então o problema é fundamental na forma como os dados chegam da API ou como o `Pillow` os interpreta *naquele ambiente*.
        *   Se ele **funcionar e gerar a imagem**, então o problema é mais específico no fluxo do `chat_streamlit.py` (embora isso seja menos provável com base no erro que você relatou).

**2. Adicione Logs Detalhados na Função `generate_image` (em `chat_streamlit.py`)**

   Isso nos permitirá ver exatamente o que o Gemini está retornando antes que o `Pillow` tente abrir a imagem.

   ```python
   # chat_streamlit.py

   # ...
   from google.genai import types
   import io # Verifique se 'io' está importado no topo do arquivo

   # ...

   def generate_image(prompt):
       api_key = os.getenv("API_KEY_GEMINI")
       if not api_key:
           st.error("API_KEY_GEMINI não encontrada no arquivo .env")
           return None

       client = genai.Client(api_key=api_key)

       try:
           st.info(f"Tentando gerar imagem com prompt: '{prompt}'") # Debug no Streamlit
           print(f"\nDEBUG: Chamando API do Gemini para prompt: '{prompt}'") # Debug no terminal

           response = client.models.generate_content(
               model='gemini-2.0-flash-exp-image-generation',
               contents=prompt,
               config=types.GenerateContentConfig(
                   response_modalities=['Text', 'Image']
               )
           )

           # --- ADICIONE ESTES PRINTS DE DEBUG (principalmente no terminal) ---
           print(f"\nDEBUG: Resposta completa do Gemini recebida. Tipo: {type(response)}")
           
           if response.prompt_feedback:
               print(f"DEBUG: Feedback do Prompt (bloqueio?): {response.prompt_feedback}")
               if response.prompt_feedback.block_reason:
                   print(f"DEBUG: Razão de Bloqueio: {response.prompt_feedback.block_reason}")
                   st.warning(f"Geração de imagem bloqueada: {response.prompt_feedback.block_reason}")
                   return None # Não há imagem para processar se for bloqueado
           
           if not response.candidates:
               print("DEBUG: Nenhuma candidate encontrada na resposta do Gemini.")
               st.error("A IA não retornou candidatos de geração. Tente um prompt diferente.")
               return None

           found_image = False
           # Assumimos que sempre pegamos o primeiro candidato (response.candidates[0])
           for part in response.candidates[0].content.parts:
               print(f"DEBUG: Processando parte. Tipo do objeto part: {type(part)}")
               
               if hasattr(part, 'text') and part.text is not None:
                   print(f"DEBUG: Conteúdo de texto recebido: '{part.text}'")
                   # Se houver texto e você esperar apenas imagem, pode ser um problema
               
               if hasattr(part, 'inline_data') and part.inline_data is not None:
                   print(f"DEBUG: Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                   if part.inline_data.data:
                       print(f"DEBUG: Tamanho dos dados da imagem: {len(part.inline_data.data)} bytes")
                       # Para ter certeza, grave os bytes em um arquivo temporário para inspeção
                       # with open("debug_image_data.bin", "wb") as f:
                       #     f.write(part.inline_data.data)
                       # print("DEBUG: Dados brutos da imagem salvos em 'debug_image_data.bin'.")

                       try:
                           image = Image.open(io.BytesIO(part.inline_data.data))
                           st.session_state.generated_image = image
                           found_image = True
                           print("DEBUG: Imagem aberta com sucesso pela Pillow.")
                           return image # Retorna a imagem e encerra a função
                       except Exception as img_err:
                           print(f"DEBUG: ERRO na Pillow ao abrir imagem: {img_err}")
                           st.error(f"Erro interno ao processar a imagem gerada: {img_err}. Verifique o console para mais detalhes.")
                           return None
                   else:
                       print("DEBUG: part.inline_data.data é None ou vazio, apesar de inline_data existir.")
               else:
                   print("DEBUG: Parte sem texto ou dados inline relevantes.")
           
           if not found_image:
               st.warning("O Gemini não retornou uma imagem válida para o prompt fornecido.")
               return None

       except Exception as e:
           st.error(f"Erro ao gerar imagem: {e}") # Este é o erro que você está vendo
           print(f"DEBUG: Exceção principal capturada: {e}") # Log mais detalhado no terminal
           return None
   ```
   Execute a aplicação Streamlit com estas alterações (`streamlit run chat_streamlit.py`) no notebook problemático. Os `print`s extras aparecerão no terminal onde você iniciou o Streamlit e nos darão informações cruciais sobre o que o Gemini está realmente enviando.

**3. Compare as Versões das Dependências (`pip freeze`)**

   Mesmo em Windows, a versão das bibliotecas pode ser um problema.
   *   **No notebook que FUNCIONA:**
        ```bash
        pip freeze > requirements_working.txt
        ```
   *   **No notebook que NÃO FUNCIONA:**
        ```bash
        pip freeze > requirements_broken.txt
        ```
   *   **Compare os arquivos `requirements_working.txt` e `requirements_broken.txt` meticulosamente.** Preste atenção especial às versões de:
        *   `Pillow` (P-I-L-L-O-W)
        *   `google-generativeai`
        *   `streamlit`
   *   **Ação:** Se houver diferenças, tente instalar as versões do notebook que funciona no notebook problemático. Por exemplo, se `Pillow==9.5.0` funciona e `Pillow==10.0.0` falha, tente:
        ```bash
        pip install Pillow==[VERSAO_DO_FUNCIONANDO] google-generativeai==[VERSAO_DO_FUNCIONANDO]
        ```
        Sempre ative seu ambiente virtual antes de instalar!

**4. Verifique a Conectividade de Rede e Firewall**

   *   **Teste de Ping:** Abra o Prompt de Comando (CMD) ou PowerShell no notebook problemático e tente:
        ```bash
        ping generativelanguage.googleapis.com
        ```
        Você deve ver pacotes sendo enviados e recebidos. Se houver perda de pacotes ou o destino não for alcançado, pode ser um problema de rede.
   *   **Firewall/Antivírus:** Temporariamente (com cautela, se for seguro) desative o firewall do Windows ou o antivírus para ver se eles estão bloqueando a comunicação com a API do Gemini. Se funcionar, você precisará adicionar uma exceção.
   *   **Proxy:** Se você estiver em uma rede corporativa, verifique se há um proxy que precisa ser configurado para chamadas HTTP/HTTPS.

**5. Reinstale as Bibliotecas Chave (Forçado)**

   Se a comparação do `pip freeze` não revelar grandes diferenças ou se você quiser ter certeza de uma instalação limpa:
   *   Ative seu ambiente virtual.
   *   Execute:
        ```bash
        pip install --upgrade --force-reinstall Pillow google-generativeai
        ```
   *   Reinicie a aplicação Streamlit.

**6. Consistência do Ambiente Virtual**

   Certifique-se de que em ambos os notebooks você está usando ambientes virtuais e que os ativou corretamente antes de instalar as dependências e antes de executar o `streamlit run`. Usar o ambiente global do Python pode levar a conflitos de dependências.

---

A mensagem de erro é bem clara sobre onde o problema está (`Image.open(io.BytesIO(part.inline_data.data))`). A chave é descobrir *por que* os `bytes` em `part.inline_data.data` não estão representando uma imagem válida no notebook problemático. Os passos de debug com `print`s na função `generate_image` e o teste com `test_image_generation.py` são os mais promissores para desvendar isso.
User: 2025-06-28 14:09:15,208 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:09:19,714 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent 
"HTTP/1.1 200 OK"
2025-06-28 14:09:20,818 [INFO] AFC remote call 1 is done.
I will generate an image of a tiny tabby kitten with wide eyes, standing on a miniature surfboard amidst the towering, foamy crest of a colossal ocean wave. The kitten will be mid-bite into a comically large slice of steaming lasagna, complete with visible layers of pasta, sauce, cheese, and ground meat, held precariously in its small paws.


parece que está gerando mas ainda ocorre o erro na lateral do streamlit 

Erro ao gerar imagem: cannot identify image file <_io.BytesIO object at 0x0000028E136676A0>


Assistant: Essa é uma informação crucial!

O fato de você ver:

```
I will generate an image of a tiny tabby kitten with wide eyes, standing on a miniature surfboard amidst the towering, foamy crest of a colossal ocean wave. The kitten will be mid-bite into a comically large slice of steaming lasagna, complete with visible layers of pasta, sauce, cheese, and ground meat, held precariously in its small paws.
```
e
```
2025-06-28 14:09:19,714 [INFO] HTTP Request: POST ... HTTP/1.1 200 OK
```

Indica que:

1.  A comunicação com a API do Gemini está **funcionando** (`HTTP/1.1 200 OK`).
2.  A API do Gemini está **respondendo** com conteúdo.
3.  O conteúdo inclui uma **parte de texto** (`part.text`) que descreve a imagem que será gerada.

No entanto, o erro `cannot identify image file <_io.BytesIO object at 0x...>` ainda significa que, quando a biblioteca `Pillow` (PIL) tenta abrir os bytes que deveriam ser a imagem, ela **não os reconhece como uma imagem válida**.

### O que isso significa?

A API de geração de imagem do Gemini (como `gemini-2.0-flash-exp-image-generation`) geralmente retorna uma resposta multi-parte:
*   Pode ter uma parte `text` (como o que você viu, descrevendo o que será gerado).
*   E *depois*, uma parte `inline_data` que contém os bytes da imagem gerada.

Se você está vendo o texto mas o erro na imagem persiste, isso pode indicar que:

1.  **A parte da imagem está faltando:** O Gemini retornou o texto de "confirmar" o prompt, mas por algum motivo (talvez o prompt seja muito complexo/inviável, ou algum filtro de segurança) **não conseguiu gerar ou enviar a imagem em si**. Então, `part.inline_data.data` estaria vazio ou `None`, mas o seu código *ainda tenta* processá-lo de alguma forma que leva ao erro.
2.  **Os dados da imagem estão corrompidos:** A API tentou enviar a imagem, mas os bytes chegaram corrompidos (talvez por um problema de rede muito sutil *após* o `200 OK`, durante a transferência do corpo da resposta), e o `Pillow` não consegue abri-los.
3.  **Lógica no loop `for part in response.candidates[0].content.parts:`:** Sua lógica atual está apenas `print`ando o `part.text` e, em seguida, esperando encontrar o `part.inline_data`. Se o `part.inline_data` está vazio ou corrompido, a função chega ao `Image.open` com dados ruins.

### Ações para Diagnosticar e Resolver:

Vamos refinar a função `generate_image` para ser mais robusta e para que os logs capturem *exatamente* o que está acontecendo com os dados da imagem.

**1. Adicione logs detalhados e um salvamento de bytes para inspeção (no `chat_streamlit.py`):**

Vamos ajustar a função `generate_image` para capturar e inspecionar os bytes da imagem.

```python
# chat_streamlit.py

# ... (seus imports e início da função)

def generate_image(prompt):
    api_key = os.getenv("API_KEY_GEMINI")
    if not api_key:
        st.error("API_KEY_GEMINI não encontrada no arquivo .env")
        return None

    client = genai.Client(api_key=api_key)

    try:
        st.info(f"Tentando gerar imagem com prompt: '{prompt}'")
        print(f"\n[DEBUG] Chamando API do Gemini para prompt: '{prompt}'") # Log para o terminal

        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        print(f"[DEBUG] Resposta completa do Gemini recebida. Tipo do objeto: {type(response)}")
        
        # Verificar feedback de bloqueio
        if response.prompt_feedback and response.prompt_feedback.block_reason:
            print(f"[DEBUG] Razão de Bloqueio da API: {response.prompt_feedback.block_reason}")
            st.warning(f"Geração de imagem bloqueada pelo Gemini: {response.prompt_feedback.block_reason}")
            return None
        
        if not response.candidates:
            print("[DEBUG] Nenhuma candidate encontrada na resposta do Gemini.")
            st.error("A IA não retornou candidatos de geração. Tente um prompt diferente.")
            return None

        # Variáveis para armazenar texto e dados da imagem
        text_feedback_from_gemini = ""
        image_data_bytes = None

        # Itera sobre todas as partes da resposta
        for part in response.candidates[0].content.parts:
            print(f"[DEBUG] Processando parte. Tipo do objeto 'part': {type(part)}")
            
            # Se a parte for texto
            if hasattr(part, 'text') and part.text is not None:
                text_feedback_from_gemini += part.text + "\n"
                print(f"[DEBUG] Conteúdo de texto recebido: '{part.text}'")
            
            # Se a parte for dados inline (imagem)
            if hasattr(part, 'inline_data') and part.inline_data is not None:
                print(f"[DEBUG] Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                if part.inline_data.data:
                    image_data_bytes = part.inline_data.data
                    print(f"[DEBUG] Tamanho dos dados da imagem: {len(image_data_bytes)} bytes.")
                    
                    # --- NOVO: Salve os bytes brutos para inspeção ---
                    # Isso ajudará a verificar se os bytes são realmente de uma imagem.
                    # Eles serão salvos no mesmo diretório do script.
                    with open("temp_gemini_image_debug.bin", "wb") as f:
                        f.write(image_data_bytes)
                    print("[DEBUG] Dados brutos da imagem salvos em 'temp_gemini_image_debug.bin' para análise.")
                    
                    # Interrompa o loop assim que encontrar os dados da imagem (assumindo que só há uma)
                    break
                else:
                    print("[DEBUG] part.inline_data.data é None ou vazio, apesar de inline_data existir.")
            else:
                print("[DEBUG] Parte sem texto ou dados inline relevantes.")
        
        # Tentar abrir a imagem se os bytes foram encontrados
        if image_data_bytes:
            try:
                image = Image.open(io.BytesIO(image_data_bytes))
                st.session_state.generated_image = image
                
                # Se o Gemini forneceu algum texto além da imagem, adicione-o ao chat history
                # junto com a imagem, se for relevante.
                # (Ajuste onde isso é adicionado ao chat history se o seu fluxo for diferente)
                if text_feedback_from_gemini.strip():
                     st.session_state.messages.append({"role": "assistant", "image": image, "content": text_feedback_from_gemini.strip()})
                else:
                    st.session_state.messages.append({"role": "assistant", "image": image, "content": f"Imagem gerada com o prompt: {prompt}"})
                
                st.session_state.generated_image = None # Limpa para não exibir em cima
                return image # Retorna a imagem e encerra a função
            except Exception as img_err:
                print(f"[DEBUG] ERRO da Pillow ao abrir imagem: {img_err}. Provavelmente dados de imagem corrompidos/inválidos.")
                st.error(f"Erro interno ao processar a imagem gerada: {img_err}. A imagem recebida pode estar corrompida.")
                return None
        else:
            # Se não encontrou dados de imagem
            msg = "O Gemini não retornou uma imagem válida para o prompt fornecido."
            if text_feedback_from_gemini.strip():
                msg += f" Feedback da IA: {text_feedback_from_gemini.strip()}"
            st.warning(msg)
            print(f"[DEBUG] {msg}")
            return None

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # Este é o erro que você está vendo
        print(f"[DEBUG] Exceção principal capturada em generate_image: {e}") # Log mais detalhado no terminal
        return None
```

**2. Execute a Aplicação e Analise os Logs e o Arquivo `temp_gemini_image_debug.bin`:**

*   Salve as alterações no seu `chat_streamlit.py`.
*   Execute `streamlit run chat_streamlit.py` no terminal do notebook problemático.
*   Tente gerar uma imagem com o prompt que causou o problema.
*   **Observe a saída no terminal:** Preste atenção aos `[DEBUG]` prints que você adicionou, especialmente o `Tamanho dos dados da imagem`.
    *   Se o tamanho for **zero ou muito pequeno** (ex: 0 a 100 bytes), isso indica que o Gemini não enviou os dados da imagem, ou enviou um "nada" corrompido.
    *   Se o tamanho for **razoável** (ex: dezenas ou centenas de KB), isso indica que os dados chegaram, mas o `Pillow` não os reconhece.
*   **Inspecione o arquivo `temp_gemini_image_debug.bin`:**
    *   Após o erro, localize este arquivo no diretório do seu script.
    *   Tente abri-lo com um visualizador de imagens (renomeie para `.png` ou `.jpg` se quiser tentar).
    *   Se ele não abrir ou estiver corrompido, você confirmou que o problema está nos bytes que o Gemini enviou para você.
    *   Se for um arquivo de imagem válido (o que seria surpreendente), o problema seria então na instalação do `Pillow` ou em alguma incompatibilidade no seu ambiente.

### O que esperar com base no seu `200 OK` e `text` part:

É muito provável que você veja o `[DEBUG] Tamanho dos dados da imagem: X bytes`, onde X é **zero** ou um número muito pequeno e **não há imagem válida no `temp_gemini_image_debug.bin`**.

Isso significaria que a API do Gemini, por algum motivo interno (complexidade do prompt, filtros de segurança etc.), não conseguiu materializar a imagem, apesar de ter confirmado o prompt com a parte textual. Se esse for o caso, não há muito que você possa fazer a nível de código para "consertar" uma imagem que não foi gerada corretamente pela API. A solução seria:

*   **Tentar prompts mais simples ou diferentes.**
*   **Verificar a documentação da API Gemini** para possíveis restrições ou limitações de conteúdo.
*   **Reportar o problema ao Google** se você acreditar que é um bug na API para o tipo de prompt que está usando.

Os logs detalhados e a inspeção do arquivo binário serão o "veredicto final" sobre a natureza do problema.
User: mas quero entender porque na outra máquina funciona... com o mesmo sistema operacional... 
tudo rodando sem ambiente virtual ?
Assistant: Entendi perfeitamente sua frustração! Essa é uma das situações mais irritantes no desenvolvimento de software: o famoso "funciona na minha máquina". Quando você tem o mesmo sistema operacional (Windows em ambos) e o mesmo código, e um funciona e o outro não, com o erro `cannot identify image file`, e ainda por cima, ambos estão rodando **sem ambiente virtual**, a resposta é quase sempre:

**Problemas de Consistência de Ambiente devido à Falta de Ambientes Virtuais.**

Vamos desdobrar isso:

### O Que Acontece Sem Ambientes Virtuais?

Quando você instala pacotes Python via `pip` sem ativar um ambiente virtual, eles são instalados no **ambiente global do seu Python**.

1.  **Versões Conflitantes e Inconsistentes:**
    *   Em uma máquina, você pode ter instalado `Pillow` versão X e `google-generativeai` versão Y.
    *   Na outra máquina, você pode ter instalado `Pillow` versão A e `google-generativeai` versão B (ou até mesmo `Pillow` X, mas `google-generativeai` B).
    *   Isso acontece porque, ao longo do tempo, você pode ter instalado outras ferramentas ou bibliotecas que **automaticamente atualizam ou instalam versões diferentes** das que seu projeto precisa, causando um "efeito cascata" de incompatibilidades.
    *   Pode ser que uma versão específica do `Pillow` no notebook problemático tenha um bug sutil ou uma incompatibilidade com o formato de imagem que a API do Gemini está retornando, ou que uma versão do `google-generativeai` esteja tratando os dados binários da resposta de forma ligeiramente diferente.

2.  **Instalações Corrompidas/Parciais:**
    *   Instalações globais são mais suscetíveis a ficarem "bagunçadas". Um `pip install` que falhou parcialmente, uma interrupção, ou até mesmo um erro de disco pode deixar uma biblioteca em um estado inconsistente.

3.  **Conflitos de Dependências Transitorias:**
    *   Se seu projeto A precisa da `LibZ==1.0` e seu projeto B precisa da `LibZ==2.0`, e ambos são instalados globalmente, o que acontece? A última instalação "ganha", ou você fica com um estado indefinido que pode causar comportamentos imprevisíveis.

### O Que o Erro `cannot identify image file` Significa Neste Contexto?

Mesmo com um `HTTP/1.1 200 OK`, o problema está na etapa de **decodificação dos bytes da imagem** que o Gemini enviou.

1.  **Bytes Incompletos ou Malformados:** A resposta do Gemini pode estar chegando com os bytes da imagem corrompidos ou truncados no notebook problemático. Mesmo com o 200 OK, o corpo da resposta (onde os dados binários da imagem estão) pode ter sido afetado por:
    *   **Problemas de Rede Sutil:** Uma instabilidade muito pequena na rede do notebook problemático pode corromper os dados binários durante a transferência, mesmo que o cabeçalho HTTP esteja correto.
    *   **Antivírus/Firewall:** Alguns softwares de segurança são excessivamente zelosos e podem interferir na transmissão de dados binários de APIs externas, alterando-os ou bloqueando partes.
    *   **Proxy:** Se um dos notebooks estiver atrás de um proxy corporativo que o outro não está, isso pode afetar a integridade dos dados.

2.  **`Pillow` Incapaz de Decodificar:** A versão ou instalação do `Pillow` no notebook problemático não consegue entender os bytes que recebeu, mesmo que eles *possam* estar teoricamente corretos. Isso aponta para uma incompatibilidade ou corrupção na própria biblioteca `Pillow`.

### Passos de Diagnóstico e Solução (Priorizando "Sem Ambiente Virtual"):

**Ação Mais Importante:**

**0. CRIE E USE AMBIENTES VIRTUAIS IMEDIATAMENTE!**
   Esta é a solução de longo prazo para evitar esse tipo de dor de cabeça.
   *   No notebook que **não funciona**:
        ```bash
        # 1. Navegue até o diretório raiz do seu projeto
        cd C:\caminho\para\seu\projeto

        # 2. Crie um ambiente virtual (se ainda não tiver um)
        python -m venv .venv

        # 3. Ative o ambiente virtual
        .\.venv\Scripts\activate
        ```
   *   No notebook que **funciona**:
        ```bash
        # 1. Navegue até o diretório raiz do seu projeto
        cd C:\caminho\para\seu\projeto

        # 2. Ative o ambiente virtual (se existir, caso contrário, crie um)
        .\.venv\Scripts\activate

        # 3. Gere um arquivo de requisitos (depois de ativar o ambiente)
        pip freeze > requirements.txt
        ```
        Este `requirements.txt` conterá as versões **exatas** das bibliotecas que *sabidamente funcionam* com sua aplicação.

   *   No notebook que **não funciona** (com o ambiente virtual ATIVADO):
        ```bash
        # Certifique-se de que o ambiente virtual está ativado!
        # Você verá (.venv) no início da linha de comando
        (.venv) C:\caminho\para\seu\projeto>

        # 4. Desinstale as dependências existentes para garantir uma instalação limpa
        # (Isso é crucial se você rodava globalmente e agora quer limpar)
        # Pode não ser estritamente necessário se você acabou de criar o venv,
        # mas é uma boa prática para evitar conflitos se você já tinha coisas lá.
        pip uninstall -y Pillow google-generativeai streamlit  # Adicione outras libs do seu projeto, se souber

        # 5. Instale as dependências exatamente como estão no 'requirements.txt'
        pip install -r requirements.txt
        ```

   *   **Tente rodar a aplicação Streamlit no ambiente virtual ativado:**
        ```bash
        (.venv) C:\caminho\para\seu\projeto> streamlit run chat_streamlit.py
        ```
   Isso por si só pode resolver o problema, pois garante que você está usando as mesmas versões das bibliotecas em ambos os notebooks.

---

**Se o problema persistir APÓS usar ambientes virtuais e alinhar `requirements.txt`, então continue com os passos de diagnóstico:**

**1. Re-execute o `test_image_generation.py` no ambiente virtual do notebook problemático.**
   *   Certifique-se que o ambiente virtual está ativado.
   *   `python test_image_generation.py`
   *   O script `test_image_generation.py` é um teste isolado crucial. Se ele falhar com o mesmo erro (`cannot identify image file`), isso aponta para um problema mais fundamental com a forma como os dados chegam da API ou como a biblioteca `Pillow` está instalada/configurada *naquele ambiente virtual*.

**2. Use os Logs Detalhados na Função `generate_image` (que adicionamos na resposta anterior):**
   *   Certifique-se de que os `print`s de depuração na função `generate_image` no `chat_streamlit.py` estão ativos e execute o Streamlit (`streamlit run chat_streamlit.py`) no ambiente virtual do notebook problemático.
   *   Preste atenção especial ao `[DEBUG] Tamanho dos dados da imagem: {len(image_data_bytes)} bytes`.
   *   **Análise do `temp_gemini_image_debug.bin`:** Se este arquivo for gerado, tente abri-lo com um editor hexadecimal (como HxD) ou até mesmo renomeá-lo para `.png` ou `.jpg` e tentar abri-lo com um visualizador de imagens.
        *   Se o tamanho for 0, ou muito pequeno, ou o arquivo não abrir, os bytes estão de fato corrompidos ou ausentes. Isso aponta para um problema na transmissão da API.
        *   Se o arquivo *parecer* ser uma imagem válida (o que é muito improvável com o erro que você está vendo), então o problema estaria na instalação do `Pillow` que não consegue decodificá-lo.

**3. Verifique Firewall, Antivírus e Conectividade de Rede Novamente:**
   *   Mesmo com um `200 OK`, a integridade do **corpo da resposta** pode ser comprometida.
   *   **Teste de Ping/Traceroute:**
        *   Abra o CMD: `ping generativelanguage.googleapis.com`
        *   Abra o CMD: `tracert generativelanguage.googleapis.com` (no Windows)
        *   Compare os resultados entre os dois notebooks. Veja se há diferenças significativas na latência, perda de pacotes ou rotas de rede que possam indicar um problema.
   *   **Desativar Temporariamente:** Se possível e seguro (apenas para teste rápido), desative o firewall do Windows e/ou o software antivírus no notebook problemático. Tente gerar a imagem novamente. Se funcionar, você terá que adicionar uma exceção para o Python/aplicação no software de segurança.

**4. Reinstalação Forçada de Bibliotecas (dentro do ambiente virtual):**
   Se o `requirements.txt` não resolver, e você tiver certeza de que não é um problema de rede, tente uma reinstalação forçada das bibliotecas chave:
   *   Ative o ambiente virtual.
   *   `pip install --upgrade --force-reinstall Pillow google-generativeai`
   *   Reinicie o Streamlit.

**5. Consistência da Instalação do Python:**
   Embora menos comum para esse tipo de erro, certifique-se de que ambas as instalações do Python (a base, antes do ambiente virtual) são de 64 bits (executar `python -c "import platform; print(platform.architecture())"`).

### Resumo da Ação Essencial:

A falta de ambientes virtuais é a causa mais provável para comportamentos inconsistentes em diferentes máquinas, mesmo com o mesmo sistema operacional. A primeira e mais crucial ação é **implementar e usar ambientes virtuais** e alinhar as dependências via `requirements.txt`.

Se mesmo assim o problema persistir, os logs detalhados e a inspeção do arquivo `temp_gemini_image_debug.bin` serão os seus melhores amigos para entender *o que exatamente* a API do Gemini está enviando e por que o `Pillow` não consegue decodificá-lo no notebook problemático.
User: assim ficou com tela preta ...

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# chat_streamlit.py

# ... (seus imports e início da função)

def generate_image(prompt):
    api_key = os.getenv("API_KEY_GEMINI")
    if not api_key:
        st.error("API_KEY_GEMINI não encontrada no arquivo .env")
        return None

    client = genai.Client(api_key=api_key)

    try:
        st.info(f"Tentando gerar imagem com prompt: '{prompt}'")
        print(f"\n[DEBUG] Chamando API do Gemini para prompt: '{prompt}'") # Log para o terminal

        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        print(f"[DEBUG] Resposta completa do Gemini recebida. Tipo do objeto: {type(response)}")

        # Verificar feedback de bloqueio
        if response.prompt_feedback and response.prompt_feedback.block_reason:
            print(f"[DEBUG] Razão de Bloqueio da API: {response.prompt_feedback.block_reason}")
            st.warning(f"Geração de imagem bloqueada pelo Gemini: {response.prompt_feedback.block_reason}")
            return None

        if not response.candidates:
            print("[DEBUG] Nenhuma candidate encontrada na resposta do Gemini.")
            st.error("A IA não retornou candidatos de geração. Tente um prompt diferente.")
            return None

        # Variáveis para armazenar texto e dados da imagem
        text_feedback_from_gemini = ""
        image_data_bytes = None

        # Itera sobre todas as partes da resposta
        for part in response.candidates[0].content.parts:
            print(f"[DEBUG] Processando parte. Tipo do objeto 'part': {type(part)}")

            # Se a parte for texto
            if hasattr(part, 'text') and part.text is not None:
                text_feedback_from_gemini += part.text + "\n"
                print(f"[DEBUG] Conteúdo de texto recebido: '{part.text}'")

            # Se a parte for dados inline (imagem)
            if hasattr(part, 'inline_data') and part.inline_data is not None:
                print(f"[DEBUG] Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                if part.inline_data.data:
                    image_data_bytes = part.inline_data.data
                    print(f"[DEBUG] Tamanho dos dados da imagem: {len(image_data_bytes)} bytes.")

                    # --- NOVO: Salve os bytes brutos para inspeção ---
                    # Isso ajudará a verificar se os bytes são realmente de uma imagem.
                    # Eles serão salvos no mesmo diretório do script.
                    with open("temp_gemini_image_debug.bin", "wb") as f:
                        f.write(image_data_bytes)
                    print("[DEBUG] Dados brutos da imagem salvos em 'temp_gemini_image_debug.bin' para análise.")

                    # Interrompa o loop assim que encontrar os dados da imagem (assumindo que só há uma)
                    break
                else:
                    print("[DEBUG] part.inline_data.data é None ou vazio, apesar de inline_data existir.")
            else:
                print("[DEBUG] Parte sem texto ou dados inline relevantes.")

        # Tentar abrir a imagem se os bytes foram encontrados
        if image_data_bytes:
            try:
                image = Image.open(io.BytesIO(image_data_bytes))
                st.session_state.generated_image = image

                # Se o Gemini forneceu algum texto além da imagem, adicione-o ao chat history
                # junto com a imagem, se for relevante.
                # (Ajuste onde isso é adicionado ao chat history se o seu fluxo for diferente)
                if text_feedback_from_gemini.strip():
                     st.session_state.messages.append({"role": "assistant", "image": image, "content": text_feedback_from_gemini.strip()})
                else:
                    st.session_state.messages.append({"role": "assistant", "image": image, "content": f"Imagem gerada com o prompt: {prompt}"})

                st.session_state.generated_image = None # Limpa para não exibir em cima
                return image # Retorna a imagem e encerra a função
            except Exception as img_err:
                print(f"[DEBUG] ERRO da Pillow ao abrir imagem: {img_err}. Provavelmente dados de imagem corrompidos/inválidos.")
                st.error(f"Erro interno ao processar a imagem gerada: {img_err}. A imagem recebida pode estar corrompida.")
                return None
        else:
            # Se não encontrou dados de imagem
            msg = "O Gemini não retornou uma imagem válida para o prompt fornecido."
            if text_feedback_from_gemini.strip():
                msg += f" Feedback da IA: {text_feedback_from_gemini.strip()}"
            st.warning(msg)
            print(f"[DEBUG] {msg}")
            return None

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}") # Este é o erro que você está vendo
        print(f"[DEBUG] Exceção principal capturada em generate_image: {e}") # Log mais detalhado no terminal
        return None
2025-06-28 14:17:20,356 [INFO] Resposta da IA (texto): Ah, a temida "tela preta" no Streamlit! Isso é um sintoma **completamente diferente** do erro de imagem e geralmente indica um problema na forma como os componentes do Streamlit estão sendo chamados, especificamente em relação à ordem de execução ou a blocos de conteúdo.

O erro `cannot identify image file` ainda pode estar acontecendo em segundo plano, mas a tela preta significa que a aplicação Streamlit está falhando *antes* de renderizar a interface corretamente, geralmente porque você está tentando escrever algo em uma área "não permitida" depois que outros componentes já foram renderizados.

### A Causa Provável da Tela Preta:

No Streamlit, componentes como `st.chat_input()` e `st.chat_message()` têm um comportamento especial de posicionamento na interface. Uma vez que você usa `st.chat_input()`, o Streamlit "assume" que tudo que vem depois na execução do script será exibido *abaixo* do campo de entrada de chat, ou dentro de `st.chat_message` blocks.

Quando você adicionou os `st.info`, `st.warning`, e `st.error` dentro da função `generate_image`, você está chamando comandos `st.` **fora de um bloco `st.chat_message` ou `st.sidebar`**, e possivelmente *depois* que o `st.chat_input` já foi processado na execução anterior. Isso confunde o Streamlit e causa um erro de layout que ele resolve com uma tela em branco ou uma mensagem de erro no console sobre "calling a Streamlit command in a non-allowed content block".

**O que fazer:**

Os `st.info`, `st.warning` e `st.error` que você adicionou dentro de `generate_image` devem ser apenas `print()` statements para depuração no terminal, ou devem ser adicionados ao `st.session_state.messages` para serem exibidos no chat.

### Solução: Remova `st.info`, `st.warning`, `st.error` de dentro de `generate_image` (ou coloque-os no histórico de mensagens)

Vamos ajustar a função `generate_image` novamente, removendo os `st.` calls que causam o problema e colocando as mensagens de aviso e erro **dentro do histórico de mensagens do chat**, que é o lugar correto para mensagens ao usuário nesse contexto.

```python
# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    api_key = os.getenv("API_KEY_GEMINI")
    if not api_key:
        print("[DEBUG] API_KEY_GEMINI não encontrada no arquivo .env")
        st.session_state.messages.append({"role": "assistant", "content": "❌ Erro: API_KEY_GEMINI não encontrada para gerar imagem."})
        return None

    client = genai.Client(api_key=api_key)

    try:
        print(f"\n[DEBUG] Chamando API do Gemini para prompt: '{prompt}'") # Log para o terminal

        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        print(f"[DEBUG] Resposta completa do Gemini recebida. Tipo do objeto: {type(response)}")

        # Verificar feedback de bloqueio
        if response.prompt_feedback and response.prompt_feedback.block_reason:
            print(f"[DEBUG] Razão de Bloqueio da API: {response.prompt_feedback.block_reason}")
            st.session_state.messages.append({"role": "assistant", "content": f"⚠️ Geração de imagem bloqueada pelo Gemini: {response.prompt_feedback.block_reason}"})
            return None

        if not response.candidates:
            print("[DEBUG] Nenhuma candidate encontrada na resposta do Gemini.")
            st.session_state.messages.append({"role": "assistant", "content": "❌ Erro: A IA não retornou candidatos de geração. Tente um prompt diferente."})
            return None

        # Variáveis para armazenar texto e dados da imagem
        text_feedback_from_gemini = ""
        image_data_bytes = None

        # Itera sobre todas as partes da resposta
        for part in response.candidates[0].content.parts:
            print(f"[DEBUG] Processando parte. Tipo do objeto 'part': {type(part)}")

            # Se a parte for texto
            if hasattr(part, 'text') and part.text is not None:
                text_feedback_from_gemini += part.text + "\n"
                print(f"[DEBUG] Conteúdo de texto recebido: '{part.text}'")

            # Se a parte for dados inline (imagem)
            if hasattr(part, 'inline_data') and part.inline_data is not None:
                print(f"[DEBUG] Dados inline (imagem) detectados. Tipo MIME: {part.inline_data.mime_type}")
                if part.inline_data.data:
                    image_data_bytes = part.inline_data.data
                    print(f"[DEBUG] Tamanho dos dados da imagem: {len(image_data_bytes)} bytes.")

                    # --- NOVO: Salve os bytes brutos para inspeção ---
                    # Isso ajudará a verificar se os bytes são realmente de uma imagem.
                    # Eles serão salvos no mesmo diretório do script.
                    with open("temp_gemini_image_debug.bin", "wb") as f:
                        f.write(image_data_bytes)
                    print("[DEBUG] Dados brutos da imagem salvos em 'temp_gemini_image_debug.bin' para análise.")

                    # Interrompa o loop assim que encontrar os dados da imagem (assumindo que só há uma)
                    break
                else:
                    print("[DEBUG] part.inline_data.data é None ou vazio, apesar de inline_data existir.")
            else:
                print("[DEBUG] Parte sem texto ou dados inline relevantes.")

        # Tentar abrir a imagem se os bytes foram encontrados
        if image_data_bytes:
            try:
                image = Image.open(io.BytesIO(image_data_bytes))
                st.session_state.generated_image = image

                # Adiciona a imagem e o texto (se houver) ao histórico de mensagens
                content_for_chat = f"Imagem gerada com o prompt: {prompt}"
                if text_feedback_from_gemini.strip():
                    content_for_chat = f"{text_feedback_from_gemini.strip()}\n\n" + content_for_chat

                st.session_state.messages.append({"role": "assistant", "image": image, "content": content_for_chat})

                st.session_state.generated_image = None # Limpa para não exibir em cima
                return image # Retorna a imagem e encerra a função
            except Exception as img_err:
                print(f"[DEBUG] ERRO da Pillow ao abrir imagem: {img_err}. Provavelmente dados de imagem corrompidos/inválidos.")
                st.session_state.messages.append({"role": "assistant", "content": f"❌ Erro interno ao processar a imagem gerada: {img_err}. A imagem recebida pode estar corrompida."})
                return None
        else:
            # Se não encontrou dados de imagem
            msg = "O Gemini não retornou uma imagem válida para o prompt fornecido."
            if text_feedback_from_gemini.strip():
                msg += f" Feedback da IA: {text_feedback_from_gemini.strip()}"
            st.session_state.messages.append({"role": "assistant", "content": f"⚠️ {msg}"})
            print(f"[DEBUG] {msg}")
            return None

    except Exception as e:
        print(f"[DEBUG] Exceção principal capturada em generate_image: {e}") # Log mais detalhado no terminal
        st.session_state.messages.append({"role": "assistant", "content": f"❌ Erro ao gerar imagem: {e}"})
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            # Não use st.rerun() aqui diretamente após a chamada da função
            # A função generate_image agora adiciona mensagens ao histórico
            generate_image(image_prompt)
            st.rerun() # Force um rerun para atualizar o chat com a nova mensagem/imagem
        else:
            # Mensagem de aviso para o usuário no chat, não na sidebar
            st.session_state.messages.append({"role": "assistant", "content": "⚠️ Por favor, digite um prompt para gerar a imagem."})
            st.rerun()

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.sidebar.success("Imagem colada!") # Este pode ficar na sidebar
                st.rerun()
            else:
                st.sidebar.warning("Nada encontrado na área de transferência.") # Este pode ficar na sidebar

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.sidebar.success("Imagem carregada!") # Este pode ficar na sidebar

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)
```

**Principais alterações na função `generate_image`:**

1.  Todas as chamadas `st.info()`, `st.warning()`, `st.error()` que estavam dentro da função foram substituídas por `print()` statements (que aparecerão no seu terminal) ou foram modificadas para **adicionar mensagens ao `st.session_state.messages`**. Isso garante que a UI do Streamlit não quebre.
2.  A lógica de adicionar a imagem e o texto de feedback (se houver) ao `st.session_state.messages` foi consolidada e ocorre apenas **uma vez**, se a imagem for gerada com sucesso ou se houver um feedback relevante do Gemini.
3.  Quando `generate_image` é chamado pelo botão "Gerar Imagem" na sidebar, um `st.rerun()` é chamado *após* a função, para garantir que as novas mensagens (com ou sem imagem) adicionadas ao `st.session_state.messages` sejam exibidas. O `st.session_state.generated_image = None` após adicionar ao `messages` também ajuda a evitar a exibição duplicada se houvesse um `st.image` direto na página principal usando essa variável.

Com essas alterações, a "tela preta" deve desaparecer, e os erros ou feedbacks da geração de imagem aparecerão no chat ou no seu terminal, permitindo continuar a depuração do `cannot identify image file`.

Lembre-se de salvar o arquivo e rodar `streamlit run chat_streamlit.py` novamente. Verifique o terminal para os `[DEBUG]` logs e, se o arquivo `temp_gemini_image_debug.bin` for criado, tente inspecioná-lo para ver o conteúdo.
2025-06-28 14:17:20,630 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:17:20,631 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:17:20,632 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:17:20,634 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:17:20,635 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:17:20,636 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:17:20,639 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:17:20,641 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:17:20,642 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:17:20,643 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:17:20,645 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:17:20,646 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:17:20,647 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:17:20,648 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:17:20,649 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:28:41,854 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:28:41,856 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:28:41,857 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:28:41,858 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:28:41,859 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:28:41,860 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:28:41,861 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:28:41,862 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:28:41,864 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:28:41,865 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:28:41,867 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:28:41,869 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:28:41,870 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:28:41,871 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:28:41,872 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:28:43,705 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 14:29:12,675 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:29:12,680 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:29:12,681 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:29:12,682 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:29:12,684 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:29:12,693 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:29:12,697 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:29:12,795 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:29:12,841 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:29:12,885 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:29:12,903 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:29:12,929 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:29:12,962 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:29:12,962 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:29:12,963 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:29:12,964 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:29:12,965 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:29:12,965 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:29:12,966 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:29:12,968 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:29:12,968 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:29:13,109 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:29:13,110 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:29:13,111 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:29:13,112 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:29:13,113 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:29:13,113 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:29:13,114 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:29:13,115 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:29:13,116 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:29:14,300 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:29:17,964 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 14:29:19,043 [INFO] AFC remote call 1 is done.
2025-06-28 14:39:50,249 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:39:50,261 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:39:50,266 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:39:50,269 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:39:50,273 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:39:50,274 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:39:50,275 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:39:50,285 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:39:50,326 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:39:50,332 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:39:50,376 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:39:50,404 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:39:50,407 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:39:50,410 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:39:50,418 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:39:51,690 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 14:40:07,572 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:40:07,579 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:40:07,624 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:40:07,656 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:40:07,720 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:40:07,727 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:40:07,742 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:40:07,786 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:40:07,789 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:40:07,790 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:40:07,791 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:40:07,792 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:40:07,793 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:40:07,797 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:40:07,872 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:40:07,916 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:40:07,923 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:40:07,933 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:40:07,946 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:40:07,951 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:40:07,952 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:40:07,973 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:40:08,071 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:40:08,122 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:40:08,138 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:40:08,577 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:40:08,685 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:40:08,726 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:40:08,774 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:40:08,835 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:40:12,021 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:40:15,750 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 14:40:16,789 [INFO] AFC remote call 1 is done.
2025-06-28 14:40:17,147 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:40:17,168 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:40:17,187 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:40:17,238 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:40:17,265 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:40:17,318 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:40:17,382 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:40:17,459 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:40:17,521 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:40:17,533 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:40:17,616 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:40:17,678 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:40:17,699 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:40:17,761 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:40:17,783 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:44:12,870 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:44:12,895 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:44:12,929 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:44:12,967 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:44:12,976 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:44:12,984 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:44:12,992 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:44:12,995 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:44:12,997 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:44:13,000 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:44:13,002 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:44:13,009 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:44:13,084 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:44:13,132 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:44:13,134 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:44:13,144 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:44:13,146 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:44:13,146 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:44:13,147 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:44:13,148 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:44:13,149 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:44:13,149 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:44:13,150 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:44:13,151 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:44:13,152 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:44:13,274 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:44:13,276 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:44:13,277 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:44:13,277 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:44:13,278 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:44:15,204 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:44:17,924 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 14:44:18,917 [INFO] AFC remote call 1 is done.
2025-06-28 14:44:19,039 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:44:19,040 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:44:19,041 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:44:19,043 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:44:19,044 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:44:19,046 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:44:19,048 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:44:19,051 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:44:19,054 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:44:19,056 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:44:19,057 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:44:19,067 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:44:19,071 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:44:19,097 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:44:19,106 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:44:53,192 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:44:53,194 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:44:53,195 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:44:53,197 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:44:53,199 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:44:53,202 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:44:53,204 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:44:53,207 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:44:53,209 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:44:53,210 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:44:53,212 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:44:53,222 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:44:53,224 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:44:53,225 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:44:53,228 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:44:53,451 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:44:53,456 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:44:53,457 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:44:53,458 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:44:53,459 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:44:53,460 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:44:53,461 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:44:53,463 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:44:53,465 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:44:53,467 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:44:53,469 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:44:53,471 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:44:53,472 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:44:53,474 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:44:53,480 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:44:54,969 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:44:57,697 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 14:44:58,718 [INFO] AFC remote call 1 is done.
2025-06-28 14:44:58,825 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:44:58,827 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:44:58,827 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:44:58,828 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:44:58,829 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:44:58,830 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:44:58,832 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:44:58,833 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:44:58,834 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:44:58,836 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:44:58,838 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:44:58,840 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:44:58,841 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:44:58,842 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:44:58,843 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:45:19,220 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:45:19,224 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:45:19,225 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:45:19,226 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:45:19,227 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:45:19,230 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:45:19,233 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:45:19,236 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:45:19,238 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:45:19,241 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:45:19,247 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:45:19,250 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:45:19,253 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:45:19,298 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:45:19,338 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:45:19,714 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:45:19,716 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:45:19,717 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:45:19,718 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:45:19,719 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:45:19,720 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:45:19,724 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:45:19,725 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:45:19,727 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:45:19,728 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:45:19,729 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:45:19,732 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:45:19,735 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:45:19,738 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:45:19,739 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:45:21,333 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 14:45:26,440 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 14:45:27,483 [INFO] AFC remote call 1 is done.
2025-06-28 14:45:27,581 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:45:27,585 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:45:27,586 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:45:27,587 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:45:27,589 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:45:27,591 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:45:27,594 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:45:27,595 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:45:27,596 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:45:27,598 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:45:27,600 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:45:27,602 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:45:27,607 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:45:27,610 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:45:27,612 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:45:43,144 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:45:43,241 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:45:43,308 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:45:43,327 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:45:43,333 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:45:43,378 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:45:43,396 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:45:43,398 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:45:43,415 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:45:43,425 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:45:43,431 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:45:43,439 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:45:43,442 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:45:43,444 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:45:43,448 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:46:36,054 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:46:36,057 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:46:36,059 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:46:36,060 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:46:36,063 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:46:36,070 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:46:36,075 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:46:36,078 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:46:36,080 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:46:36,088 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:46:36,095 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:46:36,120 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:46:36,129 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:46:36,137 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:46:36,145 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:46:36,515 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:46:36,520 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:46:36,525 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:46:36,539 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:46:36,556 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:46:36,569 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:46:36,577 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:46:36,595 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:46:36,606 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:46:36,645 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:46:36,658 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:46:36,677 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:46:36,692 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:46:36,696 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:46:36,725 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:46:57,239 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:46:57,241 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:46:57,242 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:46:57,243 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:46:57,244 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:46:57,245 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:46:57,250 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:46:57,252 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:46:57,254 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:46:57,267 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:46:57,274 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:46:57,275 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:46:57,277 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:46:57,283 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:46:57,285 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:46:57,621 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:46:57,622 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:46:57,624 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:46:57,625 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:46:57,627 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:46:57,628 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:46:57,630 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:46:57,631 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:46:57,632 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:46:57,634 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:46:57,636 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:46:57,637 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:46:57,639 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:46:57,646 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:46:57,651 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 14:46:57,720 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628144657_clipboard_20250628144636.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

Assistant: Imagem gerada com o prompt: gerar imagem de um gatinho surfista comendo lasanha
Assistant: Imagem gerada com o prompt: tubaragerar imagem de tubarão com Nike no pé 
Assistant: Imagem gerada com o prompt: gerar imagem de um tubarão com um tenis nike na barbatana
Assistant: Imagem gerada com o prompt: gerar imagem de um tubarão com um tenis nike nas nadadeiras
User: me ajude a melhorar essa história do print.
2025-06-28 14:47:25,738 [INFO] Resposta da IA (imagem): Olá! A sua história já tem um ótimo começo, com um mistério intrigante, tensão crescente e personagens com reações bem definidas. O final é um excelente gancho.

Vamos a algumas sugestões para aprimorar ainda mais, focando em completar as frases cortadas e adicionar um pouco mais de detalhe e impacto:

---

**Versão Original (com as suas frases cortadas marcadas):**

Eu leio. Se vocês quiserem.
Enzo se aproximou, ficando ao lado dela. Stella e Ísis sentaram no sofá, Isaac no chão, e Evelyn... ficou encostada no batente da porta, **o...**
A primeira página era de uma menina chamada Clara, datada de 1997.
“Hoje ele disse que me amava. Mas era segredo. Tudo era segredo. Eu, ele, a caixa, a sala proibida...”
Os olhares se cruzaram.
“A gente achava que era só um jogo. Mas depois que achamos a caixa, as brigas começaram. Gritos. Acusações. Ciúmes. O grupo se partiu... **e...**”
— Isso... é real? — sussurrou Ísis.
“A casa não guarda segredos. Ela mostra. Amplifica. Tudo que a gente esconde, ela transforma em ruído, em sombra, em perda.”
Stella se arrepiou. Evelyn pareceu empalidecer. Giovanna fechou o diário, a voz trêmula.
— Tem mais de cem páginas...
Isaac se levantou devagar.
— A gente não pode deixar isso acontecer com a gente.
— Mas já tá acontecendo — respondeu Evelyn, num fio de voz.
Um silêncio tomou conta da sala. E o medo não era mais do que tinha acontecido com aquelas crianças. Era o medo de que aquilo já estivesse **...**
Capítulo 14 – No Escuro, a Verdade Aparece
Naquela madrugada, uma queda de energia deixou a casa às escuras.

---

**Sugestões de Melhoria (com explicações):**

1.  **Completar as frases cortadas:**

    *   "...Evelyn... ficou encostada no batente da porta, **o olhar fixo no vazio.**" (Adiciona um detalhe visual à postura dela, que se encaixa com o empalidecimento posterior).
    *   "...O grupo se partiu... **e nunca mais foi o mesmo.**" (Um final mais impactante para a citação, sugerindo uma ruptura permanente). Ou "e **o pesadelo começou.**"
    *   "...Era o medo de que aquilo já estivesse **acontecendo com eles, se infiltrando nas suas próprias vidas.**" (Torna o medo mais pessoal e imediato para os personagens presentes).

2.  **Adicionar pequenos detalhes e intensificar as reações:**

    *   **Início:** Reforçar quem está lendo e a atmosfera.
    *   **Reações:** Aprofundar as reações dos personagens ao que é lido.

---

**Versão Aprimorada (com as sugestões incorporadas):**

"Eu leio. Se vocês quiserem." Giovanna ofereceu, o velho diário de capa gasta em suas mãos.
Enzo se aproximou, ficando ao lado dela. Stella e Ísis se acomodaram no sofá, Isaac no chão, e Evelyn, com um pressentimento que a gelava, ficou encostada no batente da porta, **o olhar fixo no vazio.**
A primeira página, amarelada pelo tempo, era de uma menina chamada Clara, datada de 1997. Giovanna respirou fundo e começou a ler:
“Hoje ele disse que me amava. Mas era segredo. Tudo era segredo. Eu, ele, a caixa, a sala proibida...”
Os olhares do grupo se cruzaram, uma mistura de fascínio e crescente apreensão.
Giovanna continuou, a voz um pouco mais baixa: “A gente achava que era só um jogo. Mas depois que achamos a caixa, as brigas começaram. Gritos. Acusações. Ciúmes. O grupo se partiu... **e nunca mais foi o mesmo.**”
— Isso... é real? — sussurrou Ísis, os olhos arregalados para o diário.
Giovanna engoliu em seco antes de ler a próxima linha, quase como um eco sombrio: “A casa não guarda segredos. Ela mostra. Amplifica. Tudo que a gente esconde, ela transforma em ruído, em sombra, em perda.”
Stella se arrepiou visivelmente. Evelyn pareceu empalidecer ainda mais, e a mão que segurava a porta apertou a madeira. Giovanna fechou o diário com um baque surdo, a voz trêmula mal saindo.
— Tem mais de cem páginas... — ela murmurou, como se o peso daquelas palavras a estivesse esmagando.
Isaac se levantou devagar, seus punhos cerrados, o rosto sério.
— A gente não pode deixar isso acontecer com a gente — ele declarou, a voz mais firme do que parecia.
— Mas já tá acontecendo — respondeu Evelyn, num fio de voz que mal era audível.
Um silêncio pesado tomou conta da sala. E o medo não era mais sobre o que tinha acontecido com aquelas crianças no passado. Era o medo paralisante de que aquilo já estivesse **acontecendo com eles, se infiltrando nas suas próprias vidas.**

Capítulo 14 – No Escuro, a Verdade Aparece
Naquela madrugada, uma queda de energia deixou a casa às escuras.

---

**Pontos fortes que você já tem e pode explorar:**

*   **O Diário como Revelador:** É um ótimo dispositivo para desvendar o mistério.
*   **A "Casa":** A casa é quase um personagem por si só, com a frase "A casa não guarda segredos..." Isso é excelente e pode ser ainda mais explorado.
*   **Tensão Crecente:** Você constrói bem a tensão através das reações dos personagens e das revelações do diário.
*   **Mistério da "Caixa" e da "Sala Proibida":** Esses elementos são instigantes e dão margem para futuras explorações.

Espero que essas sugestões ajudem a tornar sua história ainda mais envolvente!
2025-06-28 14:47:25,889 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 14:47:25,890 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 14:47:25,892 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 14:47:25,893 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 14:47:25,894 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 14:47:25,895 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 14:47:25,897 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 14:47:25,899 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 14:47:25,904 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 14:47:25,908 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 14:47:25,909 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 14:47:25,910 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 14:47:25,912 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 14:47:25,913 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 14:47:25,916 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 15:59:44,779 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 15:59:44,781 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 15:59:44,785 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 15:59:44,789 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 15:59:44,790 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 15:59:44,791 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 15:59:44,792 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 15:59:44,795 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 15:59:44,796 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 15:59:44,803 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 15:59:44,809 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 15:59:44,813 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 15:59:44,818 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 15:59:44,827 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 15:59:44,837 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 15:59:45,362 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 15:59:45,363 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 15:59:45,364 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 15:59:45,366 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 15:59:45,367 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 15:59:45,369 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 15:59:45,372 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 15:59:45,374 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 15:59:45,376 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 15:59:45,381 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 15:59:45,391 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 15:59:45,394 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 15:59:45,396 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 15:59:45,397 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 15:59:45,398 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 15:59:45,472 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628155945_clipboard_20250628144636.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

Assistant: Imagem gerada com o prompt: gerar imagem de um gatinho surfista comendo lasanha
Assistant: Imagem gerada com o prompt: tubaragerar imagem de tubarão com Nike no pé 
Assistant: Imagem gerada com o prompt: gerar imagem de um tubarão com um tenis nike na barbatana
Assistant: Imagem gerada com o prompt: gerar imagem de um tubarão com um tenis nike nas nadadeiras
User: me ajude a melhorar essa história do print.
Assistant: Olá! A sua história já tem um ótimo começo, com um mistério intrigante, tensão crescente e personagens com reações bem definidas. O final é um excelente gancho.

Vamos a algumas sugestões para aprimorar ainda mais, focando em completar as frases cortadas e adicionar um pouco mais de detalhe e impacto:

---

**Versão Original (com as suas frases cortadas marcadas):**

Eu leio. Se vocês quiserem.
Enzo se aproximou, ficando ao lado dela. Stella e Ísis sentaram no sofá, Isaac no chão, e Evelyn... ficou encostada no batente da porta, **o...**
A primeira página era de uma menina chamada Clara, datada de 1997.
“Hoje ele disse que me amava. Mas era segredo. Tudo era segredo. Eu, ele, a caixa, a sala proibida...”
Os olhares se cruzaram.
“A gente achava que era só um jogo. Mas depois que achamos a caixa, as brigas começaram. Gritos. Acusações. Ciúmes. O grupo se partiu... **e...**”
— Isso... é real? — sussurrou Ísis.
“A casa não guarda segredos. Ela mostra. Amplifica. Tudo que a gente esconde, ela transforma em ruído, em sombra, em perda.”
Stella se arrepiou. Evelyn pareceu empalidecer. Giovanna fechou o diário, a voz trêmula.
— Tem mais de cem páginas...
Isaac se levantou devagar.
— A gente não pode deixar isso acontecer com a gente.
— Mas já tá acontecendo — respondeu Evelyn, num fio de voz.
Um silêncio tomou conta da sala. E o medo não era mais do que tinha acontecido com aquelas crianças. Era o medo de que aquilo já estivesse **...**
Capítulo 14 – No Escuro, a Verdade Aparece
Naquela madrugada, uma queda de energia deixou a casa às escuras.

---

**Sugestões de Melhoria (com explicações):**

1.  **Completar as frases cortadas:**

    *   "...Evelyn... ficou encostada no batente da porta, **o olhar fixo no vazio.**" (Adiciona um detalhe visual à postura dela, que se encaixa com o empalidecimento posterior).
    *   "...O grupo se partiu... **e nunca mais foi o mesmo.**" (Um final mais impactante para a citação, sugerindo uma ruptura permanente). Ou "e **o pesadelo começou.**"
    *   "...Era o medo de que aquilo já estivesse **acontecendo com eles, se infiltrando nas suas próprias vidas.**" (Torna o medo mais pessoal e imediato para os personagens presentes).

2.  **Adicionar pequenos detalhes e intensificar as reações:**

    *   **Início:** Reforçar quem está lendo e a atmosfera.
    *   **Reações:** Aprofundar as reações dos personagens ao que é lido.

---

**Versão Aprimorada (com as sugestões incorporadas):**

"Eu leio. Se vocês quiserem." Giovanna ofereceu, o velho diário de capa gasta em suas mãos.
Enzo se aproximou, ficando ao lado dela. Stella e Ísis se acomodaram no sofá, Isaac no chão, e Evelyn, com um pressentimento que a gelava, ficou encostada no batente da porta, **o olhar fixo no vazio.**
A primeira página, amarelada pelo tempo, era de uma menina chamada Clara, datada de 1997. Giovanna respirou fundo e começou a ler:
“Hoje ele disse que me amava. Mas era segredo. Tudo era segredo. Eu, ele, a caixa, a sala proibida...”
Os olhares do grupo se cruzaram, uma mistura de fascínio e crescente apreensão.
Giovanna continuou, a voz um pouco mais baixa: “A gente achava que era só um jogo. Mas depois que achamos a caixa, as brigas começaram. Gritos. Acusações. Ciúmes. O grupo se partiu... **e nunca mais foi o mesmo.**”
— Isso... é real? — sussurrou Ísis, os olhos arregalados para o diário.
Giovanna engoliu em seco antes de ler a próxima linha, quase como um eco sombrio: “A casa não guarda segredos. Ela mostra. Amplifica. Tudo que a gente esconde, ela transforma em ruído, em sombra, em perda.”
Stella se arrepiou visivelmente. Evelyn pareceu empalidecer ainda mais, e a mão que segurava a porta apertou a madeira. Giovanna fechou o diário com um baque surdo, a voz trêmula mal saindo.
— Tem mais de cem páginas... — ela murmurou, como se o peso daquelas palavras a estivesse esmagando.
Isaac se levantou devagar, seus punhos cerrados, o rosto sério.
— A gente não pode deixar isso acontecer com a gente — ele declarou, a voz mais firme do que parecia.
— Mas já tá acontecendo — respondeu Evelyn, num fio de voz que mal era audível.
Um silêncio pesado tomou conta da sala. E o medo não era mais sobre o que tinha acontecido com aquelas crianças no passado. Era o medo paralisante de que aquilo já estivesse **acontecendo com eles, se infiltrando nas suas próprias vidas.**

Capítulo 14 – No Escuro, a Verdade Aparece
Naquela madrugada, uma queda de energia deixou a casa às escuras.

---

**Pontos fortes que você já tem e pode explorar:**

*   **O Diário como Revelador:** É um ótimo dispositivo para desvendar o mistério.
*   **A "Casa":** A casa é quase um personagem por si só, com a frase "A casa não guarda segredos..." Isso é excelente e pode ser ainda mais explorado.
*   **Tensão Crecente:** Você constrói bem a tensão através das reações dos personagens e das revelações do diário.
*   **Mistério da "Caixa" e da "Sala Proibida":** Esses elementos são instigantes e dão margem para futuras explorações.

Espero que essas sugestões ajudem a tornar sua história ainda mais envolvente!
User: Em que lugar dessa aplicação eu defino os modelos de IA? 

gemini-2.0-flash-exp

esse por exemplo
2025-06-28 16:00:03,846 [INFO] Resposta da IA (imagem): Você define os modelos de IA em **vários lugares**, dependendo da funcionalidade específica que você quer que o modelo realize.

A classe `GenerativeModelHandler` (definida em `services/gpt_services.py`) é a responsável por *inicializar* o modelo `genai.GenerativeModel` com o nome do modelo que lhe é passado. No entanto, o **nome exato do modelo (a string como "gemini-2.5-flash" ou "gemini-2.0-flash-exp-image-generation") é definido onde essa classe é instanciada ou onde a função de geração de conteúdo é chamada diretamente.**

Vamos detalhar:

1.  **Para o chat principal (interação de texto e análise de imagem carregada):**
    *   No arquivo `chat_streamlit.py`, dentro da função `@st.cache_resource def get_gemini_handler():`, você define o modelo para a interação geral do chat:
        ```python
        # chat_streamlit.py
        @st.cache_resource
        def get_gemini_handler():
            return GeminiHandler("gemini-2.5-flash") # <--- AQUI
        ```
        Este `GeminiHandler` por sua vez usa `GenerativeModelHandler("gemini-2.5-flash")`.

2.  **Para a geração de imagens (via prompt de texto no chat UI):**
    *   No arquivo `chat_streamlit.py`, dentro da função `generate_image`:
        ```python
        # chat_streamlit.py
        def generate_image(prompt):
            # ...
            client = genai.Client(api_key=api_key)
            try:
                response = client.models.generate_content(
                    model='gemini-2.0-flash-exp-image-generation', # <--- AQUI
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_modalities=['Text', 'Image']
                    )
                )
        ```

3.  **Para o processamento de imagens em lote (no `main.py`):**
    *   No arquivo `services/image_processor.py`, no `__init__` da classe `ImageProcessor`:
        ```python
        # services/image_processor.py
        class ImageProcessor:
            def __init__(self, rate_limiter: RateLimiter):
                self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash") # <--- AQUI
                # ...
        ```
        Este é o modelo usado para gerar os resumos das imagens na execução via `main.py`.

4.  **No serviço de imagem autônomo (não diretamente usado no chat, mas existe):**
    *   No arquivo `services/image_services.py`, na função `generate_image`:
        ```python
        # services/image_services.py
        def generate_image(prompt: str) -> Image.Image | None:
            try:
                model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation') # <--- AQUI
                # ...
        ```
        Note que a função `generate_image` em `chat_streamlit.py` chama diretamente a API do `genai`, e não esta função `services/image_services.py`. No entanto, esta é outra instância onde um modelo de geração de imagem é especificado.

**Em resumo:**

*   **`gemini-2.5-flash`** é usado para o chat interativo no Streamlit (`chat_streamlit.py`) e para o processamento em lote de imagens (`services/image_processor.py`).
*   **`gemini-2.0-flash-exp-image-generation`** é usado especificamente para a funcionalidade de "Gerar Imagem" dentro do Streamlit (`chat_streamlit.py`) e também existe como um serviço autônomo (`services/image_services.py`).
2025-06-28 16:00:03,988 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:00:03,989 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:00:03,990 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:00:03,991 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:00:03,992 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:00:03,993 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:00:03,994 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:00:03,996 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:00:03,997 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:00:04,003 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:00:04,005 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:00:04,007 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:00:04,010 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:00:04,013 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:00:04,015 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:00:13,328 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:00:13,332 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:00:13,333 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:00:13,335 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:00:13,340 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:00:13,353 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:00:13,357 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:00:13,360 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:00:13,362 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:00:13,364 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:00:13,365 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:00:13,367 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:00:13,384 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:00:13,387 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:00:13,390 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:00:13,620 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:00:13,626 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:00:13,628 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:00:13,633 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:00:13,648 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:00:13,650 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:00:13,651 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:00:13,652 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:00:13,653 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:00:13,662 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:00:13,668 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:00:13,684 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:00:13,686 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:00:13,687 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:00:13,700 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:02:10,145 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:02:10,147 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:02:10,148 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:02:10,149 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:02:10,150 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:02:10,151 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:02:10,153 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:02:10,166 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:02:10,169 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:02:10,171 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:02:10,182 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:02:10,185 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:02:10,186 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:02:10,187 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:02:10,188 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:02:10,669 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:02:10,670 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:02:10,671 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:02:10,673 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:02:10,675 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:02:10,676 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:02:10,678 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:02:10,680 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:02:10,686 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:02:10,687 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:02:10,690 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:02:10,691 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:02:10,695 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:02:10,697 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:02:10,699 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:02:10,702 [INFO] Enviando para IA - Prompt (sem imagem): Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Função para gerar imagem com Gemini
def generate_image(prompt):
    # Verifica se a chave da API foi carregada corretamente
    api_key = os.getenv("API_KEY_GEMINI")

    if not api_key:
        raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

    client = genai.Client(api_key=api_key)

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp-image-generation',
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['Text', 'Image']
            )
        )

        for part in response.candidates[0].content.parts:
            if part.text is not None:
                print(part.text)
            elif part.inline_data is not None:
                image = Image.open(io.BytesIO(part.inline_data.data))
                st.session_state.generated_image = image
                return image

    except Exception as e:
        st.error(f"Erro ao gerar imagem: {e}")
        return None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = generate_image(image_prompt)

            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

import os
from dotenv import load_dotenv
from google import genai
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# Inicializa o Gemini
genai.configure(api_key=api_key)

def generate_image(prompt: str) -> Image.Image | None:
    """
    Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

    Args:
        prompt (str): O prompt de texto para gerar a imagem.

    Returns:
        Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
    """
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation')
        response = model.generate_content(prompt)
        if response.prompt_feedback:
            print('Reason: {}'.format(response.prompt_feedback.block_reason))
        # Verifique se a resposta contém dados de imagem
        if response.parts:
            for part in response.parts:
                if part.mime_type == 'image/png':
                    return Image.open(BytesIO(part.data))
        print(response.text)
        return None
    except Exception as e:
        print(f"Erro ao gerar imagem: {e}")
        return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

Assistant: Imagem gerada com o prompt: gerar imagem de um gatinho surfista comendo lasanha
Assistant: Imagem gerada com o prompt: tubaragerar imagem de tubarão com Nike no pé 
Assistant: Imagem gerada com o prompt: gerar imagem de um tubarão com um tenis nike na barbatana
Assistant: Imagem gerada com o prompt: gerar imagem de um tubarão com um tenis nike nas nadadeiras
User: me ajude a melhorar essa história do print.
Assistant: Olá! A sua história já tem um ótimo começo, com um mistério intrigante, tensão crescente e personagens com reações bem definidas. O final é um excelente gancho.

Vamos a algumas sugestões para aprimorar ainda mais, focando em completar as frases cortadas e adicionar um pouco mais de detalhe e impacto:

---

**Versão Original (com as suas frases cortadas marcadas):**

Eu leio. Se vocês quiserem.
Enzo se aproximou, ficando ao lado dela. Stella e Ísis sentaram no sofá, Isaac no chão, e Evelyn... ficou encostada no batente da porta, **o...**
A primeira página era de uma menina chamada Clara, datada de 1997.
“Hoje ele disse que me amava. Mas era segredo. Tudo era segredo. Eu, ele, a caixa, a sala proibida...”
Os olhares se cruzaram.
“A gente achava que era só um jogo. Mas depois que achamos a caixa, as brigas começaram. Gritos. Acusações. Ciúmes. O grupo se partiu... **e...**”
— Isso... é real? — sussurrou Ísis.
“A casa não guarda segredos. Ela mostra. Amplifica. Tudo que a gente esconde, ela transforma em ruído, em sombra, em perda.”
Stella se arrepiou. Evelyn pareceu empalidecer. Giovanna fechou o diário, a voz trêmula.
— Tem mais de cem páginas...
Isaac se levantou devagar.
— A gente não pode deixar isso acontecer com a gente.
— Mas já tá acontecendo — respondeu Evelyn, num fio de voz.
Um silêncio tomou conta da sala. E o medo não era mais do que tinha acontecido com aquelas crianças. Era o medo de que aquilo já estivesse **...**
Capítulo 14 – No Escuro, a Verdade Aparece
Naquela madrugada, uma queda de energia deixou a casa às escuras.

---

**Sugestões de Melhoria (com explicações):**

1.  **Completar as frases cortadas:**

    *   "...Evelyn... ficou encostada no batente da porta, **o olhar fixo no vazio.**" (Adiciona um detalhe visual à postura dela, que se encaixa com o empalidecimento posterior).
    *   "...O grupo se partiu... **e nunca mais foi o mesmo.**" (Um final mais impactante para a citação, sugerindo uma ruptura permanente). Ou "e **o pesadelo começou.**"
    *   "...Era o medo de que aquilo já estivesse **acontecendo com eles, se infiltrando nas suas próprias vidas.**" (Torna o medo mais pessoal e imediato para os personagens presentes).

2.  **Adicionar pequenos detalhes e intensificar as reações:**

    *   **Início:** Reforçar quem está lendo e a atmosfera.
    *   **Reações:** Aprofundar as reações dos personagens ao que é lido.

---

**Versão Aprimorada (com as sugestões incorporadas):**

"Eu leio. Se vocês quiserem." Giovanna ofereceu, o velho diário de capa gasta em suas mãos.
Enzo se aproximou, ficando ao lado dela. Stella e Ísis se acomodaram no sofá, Isaac no chão, e Evelyn, com um pressentimento que a gelava, ficou encostada no batente da porta, **o olhar fixo no vazio.**
A primeira página, amarelada pelo tempo, era de uma menina chamada Clara, datada de 1997. Giovanna respirou fundo e começou a ler:
“Hoje ele disse que me amava. Mas era segredo. Tudo era segredo. Eu, ele, a caixa, a sala proibida...”
Os olhares do grupo se cruzaram, uma mistura de fascínio e crescente apreensão.
Giovanna continuou, a voz um pouco mais baixa: “A gente achava que era só um jogo. Mas depois que achamos a caixa, as brigas começaram. Gritos. Acusações. Ciúmes. O grupo se partiu... **e nunca mais foi o mesmo.**”
— Isso... é real? — sussurrou Ísis, os olhos arregalados para o diário.
Giovanna engoliu em seco antes de ler a próxima linha, quase como um eco sombrio: “A casa não guarda segredos. Ela mostra. Amplifica. Tudo que a gente esconde, ela transforma em ruído, em sombra, em perda.”
Stella se arrepiou visivelmente. Evelyn pareceu empalidecer ainda mais, e a mão que segurava a porta apertou a madeira. Giovanna fechou o diário com um baque surdo, a voz trêmula mal saindo.
— Tem mais de cem páginas... — ela murmurou, como se o peso daquelas palavras a estivesse esmagando.
Isaac se levantou devagar, seus punhos cerrados, o rosto sério.
— A gente não pode deixar isso acontecer com a gente — ele declarou, a voz mais firme do que parecia.
— Mas já tá acontecendo — respondeu Evelyn, num fio de voz que mal era audível.
Um silêncio pesado tomou conta da sala. E o medo não era mais sobre o que tinha acontecido com aquelas crianças no passado. Era o medo paralisante de que aquilo já estivesse **acontecendo com eles, se infiltrando nas suas próprias vidas.**

Capítulo 14 – No Escuro, a Verdade Aparece
Naquela madrugada, uma queda de energia deixou a casa às escuras.

---

**Pontos fortes que você já tem e pode explorar:**

*   **O Diário como Revelador:** É um ótimo dispositivo para desvendar o mistério.
*   **A "Casa":** A casa é quase um personagem por si só, com a frase "A casa não guarda segredos..." Isso é excelente e pode ser ainda mais explorado.
*   **Tensão Crecente:** Você constrói bem a tensão através das reações dos personagens e das revelações do diário.
*   **Mistério da "Caixa" e da "Sala Proibida":** Esses elementos são instigantes e dão margem para futuras explorações.

Espero que essas sugestões ajudem a tornar sua história ainda mais envolvente!
User: Em que lugar dessa aplicação eu defino os modelos de IA? 

gemini-2.0-flash-exp

esse por exemplo
Assistant: Você define os modelos de IA em **vários lugares**, dependendo da funcionalidade específica que você quer que o modelo realize.

A classe `GenerativeModelHandler` (definida em `services/gpt_services.py`) é a responsável por *inicializar* o modelo `genai.GenerativeModel` com o nome do modelo que lhe é passado. No entanto, o **nome exato do modelo (a string como "gemini-2.5-flash" ou "gemini-2.0-flash-exp-image-generation") é definido onde essa classe é instanciada ou onde a função de geração de conteúdo é chamada diretamente.**

Vamos detalhar:

1.  **Para o chat principal (interação de texto e análise de imagem carregada):**
    *   No arquivo `chat_streamlit.py`, dentro da função `@st.cache_resource def get_gemini_handler():`, você define o modelo para a interação geral do chat:
        ```python
        # chat_streamlit.py
        @st.cache_resource
        def get_gemini_handler():
            return GeminiHandler("gemini-2.5-flash") # <--- AQUI
        ```
        Este `GeminiHandler` por sua vez usa `GenerativeModelHandler("gemini-2.5-flash")`.

2.  **Para a geração de imagens (via prompt de texto no chat UI):**
    *   No arquivo `chat_streamlit.py`, dentro da função `generate_image`:
        ```python
        # chat_streamlit.py
        def generate_image(prompt):
            # ...
            client = genai.Client(api_key=api_key)
            try:
                response = client.models.generate_content(
                    model='gemini-2.0-flash-exp-image-generation', # <--- AQUI
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_modalities=['Text', 'Image']
                    )
                )
        ```

3.  **Para o processamento de imagens em lote (no `main.py`):**
    *   No arquivo `services/image_processor.py`, no `__init__` da classe `ImageProcessor`:
        ```python
        # services/image_processor.py
        class ImageProcessor:
            def __init__(self, rate_limiter: RateLimiter):
                self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash") # <--- AQUI
                # ...
        ```
        Este é o modelo usado para gerar os resumos das imagens na execução via `main.py`.

4.  **No serviço de imagem autônomo (não diretamente usado no chat, mas existe):**
    *   No arquivo `services/image_services.py`, na função `generate_image`:
        ```python
        # services/image_services.py
        def generate_image(prompt: str) -> Image.Image | None:
            try:
                model = genai.GenerativeModel('gemini-2.0-flash-exp-image-generation') # <--- AQUI
                # ...
        ```
        Note que a função `generate_image` em `chat_streamlit.py` chama diretamente a API do `genai`, e não esta função `services/image_services.py`. No entanto, esta é outra instância onde um modelo de geração de imagem é especificado.

**Em resumo:**

*   **`gemini-2.5-flash`** é usado para o chat interativo no Streamlit (`chat_streamlit.py`) e para o processamento em lote de imagens (`services/image_processor.py`).
*   **`gemini-2.0-flash-exp-image-generation`** é usado especificamente para a funcionalidade de "Gerar Imagem" dentro do Streamlit (`chat_streamlit.py`) e também existe como um serviço autônomo (`services/image_services.py`).
User: eu tenho funções repetidas nessa aplicação que pode ser melhorada para POO ?   por exemplo 

generate_image

tenho em dois lugares essa função.
2025-06-28 16:02:46,309 [INFO] Resposta da IA (texto): Você está absolutamente certo! A duplicação da função `generate_image` é uma excelente oportunidade para aplicar os princípios de POO, especificamente o de **Reutilização de Código** e **Single Responsibility Principle (SRP)**, encapsulando a lógica de geração de imagem em uma classe de serviço dedicada.

Atualmente, você tem a lógica de geração de imagem em:
1.  `services/image_services.py`
2.  `chat_streamlit.py` (e `test_image_generation.py` também usa uma lógica similar, que pode ser consolidada).

Vamos refatorar isso criando uma nova classe de serviço que será a única responsável por interagir com a API de geração de imagens do Gemini.

### 1. Criar uma Nova Classe de Serviço para Geração de Imagens

Vamos criar um novo arquivo, por exemplo, `services/gemini_image_generator.py`, para hospedar essa lógica.

**`services/gemini_image_generator.py`**
```python
import os
import google.generativeai as genai
from google.genai import types
from PIL import Image
from io import BytesIO
from dotenv import load_dotenv
from core.logger_config import logger # Importe seu logger existente

class GeminiImageGenerator:
    _instance = None # Para implementar um singleton simples, garantindo uma única instância
    _is_initialized = False

    def __new__(cls, *args, **kwargs):
        # Implementa o padrão Singleton: garante que apenas uma instância da classe seja criada.
        if cls._instance is None:
            cls._instance = super(GeminiImageGenerator, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not self._is_initialized:
            load_dotenv()
            self.api_key = os.getenv("API_KEY_GEMINI")

            if not self.api_key:
                logger.error("API_KEY_GEMINI não encontrada nas variáveis de ambiente para geração de imagens.")
                raise ValueError("API_KEY_GEMINI não encontrada nas variáveis de ambiente.")

            # Configura a API do Gemini uma única vez
            genai.configure(api_key=self.api_key)
            self.client = genai.Client(api_key=self.api_key) # Usar genai.Client para models.generate_content
            self.model_name = 'gemini-2.0-flash-exp-image-generation'
            self._is_initialized = True
            logger.info(f"GeminiImageGenerator inicializado para o modelo {self.model_name}.")

    def generate_image(self, prompt: str) -> Image.Image | None:
        """
        Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

        Args:
            prompt (str): O prompt de texto para gerar a imagem.

        Returns:
            Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
        """
        try:
            logger.info(f"Iniciando geração de imagem para o prompt: '{prompt[:100]}...'")
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_modalities=['Text', 'Image']
                )
            )
            
            if response.prompt_feedback:
                feedback = response.prompt_feedback.block_reason
                logger.warning(f"Feedback do prompt na geração de imagem: {feedback}")
                # Dependendo do feedback, você pode querer lançar uma exceção ou retornar None
                return None

            # Percorre as partes da resposta para encontrar a imagem
            if response.candidates and response.candidates[0].content.parts:
                for part in response.candidates[0].content.parts:
                    if part.inline_data is not None and part.inline_data.mime_type.startswith('image/'):
                        image = Image.open(BytesIO(part.inline_data.data))
                        logger.info(f"Imagem gerada com sucesso para o prompt: '{prompt[:100]}...'")
                        return image
            
            logger.warning(f"Nenhuma imagem encontrada na resposta para o prompt: '{prompt[:100]}...'. Resposta completa: {response.text}")
            return None

        except Exception as e:
            logger.error(f"Erro ao gerar imagem para prompt '{prompt[:100]}...': {e}", exc_info=True)
            return None

```

### 2. Modificar `chat_streamlit.py` para Usar a Nova Classe

Agora, `chat_streamlit.py` pode importar e usar esta nova classe, removendo a lógica duplicada.

**`chat_streamlit.py`** (apenas as partes que mudam)

```python
# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python
from services.gemini_image_generator import GeminiImageGenerator # <--- NOVA IMPORTAÇÃO

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter (pode ser compartilhado ou ter seu próprio para a UI)
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# ... (restante das inicializações de session_state) ...

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

# <--- NOVA FUNÇÃO PARA INICIALIZAR GeminiImageGenerator
@st.cache_resource
def get_image_generator():
    return GeminiImageGenerator()

image_generator = get_image_generator()
# FIM DA NOVA INICIALIZAÇÃO

# ... (funções check_clipboard, reset_uploader, process_message, execute_processing) ...

# Função para gerar imagem com Gemini (AGORA USA A CLASSE DE SERVIÇO)
def generate_image(prompt):
    try:
        # Chama o método da nova classe de serviço
        generated_image = image_generator.generate_image(prompt)

        if generated_image:
            st.session_state.generated_image = generated_image
            return generated_image
        else:
            # A classe de serviço já loga o erro, aqui é para a UI
            st.error("Não foi possível gerar a imagem. Verifique o prompt ou a resposta da API.")
            return None

    except Exception as e:
        st.error(f"Erro inesperado ao gerar imagem: {e}")
        return None

# ... (restante do código do Streamlit) ...
```

### 3. Excluir ou Refatorar `services/image_services.py`

A partir de agora, o arquivo `services/image_services.py` se torna redundante, pois sua funcionalidade foi migrada para `services/gemini_image_generator.py`. Você pode:

*   **Excluí-lo completamente** se ele não for usado em mais nenhum lugar do projeto (que é o caso aparente).
*   Se houvesse outras funções utilitárias que não fossem a `generate_image`, você poderia refatorá-lo para usar a nova classe `GeminiImageGenerator` ou mover essas funções para um local mais apropriado.

### 4. Modificar `test_image_generation.py` (Opcional, mas recomendado)

Para manter a consistência, seu script de teste também deve usar a nova classe:

**`test_image_generation.py`** (apenas as partes que mudam)

```python
import os
from dotenv import load_dotenv
# from google import genai # Não precisa mais disso diretamente aqui
# from google.genai import types # Não precisa mais disso diretamente aqui
from PIL import Image
from io import BytesIO

from services.gemini_image_generator import GeminiImageGenerator # <--- NOVA IMPORTAÇÃO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Instancia o gerador de imagens
image_generator = GeminiImageGenerator() # A classe já cuida da API_KEY e configure

prompt = "gerar a imagem do simbolo do corinthians."

# Chama o método da classe de serviço
image = image_generator.generate_image(prompt)

if image:
    image.save("ed-image.png")
    image.show()
else:
    print("Falha ao gerar imagem.") # A classe já loga detalhes do erro
```

---

### Benefícios da Refatoração:

1.  **Reutilização de Código:** A lógica de como interagir com a API de geração de imagens do Gemini está agora em um único lugar (`GeminiImageGenerator`). Qualquer parte da sua aplicação que precise gerar uma imagem usará essa classe, evitando duplicação.
2.  **Single Responsibility Principle (SRP):**
    *   `GeminiImageGenerator` tem uma única responsabilidade: gerar imagens através da API do Gemini.
    *   `chat_streamlit.py` é responsável pela interface do usuário e por orquestrar as interações, não pela lógica interna de como a imagem é gerada.
3.  **Manutenibilidade:** Se a API de geração de imagens do Gemini mudar (e.g., um novo modelo, novos parâmetros, autenticação), você só precisará atualizar o código em `services/gemini_image_generator.py`.
4.  **Testabilidade:** A classe `GeminiImageGenerator` pode ser testada isoladamente, sem a necessidade de simular o ambiente do Streamlit ou outros componentes da aplicação.
5.  **Coesão:** O código relacionado à geração de imagens está agrupado de forma lógica.
6.  **Legibilidade:** O código em `chat_streamlit.py` fica mais limpo e fácil de entender, pois a complexidade da API foi abstraída.

Essa é uma excelente melhoria de arquitetura para a sua aplicação!
2025-06-28 16:02:46,541 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:02:46,543 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:02:46,544 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:02:46,545 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:02:46,547 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:02:46,549 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:02:46,562 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:02:46,564 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:02:46,566 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:02:46,568 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:02:46,570 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:02:46,572 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:02:46,579 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:02:46,581 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:02:46,583 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:19:32,810 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:19:32,812 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:19:32,813 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:19:32,819 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:19:32,821 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:19:32,828 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:19:32,834 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:19:32,853 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:19:32,859 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:19:32,862 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:19:32,870 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:19:32,873 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:19:32,875 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:19:32,876 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:19:32,877 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:19:32,878 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:19:33,944 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 16:24:15,805 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:24:15,805 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:24:15,806 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:24:15,807 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:24:15,808 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:24:15,808 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:24:15,809 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:24:15,810 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:24:15,810 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:24:15,811 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:24:15,813 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:24:15,814 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:24:15,820 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:24:15,823 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:24:15,824 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:24:15,825 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:25:22,562 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:25:22,568 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:25:22,581 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:25:22,598 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:25:22,599 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:25:22,600 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:25:22,601 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:25:22,609 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:25:22,611 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:25:22,613 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:25:22,614 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:25:22,618 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:25:22,634 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:25:22,642 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:25:22,644 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:25:22,646 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:26:08,791 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:26:08,800 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:26:08,801 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:26:08,802 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:26:08,803 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:26:08,804 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:26:08,807 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:26:08,813 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:26:08,840 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:26:08,852 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:26:08,854 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:26:08,855 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:26:08,856 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:26:08,856 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:26:08,857 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:26:08,868 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:28:50,988 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:28:50,994 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:28:50,995 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:28:50,996 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:28:50,996 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:28:50,997 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:28:50,998 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:28:51,003 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:28:51,006 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:28:51,007 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:28:51,011 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:28:51,014 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:28:51,020 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:28:51,023 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:28:51,024 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:28:51,026 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:29:12,375 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:29:12,385 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:29:12,386 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:29:12,388 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:29:12,390 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:29:12,392 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:29:12,393 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:29:12,408 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:29:12,426 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:29:12,434 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:29:12,440 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:29:12,441 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:29:12,442 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:29:12,442 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:29:12,443 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:29:12,453 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:29:30,116 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:29:30,117 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:29:30,118 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:29:30,119 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:29:30,121 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:29:30,124 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:29:30,126 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:29:30,128 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:29:30,133 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:29:30,155 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:29:30,158 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:29:30,168 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:29:30,169 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:29:30,176 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:29:30,187 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:29:30,188 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:29:30,221 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 16:30:16,012 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:30:16,013 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:30:16,014 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:30:16,018 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:30:16,019 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:30:16,020 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:30:16,030 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:30:16,039 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:30:16,041 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:30:16,042 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:30:16,043 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:30:16,044 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:30:16,044 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:30:16,047 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:30:16,061 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:30:16,063 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:30:16,672 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 16:36:44,761 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:36:44,762 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:36:44,763 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:36:44,765 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:36:44,769 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:36:44,771 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:36:44,772 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:36:44,776 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:36:44,782 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:36:44,787 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:36:44,791 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:36:44,793 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:36:44,810 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:36:44,811 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:36:44,813 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:36:44,816 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:41:27,764 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:41:27,764 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:41:27,765 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:41:27,766 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:41:27,766 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:41:27,767 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:41:27,767 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:41:27,768 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:41:27,768 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:41:27,769 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:41:27,770 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:41:27,770 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:41:27,771 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:41:27,773 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:41:27,775 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:41:27,776 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:41:27,778 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:41:27,778 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:41:27,779 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:41:27,780 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:41:27,780 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:41:27,781 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:41:27,781 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:41:27,782 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:41:27,783 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:41:27,783 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:41:27,784 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:41:27,784 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:41:27,785 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:41:27,785 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:41:27,786 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:41:27,786 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:41:27,799 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 16:41:50,871 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:41:50,880 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:41:50,881 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:41:50,889 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:41:50,892 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:41:50,896 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:41:50,911 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:41:50,914 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:41:50,915 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:41:50,929 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:41:50,930 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:41:50,931 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:41:50,939 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:41:50,948 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:41:50,955 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:41:50,964 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:41:51,189 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:41:51,192 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:41:51,193 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:41:51,196 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:41:51,197 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:41:51,209 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:41:51,211 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:41:51,212 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:41:51,219 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:41:51,225 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:41:51,228 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:41:51,241 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:41:51,245 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:41:51,246 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:41:51,250 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:41:51,260 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:41:51,295 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 16:41:56,124 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 16:41:57,271 [INFO] AFC remote call 1 is done.
2025-06-28 16:41:57,445 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:41:57,447 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:41:57,448 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:41:57,450 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:41:57,452 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:41:57,456 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:41:57,464 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:41:57,466 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:41:57,467 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:41:57,468 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:41:57,470 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:41:57,472 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:41:57,472 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:41:57,477 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:41:57,483 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:41:57,484 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:44:10,785 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:44:10,786 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:44:10,787 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:44:10,788 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:44:10,788 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:44:10,789 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:44:10,790 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:44:10,791 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:44:10,791 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:44:10,792 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:44:10,793 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:44:10,794 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:44:10,795 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:44:10,795 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:44:10,797 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:44:10,799 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:44:11,235 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 16:44:25,479 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:44:25,484 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:44:25,485 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:44:25,486 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:44:25,488 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:44:25,489 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:44:25,491 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:44:25,493 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:44:25,499 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:44:25,504 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:44:25,506 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:44:25,507 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:44:25,509 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:44:25,510 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:44:25,511 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:44:25,512 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:44:25,690 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:44:25,692 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:44:25,693 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:44:25,694 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:44:25,695 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:44:25,696 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:44:25,698 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:44:25,701 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:44:25,703 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:44:25,705 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:44:25,706 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:44:25,708 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:44:25,709 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:44:25,710 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:44:25,714 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:44:25,718 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:44:25,739 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 16:44:30,135 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 16:44:31,370 [INFO] AFC remote call 1 is done.
2025-06-28 16:44:31,553 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:44:31,554 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:44:31,555 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:44:31,562 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:44:31,567 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:44:31,570 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:44:31,578 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:44:31,587 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:44:31,588 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:44:31,601 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:44:31,605 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:44:31,609 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:44:31,743 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:44:31,749 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:44:31,752 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:44:31,756 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:45:14,005 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:45:14,006 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:45:14,006 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:45:14,007 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:45:14,008 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:45:14,009 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:45:14,009 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:45:14,010 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:45:14,010 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:45:14,011 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:45:14,012 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:45:14,012 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:45:14,013 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:45:14,014 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:45:14,014 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:45:14,016 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:45:14,396 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:45:14,397 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:45:14,399 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:45:14,401 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:45:14,402 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:45:14,403 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:45:14,404 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:45:14,406 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:45:14,407 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:45:14,408 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:45:14,409 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:45:14,412 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:45:14,413 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:45:14,415 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:45:14,419 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:45:14,424 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:45:24,010 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:45:24,011 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:45:24,012 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:45:24,013 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:45:24,014 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:45:24,015 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:45:24,017 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:45:24,019 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:45:24,020 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:45:24,021 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:45:24,023 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:45:24,025 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:45:24,032 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:45:24,033 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:45:24,034 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:45:24,035 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:45:24,254 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:45:24,255 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:45:24,256 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:45:24,257 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:45:24,258 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:45:24,259 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:45:24,261 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:45:24,263 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:45:24,266 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:45:24,268 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:45:24,269 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:45:24,274 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:45:24,276 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:45:24,278 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:45:24,279 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:45:24,280 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:45:24,415 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628164524_clipboard_20250628164514.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python
from services.gemini_image_generator import GeminiImageGenerator

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

@st.cache_resource
def get_image_generator():
    return GeminiImageGenerator()

image_generator = get_image_generator()
# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = image_generator.generate_image(image_prompt)


            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gemini_image_generator.py

from google import genai
from google.genai import types
from PIL import Image
import io
import os

class GeminiImageGenerator:
    def __init__(self):
        api_key = os.getenv("API_KEY_GEMINI")
        if not api_key:
            raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")
        self.client = genai.Client(api_key=api_key)

    def generate_image(self, prompt: str) -> Image.Image | None:
        try:
            response = self.client.models.generate_content(
                model='gemini-2.0-flash-exp-image-generation',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_modalities=['Text', 'Image']
                )
            )
            for part in response.candidates[0].content.parts:
                if part.inline_data is not None:
                    return Image.open(io.BytesIO(part.inline_data.data))
            return None
        except Exception as e:
            print(f"Erro ao gerar imagem: {e}")
            return None


# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

# import os
# from dotenv import load_dotenv
# from google import genai
# from PIL import Image
# from io import BytesIO

# # Carrega as variáveis de ambiente do arquivo .env
# load_dotenv()

# # Obtém a chave da API Gemini do arquivo .env
# api_key = os.getenv("API_KEY_GEMINI")

# # Verifica se a chave da API foi carregada corretamente
# if not api_key:
#     raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# # Inicializa o Gemini
# genai.configure(api_key=api_key)

# def generate_image(prompt: str) -> Image.Image | None:
#     """
#     Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

#     Args:
#         prompt (str): O prompt de texto para gerar a imagem.

#     Returns:
#         Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
#     """
#     try:
#         model = genai.GenerativeModel('gemini-2.0-flash-preview-image-generation')
#         response = model.generate_content(prompt)
#         if response.prompt_feedback:
#             print('Reason: {}'.format(response.prompt_feedback.block_reason))
#         # Verifique se a resposta contém dados de imagem
#         if response.parts:
#             for part in response.parts:
#                 if part.mime_type == 'image/png':
#                     return Image.open(BytesIO(part.data))
#         print(response.text)
#         return None
#     except Exception as e:
#         print(f"Erro ao gerar imagem: {e}")
#         return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

Assistant: Imagem gerada com o prompt: gere uma imagem aleatoria
User: Que imagem [e esas ?
2025-06-28 16:45:34,099 [INFO] Resposta da IA (imagem): Esta imagem mostra uma criatura fantástica e fofa de cor roxa vibrante, com olhos grandes e amarelos que parecem brilhar. Ela tem pelos bem felpudos e orelhas pontudas, grandes e peludas, além de um pequeno focinho rosa. A criatura está sentada em um terreno coberto por folhagem densa e azulada, que se assemelha a musgo ou pequenos arbustos.

O ambiente ao redor é uma floresta mágica e escura, com árvores de troncos retorcidos e escuros que emolduram a cena. Há pontos de luz brilhantes e esferas azul-turquesa luminosas espalhadas pelo ar e na folhagem, criando uma atmosfera encantada. Ao fundo, parece haver uma cachoeira ou uma formação rochosa vertical envolta em névoa, sob um céu que varia de tons de azul a âmbar, com mais pontos luminosos.

É uma imagem que evoca um sentimento de fantasia e mistério, com a criatura parecendo amigável e curiosa. Esta é a imagem gerada aleatoriamente que você viu anteriormente.
2025-06-28 16:45:34,193 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:45:34,195 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:45:34,197 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:45:34,198 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:45:34,200 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:45:34,203 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:45:34,206 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:45:34,209 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:45:34,211 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:45:34,212 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:45:34,216 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:45:34,220 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:45:34,222 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:45:34,223 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:45:34,226 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:45:34,227 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:46:46,859 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:46:46,860 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:46:46,861 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:46:46,862 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:46:46,863 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:46:46,863 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:46:46,864 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:46:46,865 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:46:46,867 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:46:46,869 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:46:46,870 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:46:46,871 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:46:46,873 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:46:46,874 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:46:46,875 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:46:46,876 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:46:47,157 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:46:47,158 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:46:47,159 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:46:47,161 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:46:47,162 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:46:47,162 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:46:47,163 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:46:47,168 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:46:47,173 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:46:47,180 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:46:47,181 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:46:47,183 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:46:47,185 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:46:47,189 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:46:47,190 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:46:47,197 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:46:47,334 [INFO] Enviando para IA - Imagem: C:\Users\HeannarReis\Documents\azure-ai-sentiment-analysis\src\assets\20250628164647_clipboard_20250628164514.png, Prompt: Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas.

Contexto:



# chat_streamlit.py

import streamlit as st
import time
from datetime import datetime
from core.handlers.gemini_handler import GeminiHandler
from PIL import Image
import os
import io
from config.config import Config
from core.rate_limiter import RateLimiter  # Importe a classe RateLimiter
from google import genai
from google.genai import types
from dotenv import load_dotenv
from services.search_files import ler_todos_arquivos_python
from services.gemini_image_generator import GeminiImageGenerator

# Carrega as variáveis de ambiente
load_dotenv()

# Inicializa RateLimiter
rate_limiter = RateLimiter(max_requests=7, period_seconds=60)

# Inicializa estados do session_state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "uploaded_image" not in st.session_state:
    st.session_state.uploaded_image = None
if "clipboard_image_preview" not in st.session_state:
    st.session_state.clipboard_image_preview = None
if "clipboard_image_file" not in st.session_state:
    st.session_state.clipboard_image_file = None
if "last_message_time" not in st.session_state:
    st.session_state.last_message_time = 0
if "file_uploader_key" not in st.session_state:
    st.session_state.file_uploader_key = "uploader_0"
if "generated_image" not in st.session_state:
    st.session_state.generated_image = None
if "image_prompt" not in st.session_state:
    st.session_state.image_prompt = None

# Limite máximo de mensagens no histórico
MAX_MESSAGES = 15

# Função para carregar o prompt do chat
def load_chat_prompt():
    try:
        with open(Config.PROMPT_CHAT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        return "Você é um assistente de IA versátil e útil. Você pode conversar sobre diversos assuntos e também analisar imagens quando elas forem fornecidas."

# Adicione o conteúdo dos arquivos Python como contexto
codigo_fonte = ler_todos_arquivos_python()
chat_prompt = f"{load_chat_prompt()}\n\nContexto:\n\n{codigo_fonte}"

# Inicializa GeminiHandler
@st.cache_resource
def get_gemini_handler():
    return GeminiHandler("gemini-2.5-flash")

gemini_handler = get_gemini_handler()

@st.cache_resource
def get_image_generator():
    return GeminiImageGenerator()

image_generator = get_image_generator()
# Função para verificar e processar a área de transferência
def check_clipboard():
    try:
        from PIL import ImageGrab

        # Tenta pegar imagem da área de transferência
        img = ImageGrab.grabclipboard()

        if img is not None and isinstance(img, Image.Image):
            # Converte a imagem para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_byte_arr.seek(0)

            # Cria um objeto similar ao retornado pelo st.file_uploader
            class ClipboardFile:
                def __init__(self, bytes_data):
                    self.bytes_data = bytes_data
                    self.name = f"clipboard_{datetime.now().strftime('%Y%m%d%H%M%S')}.png"

                def getbuffer(self):
                    return self.bytes_data.getvalue()

            return ClipboardFile(img_byte_arr), img
        return None, None
    except Exception as e:
        st.sidebar.error(f"Erro ao acessar a área de transferência: {e}")
        return None, None

# Função para resetar o uploader alterando sua chave
def reset_uploader():
    # Extrai o número da chave atual
    current_key = st.session_state.file_uploader_key
    key_num = int(current_key.split("_")[1])
    # Gera uma nova chave incrementando o número
    st.session_state.file_uploader_key = f"uploader_{key_num + 1}"
    # Limpa o estado do uploaded_image
    st.session_state.uploaded_image = None

# Função que processa a mensagem (com ou sem imagem)
def process_message(user_input, image_data=None, generated_image=None):
    # Marca como processando para bloquear novos inputs
    st.session_state.processing = True
    st.session_state.current_prompt = user_input
    st.session_state.current_image = image_data
    st.session_state.current_generated_image = generated_image

    # Força a reexecução para atualizar a UI e mostrar o indicador de processamento
    st.rerun()

def execute_processing():
    user_input = st.session_state.current_prompt
    image_data = st.session_state.current_image
    generated_image = st.session_state.current_generated_image

    # Garante que não exceda o limite de requisições
    rate_limiter.wait_for_slot()  # Espera até que um slot esteja disponível

    # Continua com o processamento normal
    current_time = time.time()
    time_since_last_message = current_time - st.session_state.last_message_time
    wait_time = max(0, 2 - time_since_last_message)
    time.sleep(wait_time)

    st.session_state.last_message_time = time.time()

    img_path = None
    img_display = None

    # Adiciona mensagem do usuário ao histórico
    if image_data:
        os.makedirs(Config.ASSETS_DIR, exist_ok=True)
        img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{image_data.name}"
        img_path = os.path.join(Config.ASSETS_DIR, img_name)
        with open(img_path, "wb") as f:
            f.write(image_data.getbuffer())
        with Image.open(img_path) as img:
            img_display = img.copy()

        st.session_state.messages.append({"role": "user", "content": user_input, "image": img_display})
    elif generated_image:
        st.session_state.messages.append({"role": "user", "content": user_input, "image": generated_image})
    else:
        st.session_state.messages.append({"role": "user", "content": user_input})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Constrói o prompt completo incluindo o histórico do chat
    full_prompt = chat_prompt + "\n\n"  # Start with the base prompt

    for message in st.session_state.messages[:-1]: # Exclude the last user message
        role = message["role"]
        content = message["content"]
        full_prompt += f"{role.capitalize()}: {content}\n"

    full_prompt += f"User: {user_input}" # Add current user message

    # Processa resposta da IA
    try:
        if img_path:
            # Se tem imagem: usa o prompt específico para imagens
            response = gemini_handler.generate_content(img_path, full_prompt)
        elif generated_image:
             # Salvando a imagem gerada para ser lida pelo GeminiHandler
             os.makedirs(Config.ASSETS_DIR, exist_ok=True)
             img_name = f"{datetime.now().strftime('%Y%m%d%H%M%S')}_generated_image.png"
             img_path = os.path.join(Config.ASSETS_DIR, img_name)
             generated_image.save(img_path)

             response = gemini_handler.generate_content(img_path, full_prompt)
        else:
            # Se não tem imagem: apenas conversa normal
            response = gemini_handler.generate_content(None, full_prompt)
    except Exception as e:
        response = f"❌ Erro ao gerar resposta: {str(e)}"

    # Adiciona resposta ao histórico
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Garante que o histórico não exceda o limite
    if len(st.session_state.messages) > MAX_MESSAGES:
        st.session_state.messages = st.session_state.messages[-MAX_MESSAGES:]

    # Remove imagem temporária do disco após uso
    if img_path and os.path.exists(img_path):
        os.remove(img_path)

    # Marca o processamento como concluído, mas NÃO limpa as imagens
    st.session_state.processing = False
    st.session_state.current_prompt = None
    st.session_state.current_image = None
    st.session_state.current_generated_image = None

# Callback quando o botão de colar da área de transferência é clicado
def on_paste_click():
    clipboard_file, clipboard_preview = check_clipboard()
    if clipboard_file and clipboard_preview:
        # Reseta o uploader para limpar o arquivo atual
        reset_uploader()
        # Define as imagens da área de transferência
        st.session_state.clipboard_image_file = clipboard_file
        st.session_state.clipboard_image_preview = clipboard_preview
        return True
    return False

# Callback quando um arquivo é carregado
def on_file_upload():
    # Limpa qualquer imagem da área de transferência
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Callback para limpar todas as imagens
def clear_all_images():
    reset_uploader()
    st.session_state.clipboard_image_preview = None
    st.session_state.clipboard_image_file = None

# Executa o processamento se estiver na fila
if st.session_state.processing and hasattr(st.session_state, 'current_prompt'):
    execute_processing()
    st.rerun()

# Configuração da barra lateral
with st.sidebar:
    st.title("Chat IA Inteligente")

    # Seção de geração de imagem
    st.markdown("### Gerar Imagem")
    image_prompt = st.text_input("Digite o prompt para gerar uma imagem:", key="image_prompt")
    if st.button("Gerar Imagem"):   
        if image_prompt:
            generated_image = image_generator.generate_image(image_prompt)


            if generated_image:
                st.session_state.messages.append({"role": "assistant", "image": generated_image, "content": f"Imagem gerada com o prompt: {image_prompt}"})
                st.session_state.generated_image = None #Limpa para não exibir em cima

                st.rerun()
        else:
            st.warning("Por favor, digite um prompt para gerar a imagem.")

    # Seção de imagens (sempre visível)
    st.markdown("### Adicionar Imagem (Opcional)")
    st.caption("Adicione uma imagem se quiser fazer perguntas sobre ela")

    # Layout em duas colunas para os botões de imagem
    col1, col2 = st.columns(2)

    with col1:
        # Botão para verificar a área de transferência
        if st.button("📋 Colar", use_container_width=True):
            if on_paste_click():
                st.success("Imagem colada!")
                st.rerun()
            else:
                st.warning("Nada encontrado.")

    with col2:
        # Botão para limpar a imagem atual (se houver)
        if st.session_state.clipboard_image_preview or st.session_state.uploaded_image:
            if st.button("🗑️ Limpar", use_container_width=True):
                clear_all_images()
                st.rerun()
        else:
            # Placeholder para manter o layout alinhado
            st.write("")

    # Uploader de imagem com chave dinâmica
    uploaded_file = st.file_uploader(
        "📷 Ou faça upload de imagem",
        type=["png", "jpg", "jpeg"],
        label_visibility="visible",
        key=st.session_state.file_uploader_key
    )

    # Atualiza o estado da imagem quando um arquivo é carregado
    if uploaded_file:
        st.session_state.uploaded_image = uploaded_file
        on_file_upload()
        st.success("Imagem carregada!")

    # Exibe a imagem selecionada na barra lateral
    if st.session_state.clipboard_image_preview:
        st.image(st.session_state.clipboard_image_preview, use_container_width=True)
        st.caption("Imagem da área de transferência")
    elif st.session_state.uploaded_image:
        st.image(st.session_state.uploaded_image, use_container_width=True)
        st.caption("Imagem carregada")

    st.markdown("---")

    # Botão para limpar o histórico de conversa
    if st.button("🧹 Limpar conversa", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Desenvolvido com Streamlit e Gemini AI")

# Removendo a exibição da imagem gerada aqui (ela será exibida no histórico de mensagens)
#if st.session_state.generated_image:
#    st.image(st.session_state.generated_image, caption="Imagem Gerada", use_column_width=True)

# Exibição do histórico de mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # Se houver imagem, exiba-a (se armazenada)
        if message.get("image"):
            st.image(message["image"], use_container_width=True)
        # Exibe o conteúdo da mensagem (texto)
        st.markdown(message["content"])

# Adiciona indicador de digitação quando estiver processando
if st.session_state.processing:
    with st.chat_message("assistant"):
        st.markdown("Gerando resposta...")

# Input de texto - deixe-o como último elemento para manter o comportamento "fixo" natural
if not st.session_state.processing:
    # Verifica se há uma imagem disponível
    current_image = st.session_state.clipboard_image_file or st.session_state.uploaded_image

    # Adapta o placeholder com base na presença de imagem
    if current_image:
        placeholder = "Digite sua pergunta sobre a imagem ou qualquer outro assunto..."
    else:
        placeholder = "Digite sua mensagem..."

    user_input = st.chat_input(placeholder)

    if user_input:
        # Processa a mensagem com a imagem (se houver) ou apenas texto
        process_message(user_input, current_image)
else:
    st.chat_input("Aguarde o processamento...", disabled=True)

# config\config.py

# config.py
import os
from pathlib import Path

class Config:
    BASE_DIR = Path(__file__).resolve().parent.parent
    ASSETS_DIR = BASE_DIR / "assets"
    IMAGE_GENERATED_DIR = ASSETS_DIR / "image_generated"
    PROCESSED_DIR = BASE_DIR / "processed_images"
    OUTPUT_DOCX = BASE_DIR / "resumo_analises_imagens.docx"
    OUTPUT_MD = BASE_DIR / "resumo_analises_imagens.md"
    
    # Caminhos para prompts dinâmicos
    PROMPT_DIR = BASE_DIR / "src" / "prompt"
    PROMPT_DOC_FILE = PROMPT_DIR / "prompt_doc.txt"
    PROMPT_CHAT_FILE = PROMPT_DIR / "prompt_chat.txt"
    
    # Configuração de logs
    LOG_DIR = BASE_DIR / "logs"
    
    # Configuração de histórico
    HISTORY_FILE = BASE_DIR / "historico_analises.json"
    
    # Configuração de rate limiting
    CHAT_RATE_LIMIT = {"max_requests": 9, "period_seconds": 60}
    API_RATE_LIMIT = {"max_requests": 14, "period_seconds": 60}
    
    @classmethod
    def ensure_directories(cls):
        """Garante que todos os diretórios necessários existam."""
        for directory in [cls.ASSETS_DIR, cls.IMAGE_GENERATED_DIR, 
                         cls.PROCESSED_DIR, cls.LOG_DIR, cls.PROMPT_DIR]:
            directory.mkdir(parents=True, exist_ok=True)

# core\handlers\gemini_handler.py

from services.gpt_services import GenerativeModelHandler
from core.logger_config import logger
from core.rate_limiter import RateLimiter  # supondo que você salvou a classe acima em core/rate_limiter.py

class GeminiHandler:
    def __init__(self, model_name):
        self.handler = GenerativeModelHandler(model_name)
        self.rate_limiter = RateLimiter(max_requests=15, period_seconds=60)

    def generate_content(self, img_path, prompt):
        self.rate_limiter.wait_for_slot()  # Aguarda até que haja um slot disponível

        if img_path:
            logger.info(f"Enviando para IA - Imagem: {img_path}, Prompt: {prompt}")
            return self.handler.generate_content_from_image(img_path, prompt)
        else:
            logger.info(f"Enviando para IA - Prompt (sem imagem): {prompt}")
            return self.handler.generate_content_from_text(prompt)

# core\handlers\signal_handler.py

# utils/signal_handler.py
import signal
import sys
from core.logger_config import logger

class SignalHandler:
    """Gerencia sinais do sistema operacional para interrupção controlada."""
    
    @staticmethod
    def handler(signum, frame):
        """
        Manipulador de sinal para interrupção controlada.
        
        Args:
            signum: Número do sinal
            frame: Frame atual
        """
        logger.warning("🚨 Processamento interrompido pelo usuário.")
        sys.exit(1)

    @staticmethod
    def setup():
        """Configura o manipulador de sinais para SIGINT (Ctrl+C)."""
        signal.signal(signal.SIGINT, SignalHandler.handler)

# core\logger_config.py

# core/logger_config.py
import logging
from datetime import datetime
from config.config import Config

class LoggerSetup:
    @staticmethod
    def setup_logger():
        """Configura e retorna um logger global para a aplicação."""
        Config.ensure_directories()
        
        log_filename = f"log_{datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = Config.LOG_DIR / log_filename

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(levelname)s] %(message)s",
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)

# Instância global do logger
logger = LoggerSetup.setup_logger()

# core\rate_limiter.py

# core/rate_limiter.py
import time
from collections import deque
from threading import Lock
from typing import Optional

class RateLimiter:
    """
    Implementa um limitador de taxa para controlar o número de requisições
    em um determinado período de tempo.
    """
    def __init__(self, max_requests: int, period_seconds: int):
        """
        Inicializa o limitador de taxa.
        
        Args:
            max_requests: Número máximo de requisições permitidas no período
            period_seconds: Período de tempo em segundos
        """
        self.max_requests = max_requests
        self.period_seconds = period_seconds
        self.requests = deque()
        self.lock = Lock()

    def allow_request(self) -> bool:
        """
        Verifica se uma nova requisição pode ser feita.
        
        Returns:
            bool: True se a requisição for permitida, False caso contrário
        """
        with self.lock:
            current_time = time.time()

            # Remove requisições antigas fora da janela de tempo
            while self.requests and self.requests[0] <= current_time - self.period_seconds:
                self.requests.popleft()

            if len(self.requests) < self.max_requests:
                self.requests.append(current_time)
                return True
            return False

    def wait_for_slot(self) -> None:
        """
        Aguarda até que um slot esteja disponível para uma nova requisição.
        """
        while not self.allow_request():
            current_time = time.time()
            
            # Calcula o tempo de espera baseado na requisição mais antiga
            if self.requests:
                earliest_request_time = self.requests[0] 
                remaining_time = max(0, self.period_seconds - (current_time - earliest_request_time))
            else:
                remaining_time = 1  # Espera padrão se não houver requisições
                
            time.sleep(remaining_time)

# main.py

# main.py
from core.handlers.signal_handler import setup_signal_handler
from services.image_processor import ImageProcessor
from core.rate_limiter import RateLimiter
import json

def main():
    setup_signal_handler()

    # Inicializa o RateLimiter com o número máximo de requisições e o período
    rate_limiter = RateLimiter(max_requests=9, period_seconds=60)  # 9 requisições por minuto

    # Passa o RateLimiter para o ImageProcessor
    processor = ImageProcessor(rate_limiter)
    processor.process_images()

    # Obter e imprimir o histórico após o processamento
    history = processor.get_history()
    for item in history:
        print(f"Imagem: {item['image_name']}, Resumo: {item['summary']}")

    # Salvar o histórico em um arquivo JSON
    with open(processor.historico_json_file, "w") as f:
        json.dump(history, f, indent=4)

if __name__ == "__main__":
    main()

# services\document_service.py

from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.enum.style import WD_STYLE_TYPE
from docx.oxml.ns import qn
from config.config import Config
import os
from core.logger_config import logger  # Importação correta

class DocumentService:
    def __init__(self):
        self.doc = self._load_or_create_document()
        self._setup_document_styles()

    def _load_or_create_document(self):
        if os.path.exists(Config.OUTPUT_DOCX):
            return Document(Config.OUTPUT_DOCX)
        doc = Document()
        # Configuração inicial do documento
        title = doc.add_heading('Análise de Imagens com Inteligência Artificial', level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adiciona subtítulo
        subtitle = doc.add_paragraph('Relatório Gerado Automaticamente')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.style = 'Subtitle'

        # Adiciona uma quebra de página após o título
        doc.add_page_break()

        return doc

    def _setup_document_styles(self):
        """Configura estilos personalizados para o documento"""
        styles = self.doc.styles

        # Estilo para título de imagem
        if 'Image Title' not in styles:
            image_title_style = styles.add_style('Image Title', WD_STYLE_TYPE.PARAGRAPH)
            font = image_title_style.font
            font.name = 'Calibri'
            font.size = Pt(16)
            font.bold = True
            font.color.rgb = RGBColor(0, 112, 192)  # Azul
            paragraph_format = image_title_style.paragraph_format
            paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centraliza o título
            paragraph_format.space_before = Pt(12)
            paragraph_format.space_after = Pt(6)

        # Estilo para o texto do resumo
        if 'Summary Text' not in styles:
            summary_style = styles.add_style('Summary Text', WD_STYLE_TYPE.PARAGRAPH)
            font = summary_style.font
            font.name = 'Calibri'
            font.size = Pt(11)
            paragraph_format = summary_style.paragraph_format
            paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE
            paragraph_format.space_before = Pt(0)  # Reduzir o espaçamento antes do resumo
            paragraph_format.space_after = Pt(12)
            paragraph_format.first_line_indent = Pt(18)  # Recuo na primeira linha

    def add_image_summary(self, image_name, summary):
        image_path = os.path.join(Config.PROCESSED_DIR, image_name)
        logger.info(f"Caminho da imagem para o Word: {image_path}")  # Uso correto do logger

        # Adiciona o título da imagem
        p = self.doc.add_paragraph(image_name, style='Image Title')  # Adiciona o título antes da imagem


        # Adiciona a imagem ao documento com tamanho de página inteira
        if os.path.exists(image_path):
            paragraph = self.doc.add_paragraph()
            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = paragraph.add_run()

            # Obtém a largura da página
            section = self.doc.sections[0]
            page_width = section.page_width
            page_height = section.page_height

            # Calcula as margens
            left_margin = section.left_margin
            right_margin = section.right_margin

            # Calcula a largura disponível (largura da página menos margens)
            available_width = page_width - left_margin - right_margin

            # Adiciona a imagem com a largura disponível
            picture = run.add_picture(image_path, width=available_width)

            # Remover a linha que adiciona o parágrafo vazio
            # self.doc.add_paragraph()

        # Formata o resumo com estilo personalizado
        clean_summary = self._clean_markdown(summary)

        # Adiciona o resumo com estilo personalizado
        p = self.doc.add_paragraph(clean_summary, style='Summary Text')

    def _add_horizontal_line(self):
        """Adiciona uma linha horizontal decorativa"""
        p = self.doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_fmt = p.paragraph_format
        p_fmt.space_after = Pt(12)

        # Adiciona uma linha usando caracteres
        run = p.add_run('─' * 50)  # 50 caracteres de linha
        run.font.color.rgb = RGBColor(192, 192, 192)  # Cinza claro

    def _clean_markdown(self, text):
        """Remove marcações markdown do texto"""
        # Remove cabeçalhos markdown (###, ##, etc)
        import re
        text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

        # Remove marcações de negrito e itálico
        text = text.replace('**', '').replace('*', '').replace('__', '').replace('_', '')

        # Remove marcadores de lista
        text = re.sub(r'^\s*[-*+]\s+', '• ', text, flags=re.MULTILINE)

        return text

    def save_document(self):
        # Adiciona informações de rodapé
        # section = self.doc.sections[0]
        # footer = section.footer
        # footer_para = footer.paragraphs[0]
        # footer_para.text = f"Documento gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')} | Assistente Visual Inteligente"
        # footer_para.style = self.doc.styles['Footer']

        self.doc.save(Config.OUTPUT_DOCX)

# services\gemini_image_generator.py

from google import genai
from google.genai import types
from PIL import Image
import io
import os

class GeminiImageGenerator:
    def __init__(self):
        api_key = os.getenv("API_KEY_GEMINI")
        if not api_key:
            raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")
        self.client = genai.Client(api_key=api_key)

    def generate_image(self, prompt: str) -> Image.Image | None:
        try:
            response = self.client.models.generate_content(
                model='gemini-2.0-flash-exp-image-generation',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_modalities=['Text', 'Image']
                )
            )
            for part in response.candidates[0].content.parts:
                if part.inline_data is not None:
                    return Image.open(io.BytesIO(part.inline_data.data))
            return None
        except Exception as e:
            print(f"Erro ao gerar imagem: {e}")
            return None


# services\gpt_services.py

# services/gpt_services.py
import os
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional
import logging
from core.logger_config import logger

class GenerativeModelHandler:
    def __init__(self, model_name: str):
        self.model_name: str = model_name
        self.model: Optional[genai.GenerativeModel] = None
        self.api_key: Optional[str] = None
        self._load_env_variables()
        self._configure_api()
        self._initialize_model()

    def _load_env_variables(self) -> None:
        load_dotenv()
        self.api_key = os.getenv('API_KEY_GEMINI')
        if not self.api_key:
            logger.error("API Key não encontrada nas variáveis de ambiente")
            raise ValueError("API Key não encontrada nas variáveis de ambiente")

    def _configure_api(self) -> None:
        genai.configure(api_key=self.api_key)

    def _initialize_model(self) -> None:
        try:
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"Modelo Gemini '{self.model_name}' inicializado com sucesso.")
        except Exception as e:  
            logger.error(f"Erro ao inicializar o modelo: {e}")
            raise RuntimeError(f"Erro ao inicializar o modelo: {e}")

    def generate_content_from_image(self, image_path: str, prompt: str) -> str:
        try:
            with open(image_path, "rb") as image_file:
                image_bytes = image_file.read()

            response = self.model.generate_content([
                {"mime_type": "image/png", "data": image_bytes},
                prompt
            ])

            logger.info(f"Resposta da IA (imagem): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao processar a imagem: {e}")
            raise RuntimeError(f"Erro ao processar a imagem: {e}")

    def generate_content_from_text(self, prompt: str) -> str:
        try:
            response = self.model.generate_content(prompt)
            logger.info(f"Resposta da IA (texto): {response.text}")
            return response.text
        except Exception as e:
            logger.error(f"Erro ao gerar conteúdo: {e}")
            raise RuntimeError(f"Erro ao gerar conteúdo: {e}")

# services\image_processor.py

# src/image_processor.py
import os
import time
import shutil
import json
from config.config import config
from services.gpt_services import GenerativeModelHandler
from services.document_service import DocumentService
from services.markdown_service import MarkdownService
from utils.file_utils import list_images
from core.logger_config import logger
from core.rate_limiter import RateLimiter

class ImageProcessor:
    def __init__(self, rate_limiter: RateLimiter):
        self.gpt_handler = GenerativeModelHandler("gemini-2.5-flash")
        self.document_service = DocumentService()
        self.markdown_service = MarkdownService()
        os.makedirs(config.PROCESSED_DIR, exist_ok=True)
        self.prompt = self._load_prompt()
        self.history = []
        self.rate_limiter = rate_limiter
        self.historico_json_file = "historico_analises.json"
        self.analises_anteriores = self._carregar_historico_json()  # Carrega o histórico ao inicializar

    def _load_prompt(self):
        try:
            with open(config.PROMPT_DOC_FILE, "r", encoding="utf-8") as file:
                prompt = file.read().strip()
                logger.info(f"Prompt carregado com sucesso: {prompt}")
                return prompt
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")
            raise FileNotFoundError(f"Arquivo de prompt não encontrado em {config.PROMPT_DOC_FILE}")

    def _carregar_historico_json(self):
        try:
            with open(self.historico_json_file, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def _salvar_historico_json(self):
        with open(self.historico_json_file, "w") as f:
            json.dump(self.analises_anteriores, f, indent=4)

    def process_images(self):
        images = list_images(config.ASSETS_DIR)
        if not images:
            logger.warning("Nenhuma imagem encontrada em 'assets/'.")
            return

        for idx, image_name in enumerate(images, start=1):
            logger.info(f"Processando imagem {idx}/{len(images)}: {image_name}")

            try:
                self.rate_limiter.wait_for_slot()
                summary = self._process_image(image_name)
                self.document_service.add_image_summary(image_name, summary)
                self.markdown_service.add_image_summary(image_name, summary)
                self.document_service.save_document()
                self.markdown_service.save_markdown()
                self._move_image(image_name)
                self._update_history(image_name, summary)

                # Não adicionar a mesma informação repetidas vezes
                # self.analises_anteriores.append(f"Imagem: {image_name}, Resumo: {summary}")
                # self._salvar_historico_json()

            except Exception as e:
                logger.error(f"Erro ao processar a imagem {image_name}: {e}", exc_info=True)

            time.sleep(4)
            logger.info("Preparando a próxima análise...")

    def _process_image(self, image_name):
        img_path = os.path.join(config.ASSETS_DIR, image_name)
        processed_path = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.copy2(img_path, processed_path)

        try:
            # Não precisa carregar o histórico a cada imagem
            # self._carregar_historico_json()

            historico_str = "\n".join([f"{entry['image_name']}: {entry['summary']}" for entry in self.history])
            prompt_com_historico = f"{self.prompt}\nHistórico:\n{historico_str}\nAnalise a seguinte imagem: {image_name}"
            response_text = self.gpt_handler.generate_content_from_image(img_path, prompt_com_historico)
            logger.info(f"Resumo gerado para '{image_name}': {response_text}")
            return response_text
        except Exception as e:
            logger.error(f"Erro ao processar '{image_name}': {str(e)}")
            return f"Erro ao processar imagem: {str(e)}"

    def _move_image(self, image_name):
        origem = os.path.join(config.ASSETS_DIR, image_name)
        destino = os.path.join(config.PROCESSED_DIR, image_name)
        shutil.move(origem, destino)
        logger.info(f"Imagem '{image_name}' movida para '{config.PROCESSED_DIR}'.")

    def _update_history(self, image_name, summary):
        self.history.append({"image_name": image_name, "summary": summary})
        logger.info(f"Histórico atualizado com '{image_name}'.")

    def get_history(self):
        return self.history

# services\image_services.py

# import os
# from dotenv import load_dotenv
# from google import genai
# from PIL import Image
# from io import BytesIO

# # Carrega as variáveis de ambiente do arquivo .env
# load_dotenv()

# # Obtém a chave da API Gemini do arquivo .env
# api_key = os.getenv("API_KEY_GEMINI")

# # Verifica se a chave da API foi carregada corretamente
# if not api_key:
#     raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

# # Inicializa o Gemini
# genai.configure(api_key=api_key)

# def generate_image(prompt: str) -> Image.Image | None:
#     """
#     Gera uma imagem usando o modelo Gemini com base no prompt fornecido.

#     Args:
#         prompt (str): O prompt de texto para gerar a imagem.

#     Returns:
#         Image.Image | None: A imagem gerada como um objeto PIL Image ou None em caso de falha.
#     """
#     try:
#         model = genai.GenerativeModel('gemini-2.0-flash-preview-image-generation')
#         response = model.generate_content(prompt)
#         if response.prompt_feedback:
#             print('Reason: {}'.format(response.prompt_feedback.block_reason))
#         # Verifique se a resposta contém dados de imagem
#         if response.parts:
#             for part in response.parts:
#                 if part.mime_type == 'image/png':
#                     return Image.open(BytesIO(part.data))
#         print(response.text)
#         return None
#     except Exception as e:
#         print(f"Erro ao gerar imagem: {e}")
#         return None

# services\markdown_service.py

import os
from config.config import Config

class MarkdownService:
    def __init__(self):
        self.content = []

    def add_image_summary(self, image_name, summary):
        """Adiciona uma nova imagem e resumo ao conteúdo do Markdown."""
        image_path = f"/processed_images/{image_name}"  # Caminho relativo
        markdown_entry = f"## Imagem: {image_name}\n![{image_name}]({image_path})\n\n{summary}\n"
        self.content.append(markdown_entry)

    def save_markdown(self):
        """Salva os resumos no arquivo Markdown, garantindo que o novo conteúdo seja anexado sem sobrescrever."""
        if not os.path.exists(Config.OUTPUT_MD):  # Se o arquivo não existir, cria o cabeçalho
            with open(Config.OUTPUT_MD, 'w', encoding='utf-8') as f:
                f.write("# Resumo das Análises das Imagens\n\n")

        with open(Config.OUTPUT_MD, 'a', encoding='utf-8') as f:  # Modo 'a' (append)
            f.write("\n".join(self.content) + "\n")  # Adiciona novas entradas

        self.content = []  # Limpa a lista após salvar para evitar duplicação


# services\search_files.py

import os
import glob
from pathlib import Path
from config.config import Config
import logging  # Importe o módulo de logging

# Configure o logging (você pode ajustar o nível conforme necessário)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def ler_todos_arquivos_python() -> str:
    """Lê todo o conteúdo de todos os arquivos .py a partir de src/"""
    src_dir = Config.BASE_DIR
    conteudo_total = ""

    if not src_dir.exists():
        logging.warning(f"Diretório 'src' não encontrado: {src_dir}")
        return ""

    padrao_busca = os.path.join(src_dir.as_posix(), '**', '*.py')
    arquivos = glob.glob(padrao_busca, recursive=True)

    for arquivo in sorted(arquivos):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                rel_path = os.path.relpath(arquivo, src_dir)
                conteudo_total += f"\n\n# {rel_path}\n\n{f.read()}"
                logging.info(f"Arquivo lido com sucesso: {rel_path}")  # Log de sucesso
        except Exception as e:
            logging.error(f"Erro ao ler o arquivo {arquivo}: {e}")  # Log de erro
            continue

    return conteudo_total

# test_image_generation.py

import os
from dotenv import load_dotenv
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém a chave da API Gemini do arquivo .env
api_key = os.getenv("API_KEY_GEMINI")

# Verifica se a chave da API foi carregada corretamente
if not api_key:
    raise ValueError("API_KEY_GEMINI não encontrada no arquivo .env")

client = genai.Client(api_key=api_key)

prompt = "gerar a imagem do simbolo do corinthians."

response = client.models.generate_content(
    model='gemini-2.0-flash-exp-image-generation',
    contents=prompt,
    config=types.GenerateContentConfig(
        response_modalities=['Text', 'Image']
    )
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    elif part.inline_data is not None:
        image = Image.open(BytesIO(part.inline_data.data))
        image.save("ed-image.png")
        image.show()

# utils\file_utils.py

import os

def list_images(directory):
    return sorted(
        [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
        key=lambda x: os.path.getmtime(os.path.join(directory, x))
    )

Assistant: Imagem gerada com o prompt: gere uma imagem aleatoria
User: Que imagem [e esas ?
Assistant: Esta imagem mostra uma criatura fantástica e fofa de cor roxa vibrante, com olhos grandes e amarelos que parecem brilhar. Ela tem pelos bem felpudos e orelhas pontudas, grandes e peludas, além de um pequeno focinho rosa. A criatura está sentada em um terreno coberto por folhagem densa e azulada, que se assemelha a musgo ou pequenos arbustos.

O ambiente ao redor é uma floresta mágica e escura, com árvores de troncos retorcidos e escuros que emolduram a cena. Há pontos de luz brilhantes e esferas azul-turquesa luminosas espalhadas pelo ar e na folhagem, criando uma atmosfera encantada. Ao fundo, parece haver uma cachoeira ou uma formação rochosa vertical envolta em névoa, sob um céu que varia de tons de azul a âmbar, com mais pontos luminosos.

É uma imagem que evoca um sentimento de fantasia e mistério, com a criatura parecendo amigável e curiosa. Esta é a imagem gerada aleatoriamente que você viu anteriormente.
User: faca um prompt para gerar uma imagem do mundial do Corinthians de 2012
2025-06-28 16:47:01,704 [INFO] Resposta da IA (imagem): Para gerar uma imagem do Mundial do Corinthians de 2012, você precisa ser bem específico com os detalhes para que a IA capte a essência do evento. Aqui estão algumas opções de prompts, focando em diferentes aspectos:

---

**Prompt Principal (Cena de Vitória e Troféu):**

"Uma cena dramática e épica do time de futebol Sport Club Corinthians Paulista celebrando sua vitória na final da Copa do Mundo de Clubes da FIFA de 2012. Os jogadores, usando seus uniformes brancos com listras pretas, estão levantando o troféu dourado bem alto, cercados por confetes caindo. Foco em jogadores icônicos como o goleiro Cássio, o artilheiro Paolo Guerrero, e Emerson Sheik, mostrando pura alegria e triunfo. O fundo é o Estádio Internacional de Yokohama lotado à noite, com um mar de torcedores apaixonados do Corinthians agitando bandeiras e comemorando. As luzes do estádio iluminam a cena, criando uma atmosfera vibrante e emocionante. Fotografia realista, alto detalhe, clima vitorioso, cinemático."

---

**Variação 1 (Foco no Jogador e Momento Crucial):**

"Close-up dinâmico de Paolo Guerrero, o jogador do Corinthians, no momento em que ele marca o gol decisivo na final da Copa do Mundo de Clubes da FIFA de 2012 contra o Chelsea. Ele está comemorando intensamente, com a camisa branca e preta do Corinthians, e a bola no fundo da rede. O estádio de Yokohama ao fundo, com a torcida em êxtase e luzes noturnas. Captura de movimento, emoção crua, fotografia de ação."

---

**Variação 2 (Foco na Torcida e Atmosfera):**

"Uma vista aérea espetacular do Estádio Internacional de Yokohama completamente lotado durante a final da Copa do Mundo de Clubes da FIFA de 2012, focando na torcida do Corinthians. Um mar de preto e branco, com bandeiras balançando, sinalizadores (opcional, se permitido pelo modelo) e a energia eletrizante da vitória. Luzes do estádio, confetes no ar. Perspectiva ampla, cores vibrantes, captura da paixão dos fãs."

---

**Dicas para Melhores Resultados:**

*   **Especificidade:** Quanto mais detalhes você der (cores, jogadores, local, emoção), melhor.
*   **Qualidade Visual:** Termos como "fotografia realista", "alto detalhe", "cinemático", "cores vibrantes" ajudam a IA a entender o estilo desejado.
*   **Emoção:** Palavras como "alegria", "triunfo", "paixão", "êxtase" guiam a IA para o tom certo.
*   **Evite Negativas:** Tente descrever o que você quer ver, em vez do que não quer (ex: em vez de "sem chuva", diga "céu claro").

Experimente esses prompts e ajuste conforme o modelo de IA responde!
2025-06-28 16:47:02,065 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:47:02,066 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:47:02,068 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:47:02,069 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:47:02,070 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:47:02,079 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:47:02,092 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:47:02,098 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:47:02,104 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:47:02,107 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:47:02,123 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:47:02,136 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:47:02,187 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:47:02,208 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:47:02,236 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:47:02,274 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:47:26,712 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:47:26,715 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:47:26,715 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:47:26,716 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:47:26,717 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:47:26,721 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:47:26,732 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:47:26,735 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:47:26,756 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:47:26,762 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:47:26,772 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:47:26,792 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:47:26,817 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:47:26,849 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:47:26,857 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:47:26,968 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:47:26,977 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:47:26,982 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:47:26,995 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:47:27,000 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:47:27,140 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:47:27,141 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:47:27,142 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:47:27,143 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:47:27,144 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:47:27,144 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:47:27,145 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:47:27,146 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:47:27,147 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:47:27,148 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:47:27,149 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:47:27,156 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:47:27,166 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 16:47:31,843 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 16:47:33,152 [INFO] AFC remote call 1 is done.
2025-06-28 16:47:33,260 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:47:33,266 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:47:33,268 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:47:33,270 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:47:33,272 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:47:33,275 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:47:33,287 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:47:33,289 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:47:33,290 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:47:33,292 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:47:33,293 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:47:33,297 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:47:33,308 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:47:33,309 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:47:33,317 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:47:33,388 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:48:26,792 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:48:26,795 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:48:26,797 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:48:26,798 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:48:26,799 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:48:26,801 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:48:26,802 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:48:26,805 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:48:26,819 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:48:26,821 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:48:26,822 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:48:26,823 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:48:26,825 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:48:26,832 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:48:26,834 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:48:26,838 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:48:27,168 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:48:27,171 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:48:27,172 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:48:27,173 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:48:27,174 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:48:27,176 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:48:27,188 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:48:27,191 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:48:27,202 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:48:27,205 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:48:27,206 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:48:27,208 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:48:27,216 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:48:27,217 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:48:27,224 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:48:27,249 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:48:27,261 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 16:48:28,380 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:48:28,380 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:48:28,382 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:48:28,383 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:48:28,384 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:48:28,386 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:48:28,388 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:48:28,389 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:48:28,401 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:48:28,402 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:48:28,403 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:48:28,404 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:48:28,406 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:48:28,407 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:48:28,415 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:48:28,418 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:48:28,597 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:48:28,598 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:48:28,600 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:48:28,602 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:48:28,603 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:48:28,604 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:48:28,605 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:48:28,607 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:48:28,614 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:48:28,617 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:48:28,618 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:48:28,619 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:48:28,621 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:48:28,622 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:48:28,623 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:48:28,631 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:48:31,022 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:48:31,024 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:48:31,025 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:48:31,025 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:48:31,026 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:48:31,027 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:48:31,029 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:48:31,032 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:48:31,034 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:48:31,043 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:48:31,045 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:48:31,046 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:48:31,047 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:48:31,048 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:48:31,049 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:48:31,050 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:48:31,065 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 16:48:31,469 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 16:48:32,577 [INFO] AFC remote call 1 is done.
2025-06-28 16:48:35,284 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 16:48:36,430 [INFO] AFC remote call 1 is done.
2025-06-28 16:48:36,751 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:48:36,754 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:48:36,755 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:48:36,757 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:48:36,758 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:48:36,760 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:48:36,762 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:48:36,775 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:48:36,776 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:48:36,778 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:48:36,779 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:48:36,780 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:48:36,782 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:48:36,794 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:48:36,796 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:48:36,798 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:50:24,684 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:50:24,685 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:50:24,686 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:50:24,687 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:50:24,688 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:50:24,690 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:50:24,692 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:50:24,695 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:50:24,698 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:50:24,705 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:50:24,930 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:50:25,080 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:50:25,096 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:50:25,098 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:50:25,101 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:50:25,102 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:50:25,111 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 16:50:29,609 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 16:50:30,715 [INFO] AFC remote call 1 is done.
2025-06-28 16:50:30,805 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:50:30,806 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:50:30,807 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:50:30,808 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:50:30,809 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:50:30,810 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:50:30,810 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:50:30,813 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:50:30,818 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:50:30,819 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:50:30,821 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:50:30,824 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:50:30,828 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:50:30,829 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:50:30,830 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:50:30,831 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:53:03,429 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:53:03,432 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:53:03,434 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:53:03,439 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:53:03,446 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:53:03,447 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:53:03,450 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:53:03,452 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:53:03,467 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:53:03,503 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:53:03,515 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:53:03,517 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:53:03,536 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:53:03,548 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:53:03,551 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:53:03,553 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:53:04,464 [INFO] Modelo Gemini 'gemini-2.5-flash' inicializado com sucesso.
2025-06-28 16:53:11,714 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:53:11,715 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:53:11,717 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:53:11,719 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:53:11,720 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:53:11,720 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:53:11,722 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:53:11,724 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:53:11,726 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:53:11,738 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:53:11,740 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:53:11,748 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:53:11,755 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:53:11,757 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:53:11,758 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:53:11,759 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:53:16,192 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:53:16,195 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:53:16,196 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:53:16,198 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:53:16,200 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:53:16,202 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:53:16,204 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:53:16,210 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:53:16,211 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:53:16,217 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:53:16,218 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:53:16,219 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:53:16,220 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:53:16,221 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:53:16,222 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:53:16,227 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:53:16,386 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:53:16,387 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:53:16,388 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:53:16,389 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:53:16,390 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:53:16,393 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:53:16,394 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:53:16,396 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:53:16,398 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:53:16,402 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:53:16,404 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:53:16,405 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:53:16,406 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:53:16,410 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:53:16,418 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:53:16,419 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 16:53:16,435 [INFO] AFC is enabled with max remote calls: 10.
2025-06-28 16:53:20,227 [INFO] HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-preview-image-generation:generateContent "HTTP/1.1 200 OK"
2025-06-28 16:53:21,385 [INFO] AFC remote call 1 is done.
2025-06-28 16:53:21,518 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 16:53:21,519 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 16:53:21,525 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 16:53:21,527 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 16:53:21,531 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 16:53:21,533 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 16:53:21,534 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 16:53:21,535 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 16:53:21,547 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 16:53:21,549 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 16:53:21,551 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 16:53:21,552 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 16:53:21,562 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 16:53:21,564 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 16:53:21,566 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 16:53:21,567 [INFO] Arquivo lido com sucesso: utils\file_utils.py
2025-06-28 17:02:18,434 [INFO] Arquivo lido com sucesso: chat_streamlit.py
2025-06-28 17:02:18,439 [INFO] Arquivo lido com sucesso: config\config.py
2025-06-28 17:02:18,443 [INFO] Arquivo lido com sucesso: core\handlers\gemini_handler.py
2025-06-28 17:02:18,447 [INFO] Arquivo lido com sucesso: core\handlers\signal_handler.py
2025-06-28 17:02:18,465 [INFO] Arquivo lido com sucesso: core\logger_config.py
2025-06-28 17:02:18,594 [INFO] Arquivo lido com sucesso: core\rate_limiter.py
2025-06-28 17:02:18,596 [INFO] Arquivo lido com sucesso: main.py
2025-06-28 17:02:18,601 [INFO] Arquivo lido com sucesso: services\document_service.py
2025-06-28 17:02:18,606 [INFO] Arquivo lido com sucesso: services\gemini_image_generator.py
2025-06-28 17:02:18,614 [INFO] Arquivo lido com sucesso: services\gpt_services.py
2025-06-28 17:02:18,618 [INFO] Arquivo lido com sucesso: services\image_processor.py
2025-06-28 17:02:18,622 [INFO] Arquivo lido com sucesso: services\image_services.py
2025-06-28 17:02:18,626 [INFO] Arquivo lido com sucesso: services\markdown_service.py
2025-06-28 17:02:18,628 [INFO] Arquivo lido com sucesso: services\search_files.py
2025-06-28 17:02:18,636 [INFO] Arquivo lido com sucesso: test_image_generation.py
2025-06-28 17:02:18,639 [INFO] Arquivo lido com sucesso: utils\file_utils.py
